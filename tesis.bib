
@article{a.goodallDatadrivenApproximationsBridge2021,
  title = {Data-Driven Approximations to the Bridge Function Yield Improved Closures for the {{Ornstein}}\textendash{{Zernike}} Equation},
  author = {A. Goodall, Rhys E. and A. Lee, Alpha},
  year = {2021},
  journal = {Soft Matter},
  volume = {17},
  number = {21},
  pages = {5393--5400},
  publisher = {{Royal Society of Chemistry}},
  doi = {10.1039/D1SM00402F},
  language = {en}
}

@article{antaFastMethodSolving1995,
  title = {A Fast Method of Solving the Hypernetted-Chain Equation for Molecular {{Lennard}}-{{Jones}} Fluids},
  author = {Anta, J. A. and Lomba, E. and Mart{\'i}n, C. and Lombardero, M. and Lado, F.},
  year = {1995},
  month = mar,
  journal = {Molecular Physics},
  volume = {84},
  number = {4},
  pages = {743--755},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268979500100511},
  abstract = {A fast and stable procedure for solving hypernetted-chain type integral equations for molecular Lennard-Jones fluids is presented. The method is a hybrid algorithm based on the combination of multidimensional angular integration of the closure relation and a linearization technique devised by Fries and Patey (1985, Molec. Phys., 55, 751). The combination of the two techniques leads to a remarkable reduction in the CPU time required to evaluate the closure relation in these systems, which is usually the most time-consuming task. As an application of the method, phase coexistence curves have been calculated for two-centre Lennard-Jones fluids with and without point dipoles.},
  annotation = {\_eprint: https://doi.org/10.1080/00268979500100511}
}

@article{asadyUtilizingArtificialNeural2014,
  title = {Utilizing Artificial Neural Network Approach for Solving Two-Dimensional Integral Equations},
  author = {Asady, B. and Hakimzadegan, F. and Nazarlue, R.},
  year = {2014},
  month = apr,
  journal = {Mathematical Sciences},
  volume = {8},
  number = {1},
  pages = {117},
  issn = {2251-7456},
  doi = {10.1007/s40096-014-0117-6},
  abstract = {This paper surveys the artificial neural networks approach. Researchers believe that these networks have the wide range of applicability, they can treat complicated problems as well. The work described here discusses an efficient computational method that can treat complicated problems. The paper intends to introduce an efficient computational method which can be applied to approximate solution of the linear two-dimensional Fredholm integral equation of the second kind. For this aim, a perceptron model based on artificial neural networks is introduced. At first, the unknown bivariate function is replaced by a multilayer perceptron neural net and also a cost function to be minimized is defined. Then a famous learning technique, namely, the steepest descent method, is employed to adjust the parameters (the weights and biases) to optimize their behavior. The article also examines application of the method which turns to be so accurate and efficient. It concludes with a survey of an example in order to investigate the accuracy of the proposed method.},
  language = {en}
}

@article{baezUsingSecondVirial2018,
  title = {Using the Second Virial Coefficient as Physical Criterion to Map the Hard-Sphere Potential onto a Continuous Potential},
  author = {B{\'a}ez, C{\'e}sar Alejandro and {Torres-Carbajal}, Alexis and {Casta{\~n}eda-Priego}, Ram{\'o}n and {Villada-Balbuena}, Alejandro and {M{\'e}ndez-Alcaraz}, Jos{\'e} Miguel and {Herrera-Velarde}, Salvador},
  year = {2018},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {149},
  number = {16},
  pages = {164907},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.5049568}
}

@article{beardmoreNumericalBifurcationAnalysis2007,
  title = {A {{Numerical Bifurcation Analysis}} of the {{Ornstein}}\textendash{{Zernike Equation}} with {{Hypernetted Chain Closure}}},
  author = {Beardmore, R. E. and Peplow, A. T. and Bresme, F.},
  year = {2007},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {29},
  number = {6},
  pages = {2442--2463},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/060650659},
  abstract = {We study the codimension-one and -two bifurcations of the Ornstein\textendash Zernike equation with hypernetted chain (HNC) closure with Lennard\textendash Jones intermolecular interaction potential. The main purpose of the paper is to present the results of a numerical study undertaken using a suite of algorithms implemented in MATLAB and based on pseudo arc-length continuation for the codimension-one case and a Newton-GMRES method for the codimension-two case. Through careful consideration of the results of our computations, an argument is formulated which shows that spinodal isothermal solution branches arising in this model cannot be reproduced numerically. Furthermore, we show that the existence of an upper bound on the density that can be realized on a vapor isothermal solution branch, which must be present at a spinodal, causes the existence of at least one fold bifurcation along that vapor branch when density is used as the bifurcation parameter. This provides an explanation for previous inconclusive attempts to compute solutions using Newton\textendash Picard methods that are popular in the physical chemistry literature.}
}

@article{bomontRenormalizationIndirectCorrelation2001,
  title = {Renormalization of the Indirect Correlation Function to Extract the Bridge Function of Simple Fluids},
  author = {Bomont, J. M. and Bretonnet, J. L.},
  year = {2001},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {114},
  number = {9},
  pages = {4141--4148},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1344610}
}

@article{boothEfficientSolutionLiquid1999,
  title = {Efficient Solution of Liquid State Integral Equations Using the {{Newton}}-{{GMRES}} Algorithm},
  author = {Booth, Michael J. and Schlijper, A. G. and Scales, L. E. and Haymet, A. D. J.},
  year = {1999},
  month = jun,
  journal = {Computer Physics Communications},
  volume = {119},
  number = {2},
  pages = {122--134},
  issn = {0010-4655},
  doi = {10.1016/S0010-4655(99)00186-1},
  abstract = {We present examples of the accurate, robust and efficient solution of Ornstein-Zernike type integral equations which describe the structure of both homogeneous and inhomogeneous fluids. In this work we use the Newton-GMRES algorithm as implemented in the public-domain nonlinear Krylov solvers NKSOL [P. Brown, Y. Saad, SIAM J. Sci. Stat. Comput. 11 (1990) 450] and NITSOL [M. Pernice, H.F. Walker, SIAM J. Sci. Comput. 19 (1998) 302]. We compare and contrast this method with more traditional approaches in the literature, using Picard iteration (successive-substitution) and hybrid Newton-Raphson and Picard methods, and a recent vector extrapolation method [H.H.H. Homeier, S. Rast, H. Krienke, Comput. Phys. Commun. 92 (1995) 188]. We find that both the performance and ease of implementation of these nonlinear solvers recommend them for the solution of this class of problem.},
  language = {en},
  keywords = {GMRES,Inhomogeneous fluids,Krylov,Newton,Nonlinear integral equation,Numerical solution,Ornstein,Raphson,Zernike}
}

@article{carvalhoIndirectSolutionOrnsteinZernike2020,
  title = {Indirect {{Solution}} of {{Ornstein}}-{{Zernike Equation Using}} the {{Hopfield Neural Network Method}}},
  author = {Carvalho, F. S. and Braga, J. P.},
  year = {2020},
  month = oct,
  journal = {Brazilian Journal of Physics},
  volume = {50},
  number = {5},
  pages = {489--494},
  issn = {1678-4448},
  doi = {10.1007/s13538-020-00769-4},
  abstract = {Microscopic information, such as the pair distribution and direct correlation functions, can be obtained from experimental data. From these correlation functions, thermodynamical quantities and the potential interaction function can be recovered. Derivations of Ornstein-Zernike equation and Hopfield Neural Network method are given first, as a theoretical background to follow the present work. From these two frameworks, structural information, such as the radial distribution (g(r)) and direct correlation (C(r)) functions, were retrieved from neutron scattering experimental data. The problem was solved considering simple initial conditions, which does not require any previous information about the system, making it clear the robustness of the Hopfield Neural Network method. The pair interaction potential was estimated in the Percus-Yevick (PY) and hypernetted chain (HNC) approximations and a poor agreement, compared with the Lennard-Jones 6-12 potential, was observed for both cases, suggesting the necessity of a more accurate closure relation to describe the system. In this sense, the Hopfield Neural Network together with experimental information provides an alternative approach to solve the Ornstein-Zernike equations, avoiding the limitations imposed by the closure relation.},
  language = {en}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  language = {en}
}

@article{edelmanHowManyZeros1995,
  title = {How Many Zeros of a Random Polynomial Are Real?},
  author = {Edelman, Alan and Kostlan, Eric},
  year = {1995},
  month = jan,
  journal = {Bulletin of the American Mathematical Society},
  volume = {32},
  number = {1},
  pages = {1--38},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-1995-00571-9},
  abstract = {We provide an elementary geometric derivation of the Kac integral formula for the expected number of real zeros of a random polynomial with independent standard normally distributed coefficients. We show that the expected number of real zeros is simply the length of the moment curve (1, t, ... , t") projected onto the surface of the unit sphere, divided by n . The probability density of the real zeros is proportional to how fast this curve is traced out. We then relax Kac's assumptions by considering a variety of random sums, series, and distributions, and we also illustrate such ideas as integral geometry and the Fubini-Study metric.},
  language = {en}
}

@article{gaoImplementingNelderMeadSimplex2012,
  title = {Implementing the {{Nelder}}-{{Mead}} Simplex Algorithm with~Adaptive Parameters},
  author = {Gao, Fuchang and Han, Lixing},
  year = {2012},
  month = jan,
  journal = {Computational Optimization and Applications},
  volume = {51},
  number = {1},
  pages = {259--277},
  issn = {1573-2894},
  doi = {10.1007/s10589-010-9329-3},
  abstract = {In this paper, we first prove that the expansion and contraction steps of the Nelder-Mead simplex algorithm possess a descent property when the objective function is uniformly convex. This property provides some new insights on why the standard Nelder-Mead algorithm becomes inefficient in high dimensions. We then propose an implementation of the Nelder-Mead method in which the expansion, contraction, and shrink parameters depend on the dimension of the optimization problem. Our numerical experiments show that the new implementation outperforms the standard Nelder-Mead method for high dimensional problems.},
  language = {en}
}

@article{gillanNewMethodSolving1979,
  title = {A New Method of Solving the Liquid Structure Integral Equations},
  author = {Gillan, M. J.},
  year = {1979},
  month = dec,
  journal = {Molecular Physics},
  volume = {38},
  number = {6},
  pages = {1781--1794},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268977900102861},
  abstract = {We present a new method of obtaining numerical solutions to the Percus-Yevick and hypernetted chain equations for liquid structure. The method, which is rapidly convergent and very stable, is a hybrid of the traditional iterative scheme and the Newton-Raphson technique. We show by numerical tests for typical potentials that the method gives well-converged solutions in 20 or 30 iterations even for very high densities. The number of iterations needed is insensitive to the choice of initial estimate, even if this is extremely inaccurate.},
  annotation = {\_eprint: https://doi.org/10.1080/00268977900102861}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
  language = {en}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
  language = {en}
}

@article{goodallInferenceUniversalOrnsteinZernike,
  title = {Inference of a {{Universal Ornstein}}-{{Zernike Closure Relationship}} with {{Machine Learning}}},
  author = {Goodall, Rhys E A and Lee, Alpha A},
  pages = {6},
  abstract = {The Ornstein-Zernike framework provides an elegant route for solving the inverse problem of determining a pairwise interaction potential for a liquid given its structure. However, in order to realise the potential of the formalism superior closure relationships are required. Current approximate closure relationships have been shown to have restricted universality and give rise to thermodynamic inconsistencies. In this work rather than attempting to analytically derive a new closure relationship we return to the point of the approximation and investigate whether machine learning can be used to infer a universal closure for the framework directly from simulation data. We show preliminary results that indicate this is a fruitful approach and identify areas for further work.},
  language = {en}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  googlebooks = {omivDQAAQBAJ},
  isbn = {978-0-262-33737-3},
  language = {en},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science}
}

@book{hammingNumericalMethodsScientists2012,
  title = {Numerical {{Methods}} for {{Scientists}} and {{Engineers}}},
  author = {Hamming, Richard},
  year = {2012},
  month = apr,
  publisher = {{Courier Corporation}},
  abstract = {Numerical analysis is a subject of extreme interest to mathematicians and computer scientists, who will welcome this first inexpensive paperback edition of a groundbreaking classic text on the subject. In an introductory chapter on numerical methods and their relevance to computing, well-known mathematician Richard Hamming (\&quot;the Hamming code,\&quot; \&quot;the Hamming distance,\&quot; and \&quot;Hamming window,\&quot; etc.), suggests that the purpose of computing is insight, not merely numbers. In that connection he outlines five main ideas that aim at producing meaningful numbers that will be read and used, but will also lead to greater understanding of how the choice of a particular formula or algorithm influences not only the computing but our understanding of the results obtained.The five main ideas involve (1) insuring that in computing there is an intimate connection between the source of the problem and the usability of the answers (2) avoiding isolated formulas and algorithms in favor of a systematic study of alternate ways of doing the problem (3) avoidance of roundoff (4) overcoming the problem of truncation error (5) insuring the stability of a feedback system.In this second edition, Professor Hamming (Naval Postgraduate School, Monterey, California) extensively rearranged, rewrote and enlarged the material. Moreover, this book is unique in its emphasis on the frequency approach and its use in the solution of problems. Contents include:I. Fundamentals and AlgorithmsII. Polynomial Approximation- Classical TheoryIll. Fourier Approximation- Modern TheoryIV. Exponential Approximation ... and moreHighly regarded by experts in the field, this is a book with unlimited applications for undergraduate and graduate students of mathematics, science and engineering. Professionals and researchers will find it a valuable reference they will turn to again and again.},
  googlebooks = {Z2owE\_0LQukC},
  isbn = {978-0-486-13482-6},
  language = {en},
  keywords = {Mathematics / Mathematical Analysis}
}

@book{hansenTheorySimpleLiquids2013,
  title = {Theory of {{Simple Liquids}}: With {{Applications}} to {{Soft Matter}}},
  shorttitle = {Theory of {{Simple Liquids}}},
  author = {Hansen, Jean-Pierre and McDonald, I. R.},
  year = {2013},
  month = aug,
  publisher = {{Academic Press}},
  abstract = {Comprehensive coverage of topics in the theory of classical liquids Widely regarded as the standard text in its field, Theory of Simple Liquids gives an advanced but self-contained account of liquid state theory within the unifying framework provided by classical statistical mechanics. The structure of this revised and updated Fourth Edition is similar to that of the previous one but there are significant shifts in emphasis and much new material has been added. Major changes and Key Features in content include: Expansion of existing sections on simulation methods, liquid-vapour coexistence, the hierarchical reference theory of criticality, and the dynamics of super-cooled liquids.New sections on binary fluid mixtures, surface tension, wetting, the asymptotic decay of pair correlations, fluids in porous media, the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely new chapter on applications to 'soft matter' of a combination of liquid state theory and coarse graining strategies, with sections on polymer solutions and polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic liquid crystals, colloidal dynamics, and on clustering and gelation. Expansion of existing sections on simulation methods, liquid-vapour coexistence, the hierarchian reference of criticality, and the dynamics of super-cooled liquids.New sections on binary fluid mixtures, surface tension, wetting, the asymptotic decay of pair correlations, fluids in porous media, the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely new chapter on applications to 'soft matter' of a combination of liquid state theory and coarse graining strategies, with sections on polymer solutions and polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic liquid crystals, colloidal dynamics, and on clustering and gelation.},
  googlebooks = {pbJfOUqZVSgC},
  isbn = {978-0-12-387033-9},
  language = {en},
  keywords = {Science / Physics / Condensed Matter}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}, {{Second Edition}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  month = aug,
  publisher = {{Springer Science \& Business Media}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  googlebooks = {tVIjmNS3Ob8C},
  isbn = {978-0-387-84858-7},
  language = {en},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Data Analytics,Computers / Information Technology,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology,Science / Life Sciences / General}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  year = {1991},
  month = jan,
  journal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(91)90009-T},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp({$\mu$}) performance criteria, for arbitrary finite input environment measures {$\mu$}, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  language = {en},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  language = {en},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation}
}

@book{hornMatrixAnalysis2012,
  title = {Matrix {{Analysis}}},
  author = {Horn, Roger A. and Johnson, Charles R.},
  year = {2012},
  month = oct,
  publisher = {{Cambridge University Press}},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  googlebooks = {O7sgAwAAQBAJ},
  isbn = {978-1-139-78888-5},
  language = {en},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Economics / General,Mathematics / Algebra / Abstract,Mathematics / Algebra / General,Mathematics / Geometry / Algebraic}
}

@inproceedings{houghZerosGaussianAnalytic2009,
  title = {Zeros of {{Gaussian Analytic Functions}} and {{Determinantal Point Processes}}},
  booktitle = {University {{Lecture Series}}},
  author = {Hough, J. and Krishnapur, Manjunath and Peres, Y. and Vir{\'a}g, B.},
  year = {2009},
  doi = {10.1090/ULECT/051},
  abstract = {The book examines in some depth two important classes of point processes, determinantal processes and 'Gaussian zeros', i.e., zeros of random analytic functions with Gaussian coefficients. These processes share a property of 'point-repulsion', where distinct points are less likely to fall close to each other than in processes, such as the Poisson process, that arise from independent sampling. Nevertheless, the treatment in the book emphasizes the use of independence: for random power series, the independence of coefficients is key; for determinantal processes, the number of points in a domain is a sum of independent indicators, and this yields a satisfying explanation of the central limit theorem (CLT) for this point count. Another unifying theme of the book is invariance of considered point processes under natural transformation groups. The book strives for balance between general theory and concrete examples. On the one hand, it presents a primer on modern techniques on the interface of probability and analysis. On the other hand, a wealth of determinantal processes of intrinsic interest are analyzed; these arise from random spanning trees and eigenvalues of random matrices, as well as from special power series with determinantal zeros. The material in the book formed the basis of a graduate course given at the IAS-Park City Summer School in 2007; the only background knowledge assumed can be acquired in first-year graduate courses in analysis and probability.}
}

@article{kellerIntegralEquationsMachine2019,
  title = {Integral Equations and Machine Learning},
  author = {Keller, Alexander and Dahm, Ken},
  year = {2019},
  month = jul,
  journal = {Mathematics and Computers in Simulation},
  series = {Special Issue on the {{Eleventh International Conference}} on {{Monte Carlo Methods}} and {{Applications}} ({{MCM}} 2017), Held in {{Montreal}}, {{Canada}}, {{July}} 03-07, 2017},
  volume = {161},
  pages = {2--12},
  issn = {0378-4754},
  doi = {10.1016/j.matcom.2019.01.010},
  abstract = {As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Efficiency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artificial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.},
  language = {en},
  keywords = {Artificial neural networks,Integral equations,Light transport simulation,Monte Carlo and quasi-Monte Carlo methods,Reinforcement learning}
}

@article{kelleyFastSolverOrnstein2004,
  title = {A Fast Solver for the {{Ornstein}}\textendash{{Zernike}} Equations},
  author = {Kelley, C. T. and Pettitt, B. Montgomery},
  year = {2004},
  month = jul,
  journal = {Journal of Computational Physics},
  volume = {197},
  number = {2},
  pages = {491--501},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2003.12.006},
  abstract = {In this paper, we report on the design and analysis of a multilevel method for the solution of the Ornstein\textendash Zernike Equations and related systems of integro-algebraic equations. Our approach is based on an extension of the Atkinson\textendash Brakhage method, with Newton-GMRES used as the coarse mesh solver. We report on several numerical experiments to illustrate the effectiveness of the method. The problems chosen are related to simple short ranged fluids with continuous potentials. Speedups over traditional methods for a given accuracy are reported. The new multilevel method is roughly six times faster than Newton-GMRES and 40 times faster than Picard.},
  language = {en}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{kolafaBridgeFunctionHard2002,
  title = {The Bridge Function of Hard Spheres by Direct Inversion of Computer Simulation Data},
  author = {KOLAFA, JI{\v R}{\'I} and LAB{\'I}K, STANISLAV and MALIJEVSK{\'Y}, ANATOL},
  year = {2002},
  month = aug,
  journal = {Molecular Physics},
  volume = {100},
  number = {16},
  pages = {2629--2640},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268970210136357},
  abstract = {The bridge function of the hard sphere fluid has been calculated from our new highly accurate Monte Carlo and molecular dynamics simulation data on the radial distribution function using the (inverted) Ornstein-Zernike equation. Both the systematic errors (finite size, grid size, tail) and statistical errors are analysed in detail and ways to suppress them are proposed. Uncertainties in the resulting values of B(r) are about 0.001. In contrast with many previous findings the bridge function is both positive and negative.},
  annotation = {\_eprint: https://doi.org/10.1080/00268970210136357}
}

@article{kwakEvaluationBridgefunctionDiagrams2005,
  title = {Evaluation of Bridge-Function Diagrams via {{Mayer}}-Sampling {{Monte Carlo}} Simulation},
  author = {Kwak, Sang Kyu and Kofke, David A.},
  year = {2005},
  month = mar,
  journal = {The Journal of Chemical Physics},
  volume = {122},
  number = {10},
  pages = {104508},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1860559}
}

@article{labikEfficientGaussNewtonlikeMethod1994,
  title = {An {{Efficient Gauss}}-{{Newton}}-like {{Method}} for the {{Numerical Solution}} of the {{Ornstein}}-{{Zernike Integral Equation}} for a {{Class}} of {{Fluid Models}}},
  author = {Lab{\'{\i}}k, Stanislav and Posp{\'{\i}}{\v s}il, Roman and Malijevsk{\'y}, Anatol and Smith, William Robert},
  year = {1994},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {115},
  number = {1},
  pages = {12--21},
  issn = {0021-9991},
  doi = {10.1006/jcph.1994.1174},
  abstract = {A numerical algorithm for solving the Ornstein-Zernike (OZ) integral equation of statistical mechanics is described for the class of fluids composed of molecules with axially symmetric interactions. Since the OZ equation is a nonlinear second-kind Fredholm equation whose key feature for the class of problems of interest is the highly computationally intensive nature of the kernel, the general approach employed in this paper is thus potentially useful for similar problems with this characteristic. The algorithm achieves a high degree of computational efficiency by combining iterative linearization of the most complex portion of the kernel with a combination of Newton-Raphson and Picard iteration methods for the resulting approximate equation. This approach makes the algorithm analogous to the approach of the classical Gauss-Newton method for nonlinear regression, and we call our method the GN algorithm. An example calculation is given illustrating the use of the algorithm for the hard prolate ellipsoid fluid and its results are compared directly with those of the Picard iteration method. The GN algorithm is four to ten times as fast as the Picard method, and we present evidence that it is the most efficient general method currently available.},
  language = {en}
}

@article{labikRapidlyConvergentMethod1985,
  title = {A Rapidly Convergent Method of Solving the {{OZ}} Equation},
  author = {Lab{\'i}k, Stanislav and Malijevsk{\'y}, Anatol and Vo{\v n}ka, Petr},
  year = {1985},
  month = oct,
  journal = {Molecular Physics},
  volume = {56},
  number = {3},
  pages = {709--715},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268978500102651},
  abstract = {A new method is proposed for solving numerically the Ornstein-Zernike equation for systems with a spherically symmetrical pair-potential. The method is based on expansion of the function {$\Gamma$}(r)=r[h(r) - c(r)] in suitable basis functions and on a combination of Newton-Raphson and direct iterations. Tests on the PY and HNC approximations for hard spheres and Lennard-Jones fluid have shown that the proposed method is three to nine times as rapid as the related and so far the most efficient method of Gillan. Other advantages besides the speed are low sensitivity to the choice of initial estimate and a relatively simple computational scheme.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978500102651}
}

@article{llano-restrepoBridgeFunctionCavity1992,
  title = {Bridge Function and Cavity Correlation Function for the {{Lennard}}-{{Jones}} Fluid from Simulation},
  author = {Llano-Restrepo, Mario and Chapman, Walter G.},
  year = {1992},
  month = aug,
  journal = {The Journal of Chemical Physics},
  volume = {97},
  number = {3},
  pages = {2046--2054},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.463142}
}

@article{lombaOrnsteinZernikeEquationsSimulation1993,
  title = {Ornstein-{{Zernike}} Equations and Simulation Results for Hard-Sphere Fluids Adsorbed in Porous Media},
  author = {Lomba, Enrique and Given, James A. and Stell, George and Weis, Jean Jacques and Levesque, Dominique},
  year = {1993},
  month = jul,
  journal = {Physical Review E},
  volume = {48},
  number = {1},
  pages = {233--244},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.48.233},
  abstract = {In this paper we solve the replica Ornstein-Zernike (ROZ) equations in the hypernetted-chain (HNC), Percus-Yevick (PY), and reference Percus-Yevick (RPY) approximations for partly quenched systems. The ROZ equations, which apply to the general class of partly quenched systems, are here applied to a class of models for porous media. These models involve two species of particles: an annealed or equilibrated species, which is used to model the fluid phase, and a quenched or frozen species, whose excluded-volume interactions constitute the matrix in which the fluid is adsorbed. We study two models for the quenched species of particles: a hard-sphere matrix, for which the fluid-fluid, matrix-matrix, and matrix-fluid sphere diameters {$\sigma$}11, {$\sigma$}00, and {$\sigma$}01 are additive, and a matrix of randomly overlapping particles (which still interact with the fluid particle as hard spheres) that gives a ``random'' matrix with interconnected pore structure. For the random-matrix case we study a ratio {$\sigma$}01/{$\sigma$}11 of 2.5, which is a demanding one for the theories. The HNC and RPY results represent significant improvements over the PY result when compared with the Monte Carlo simulations we have generated for this study, with the HNC result yielding the best results overall among those studied. A phenomenological percolating-fluid approximation is also found to be of comparable accuracy to the HNC results over a significant range of matrix and fluid densities. In the hard-sphere matrix case, the RPY is the best of the theories that we have considered.}
}

@article{malijevskyBridgeFunctionHard1987,
  title = {The Bridge Function for Hard Spheres},
  author = {Malijevsk{\'y}, Anatol and Lab{\'i}k, Stanislav},
  year = {1987},
  month = feb,
  journal = {Molecular Physics},
  volume = {60},
  number = {3},
  pages = {663--669},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268978700100441},
  abstract = {The paper presents an empirical formula for expressing the bridge function (the sum of elementary graphs) in terms of the interparticle separation and the density. The formulae is fully consistent with the best computer-simulation thermodynamic and structural data for hard spheres in the fluid region. It can serve as both a direct and convenient testing ground for the integral-equation theories of hard spheres and an input to the reference-hypernetted chain approximation for simple fluids.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978700100441}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, S.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-40065-5},
  abstract = {Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
  isbn = {978-0-387-30303-1},
  language = {en}
}

@inproceedings{parkMinimumWidthUniversal2020,
  title = {Minimum {{Width}} for {{Universal Approximation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Park, Sejun and Yun, Chulhee and Lee, Jaeho and Shin, Jinwoo},
  year = {2020},
  month = sep,
  abstract = {The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width...},
  language = {en}
}

@article{peplowAlgorithmsComputationSolutions2006,
  title = {Algorithms for the Computation of Solutions of the {{Ornstein}}-{{Zernike}} Equation},
  author = {Peplow, A. T. and Beardmore, R. E. and Bresme, F.},
  year = {2006},
  month = oct,
  journal = {Physical Review E},
  volume = {74},
  number = {4},
  pages = {046705},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.74.046705},
  abstract = {We introduce a robust and efficient methodology to solve the Ornstein-Zernike integral equation using the pseudoarc length (PAL) continuation method that reformulates the integral equation in an equivalent but nonstandard form. This enables the computation of solutions in regions where the compressibility experiences large changes or where the existence of multiple solutions and so-called branch points prevents Newton's method from converging. We illustrate the use of the algorithm with a difficult problem that arises in the numerical solution of integral equations, namely the evaluation of the so-called no-solution line of the Ornstein-Zernike hypernetted chain (HNC) integral equation for the Lennard-Jones potential. We are able to use the PAL algorithm to solve the integral equation along this line and to connect physical and nonphysical solution branches (both isotherms and isochores) where appropriate. We also show that PAL continuation can compute solutions within the no-solution region that cannot be computed when Newton and Picard methods are applied directly to the integral equation. While many solutions that we find are new, some correspond to states with negative compressibility and consequently are not physical.}
}

@article{rogersNewThermodynamicallyConsistent1984,
  title = {New, Thermodynamically Consistent, Integral Equation for Simple Fluids},
  author = {Rogers, Forrest J. and Young, David A.},
  year = {1984},
  month = aug,
  journal = {Physical Review A},
  volume = {30},
  number = {2},
  pages = {999--1007},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevA.30.999},
  abstract = {A new integral equation in which the hypernetted-chain and Percus-Yevick approximations are "mixed" as a function of interparticle separation is described. An adjustable parameter {$\alpha$} in the mixing function is used to enforce thermodynamic consistency. For simple 1rn potential fluids, {$\alpha$} is constant for all densities, and the solutions of the integral equations are in very good agreement with Monte Carlo calculations. For the one-component plasma, {$\alpha$} is a slowly varying function of density, but the agreement between calculated solutions and Monte Carlo is also good. This approach has definite advantages over previous thermodynamically consistent equations.}
}

@article{rubinsteinOptimizationComputerSimulation1997,
  title = {Optimization of Computer Simulation Models with Rare Events},
  author = {Rubinstein, Reuven Y.},
  year = {1997},
  month = may,
  journal = {European Journal of Operational Research},
  volume = {99},
  number = {1},
  pages = {89--112},
  issn = {0377-2217},
  doi = {10.1016/S0377-2217(96)00385-2},
  abstract = {Discrete event simulation systems (DESS) are widely used in many diverse areas such as computer-communication networks, flexible manufacturing systems, project evaluation and review techniques (PERT), and flow networks. Because of their complexity, such systems are typically analyzed via Monte Carlo simulation methods. This paper deals with optimization of complex computer simulation models involving rare events. A classic example is to find an optimal (s, S) policy in a multi-item, multicommodity inventory system, when quality standards require the backlog probability to be extremely small. Our approach is based on change of the probability measure techniques, also called likelihood ratio (LR) and importance sampling (IS) methods. Unfortunately, for arbitrary probability measures the LR estimators and the resulting optimal solution often tend to be unstable and may have large variances. Therefore, the choice of the corresponding importance sampling distribution and in particular its parameters in an optimal way is an important task. We consider the case where the IS distribution comes from the same parametric family as the original (true) one and use the stochastic counterpart method to handle simulation based optimization models. More specifically, we use a two-stage procedure: at the first stage we identify (estimate) the optimal parameter vector at the IS distribution, while at the second stage we estimate the optimal solution of the underlying constrained optimization problem. Particular emphasis will be placed on estimation of rare events and on integration of the associated performance function into stochastic optimization programs. Supporting numerical results are provided as well.},
  language = {en},
  keywords = {Inventory,Optimization,Score function,Sensitivity analysis,Simulation}
}

@article{schmidtDistillingFreeFormNatural2009,
  title = {Distilling {{Free}}-{{Form Natural Laws}} from {{Experimental Data}}},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2009},
  month = apr,
  journal = {Science},
  volume = {324},
  number = {5923},
  pages = {81--85},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1165893},
  abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the ``alphabet'' used to describe those systems. An algorithm has been developed to search for natural laws of physics in large data sets. An algorithm has been developed to search for natural laws of physics in large data sets.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  language = {en},
  pmid = {19342586}
}

@article{scholl-paschingerSelfconsistentOrnsteinZernike2003,
  title = {Self-Consistent {{Ornstein}}\textendash{{Zernike}} Approximation for a Binary Symmetric Fluid Mixture},
  author = {{Sch{\"o}ll-Paschinger}, Elisabeth and Kahl, Gerhard},
  year = {2003},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {118},
  number = {16},
  pages = {7414--7424},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1557053}
}

@article{tangAnalyticalSolutionOrnsteinZernike1995,
  title = {Analytical Solution of the {{Ornstein}}-{{Zernike}} Equation for Mixtures},
  author = {Tang, Yiping and Lu, Benjamin C.-Y.},
  year = {1995},
  month = jan,
  journal = {Molecular Physics},
  volume = {84},
  number = {1},
  pages = {89--103},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268979500100061},
  abstract = {Solution of the Ornstein-Zernike equation under the Percus-Yevick or the mean spherical approximation is presented analytically in a matrix form. The new solution is an extension of the general Ornstein-Zernike solution suggested recently for pure fluids. The development is based on further application of the Hilbert transform and multiple-dimensional space analysis. In addition to the potential matrix, only a hard core correlation function matrix and its inverse are involved in the expression. The solution achieved in this work is explicit and is applicable to any arbitrary potential functions with an additive hard core. The first-order solution for two Yukawa mixtures has been compared with the full solution reported in the literature to serve as an example.},
  annotation = {\_eprint: https://doi.org/10.1080/00268979500100061}
}

@article{torrieMonteCarloCalculation1977,
  title = {Monte {{Carlo}} Calculation of y(r) for the Hard-Sphere Fluid},
  author = {Torrie, G. and Patey, G. N.},
  year = {1977},
  month = dec,
  journal = {Molecular Physics},
  volume = {34},
  number = {6},
  pages = {1623--1628},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268977700102821},
  abstract = {The function y(r) = exp {$\beta$}u(r)g(r) is calculated for hard spheres in the region r {$<$} {$\sigma$} using umbrella-sampling Monte Carlo techniques. The resulting values are found to be well represented over the entire range 0 {$<$} r {$<$} {$\sigma$} by a simple function proposed by Grundke and Henderson.},
  annotation = {\_eprint: https://doi.org/10.1080/00268977700102821}
}

@article{tsedneeClosureOrnsteinZernikeEquation2019,
  title = {Closure for the {{Ornstein}}-{{Zernike}} Equation with Pressure and Free Energy Consistency},
  author = {Tsednee, Tsogbayar and Luchko, Tyler},
  year = {2019},
  month = mar,
  journal = {Physical Review E},
  volume = {99},
  number = {3},
  pages = {032130},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.99.032130},
  abstract = {The Ornstein-Zernike (OZ) integral equation theory is a powerful approach to simple liquids due to its low computational cost and the fact that, when combined with an appropriate closure equation, the theory is thermodynamically complete. However, approximate closures proposed to date exhibit pressure or free energy inconsistencies that produce inaccurate or ambiguous results, limiting the usefulness of the Ornstein-Zernike approach. To address this problem, we combine methods to enforce both pressure and free energy consistency to create a new closure approximation and test it for a single-component Lennard-Jones fluid. The closure is a simple power series in the direct and total correlation functions for which we have derived analytical formulas for the excess Helmholtz free energy and chemical potential. These expressions contain a partial molar volumelike term, similar to excess chemical potential correction terms recently developed. Using our bridge approximation, we have calculated the pressure, Helmholtz free energy, and chemical potential for the Lennard-Jones fluid using the Kirkwood charging, thermodynamic integration techniques, and analytic expressions. These results are compared with those from the hypernetted chain equation and the Verlet-modified closure against Monte Carlo and equations-of-state data for reduced densities of {$\rho{_\ast}<$}1 and temperatures of T{${_\ast}$}=1.5, 2.74, and 5. Our closure shows consistency among all thermodynamic paths, except for one expression of the Gibbs-Duhem relation, whereas the hypernetted chain equation and the Verlet-modified closure exhibit consistency between only a few relations. Accuracy of the closure is comparable to the Verlet-modified closure and a significant improvement to results obtained from the hypernetted chain equation.}
}

@article{vompeBridgeFunctionExpansion1994,
  title = {The Bridge Function Expansion and the Self-consistency Problem of the {{Ornstein}}\textendash{{Zernike}} Equation Solution},
  author = {Vompe, A. G. and Martynov, G. A.},
  year = {1994},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {100},
  number = {7},
  pages = {5249--5258},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.467189}
}

@article{zerahEfficientNewtonMethod1985,
  title = {An Efficient Newton's Method for the Numerical Solution of Fluid Integral Equations},
  author = {Zerah, Gilles},
  year = {1985},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {61},
  number = {2},
  pages = {280--285},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(85)90087-7},
  abstract = {We propose a stable, straightforward algorithm for the numerical solution of integral equations for fluid pair distribution functions. The integral equation is not solved by Picard's standard iterative procedure but by Newton's method of solution of non-linear equations. The large matrix appearing in Newton's method is inverted by a conjugate gradient procedure used as a rapidly converging iterative method.},
  language = {en}
}

@article{zerahSelfConsistentIntegral1986,
  title = {Self-consistent Integral Equations for Fluid Pair Distribution Functions: {{Another}} Attempt},
  shorttitle = {Self-consistent Integral Equations for Fluid Pair Distribution Functions},
  author = {Zerah, Gilles and Hansen, Jean-Pierre},
  year = {1986},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {84},
  number = {4},
  pages = {2336--2343},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.450397}
}

@article{zhouUniversalityDeepConvolutional2020,
  title = {Universality of Deep Convolutional Neural Networks},
  author = {Zhou, Ding-Xuan},
  year = {2020},
  month = mar,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {48},
  number = {2},
  pages = {787--794},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2019.06.004},
  abstract = {Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains. Deep neural network architectures and computational issues have been well studied in machine learning. But there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network architectures such as deep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough. This answers an open question in learning theory. Our quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data. Our study also demonstrates the role of convolutions in deep CNNs.},
  language = {en},
  keywords = {Approximation theory,Convolutional neural network,Deep learning,Universality}
}


