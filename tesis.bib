
@article{a.goodallDatadrivenApproximationsBridge2021,
  title = {Data-Driven Approximations to the Bridge Function Yield Improved Closures for the {{Ornstein}}\textendash{{Zernike}} Equation},
  author = {A. Goodall, Rhys E. and A. Lee, Alpha},
  year = {2021},
  journal = {Soft Matter},
  volume = {17},
  number = {21},
  pages = {5393--5400},
  publisher = {{Royal Society of Chemistry}},
  doi = {10.1039/D1SM00402F},
  langid = {english}
}

@book{allenComputerSimulationLiquids2017,
  title = {Computer {{Simulation}} of {{Liquids}}},
  author = {Allen, Michael P. and Tildesley, Dominic J.},
  year = {2017},
  month = aug,
  publisher = {{Oxford University Press}},
  abstract = {This book provides a practical guide to molecular dynamics and Monte Carlo simulation techniques used in the modelling of simple and complex liquids. Computer simulation is an essential tool in studying the chemistry and physics of condensed matter, complementing and reinforcing both experiment and theory. Simulations provide detailed information about structure and dynamics, essential to understand the many fluid systems that play a key role in our daily lives: polymers, gels, colloidal suspensions, liquid crystals, biological membranes, and glasses. The second edition of this pioneering book aims to explain how simulation programs work, how to use them, and how to interpret the results, with examples of the latest research in this rapidly evolving field. Accompanying programs in Fortran and Python provide practical, hands-on, illustrations of the ideas in the text.},
  googlebooks = {WFExDwAAQBAJ},
  isbn = {978-0-19-252470-6},
  langid = {english},
  keywords = {Computers / Computer Simulation,Science / Mechanics / Fluids,Science / Physics / General}
}

@article{antaFastMethodSolving1995,
  title = {A Fast Method of Solving the Hypernetted-Chain Equation for Molecular {{Lennard}}-{{Jones}} Fluids},
  author = {Anta, J. A. and Lomba, E. and Mart{\'i}n, C. and Lombardero, M. and Lado, F.},
  year = {1995},
  month = mar,
  journal = {Molecular Physics},
  volume = {84},
  number = {4},
  pages = {743--755},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268979500100511},
  abstract = {A fast and stable procedure for solving hypernetted-chain type integral equations for molecular Lennard-Jones fluids is presented. The method is a hybrid algorithm based on the combination of multidimensional angular integration of the closure relation and a linearization technique devised by Fries and Patey (1985, Molec. Phys., 55, 751). The combination of the two techniques leads to a remarkable reduction in the CPU time required to evaluate the closure relation in these systems, which is usually the most time-consuming task. As an application of the method, phase coexistence curves have been calculated for two-centre Lennard-Jones fluids with and without point dipoles.},
  annotation = {\_eprint: https://doi.org/10.1080/00268979500100511}
}

@inproceedings{APSAPSMarcha,
  title = {{{APS}} -{{APS March Meeting}} 2020 - {{Event}} - {{Inverse Design}} of {{Soft Materials}}: Crystals, {{Quasi Crystals}}, {{Liquid Crystals}}},
  shorttitle = {{{APS}} -{{APS March Meeting}} 2020 - {{Event}} - {{Inverse Design}} of {{Soft Materials}}},
  booktitle = {Bulletin of the {{American Physical Society}}},
  volume = {Volume 65, Number 1},
  publisher = {{American Physical Society}}
}

@article{asadyUtilizingArtificialNeural2014,
  title = {Utilizing Artificial Neural Network Approach for Solving Two-Dimensional Integral Equations},
  author = {Asady, B. and Hakimzadegan, F. and Nazarlue, R.},
  year = {2014},
  month = apr,
  journal = {Mathematical Sciences},
  volume = {8},
  number = {1},
  pages = {117},
  issn = {2251-7456},
  doi = {10.1007/s40096-014-0117-6},
  abstract = {This paper surveys the artificial neural networks approach. Researchers believe that these networks have the wide range of applicability, they can treat complicated problems as well. The work described here discusses an efficient computational method that can treat complicated problems. The paper intends to introduce an efficient computational method which can be applied to approximate solution of the linear two-dimensional Fredholm integral equation of the second kind. For this aim, a perceptron model based on artificial neural networks is introduced. At first, the unknown bivariate function is replaced by a multilayer perceptron neural net and also a cost function to be minimized is defined. Then a famous learning technique, namely, the steepest descent method, is employed to adjust the parameters (the weights and biases) to optimize their behavior. The article also examines application of the method which turns to be so accurate and efficient. It concludes with a survey of an example in order to investigate the accuracy of the proposed method.},
  langid = {english}
}

@article{baezUsingSecondVirial2018,
  title = {Using the Second Virial Coefficient as Physical Criterion to Map the Hard-Sphere Potential onto a Continuous Potential},
  author = {B{\'a}ez, C{\'e}sar Alejandro and {Torres-Carbajal}, Alexis and {Casta{\~n}eda-Priego}, Ram{\'o}n and {Villada-Balbuena}, Alejandro and {M{\'e}ndez-Alcaraz}, Jos{\'e} Miguel and {Herrera-Velarde}, Salvador},
  year = {2018},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {149},
  number = {16},
  pages = {164907},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.5049568}
}

@article{beardmoreNumericalBifurcationAnalysis2007,
  title = {A {{Numerical Bifurcation Analysis}} of the {{Ornstein}}\textendash{{Zernike Equation}} with {{Hypernetted Chain Closure}}},
  author = {Beardmore, R. E. and Peplow, A. T. and Bresme, F.},
  year = {2007},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {29},
  number = {6},
  pages = {2442--2463},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/060650659},
  abstract = {We study the codimension-one and -two bifurcations of the Ornstein\textendash Zernike equation with hypernetted chain (HNC) closure with Lennard\textendash Jones intermolecular interaction potential. The main purpose of the paper is to present the results of a numerical study undertaken using a suite of algorithms implemented in MATLAB and based on pseudo arc-length continuation for the codimension-one case and a Newton-GMRES method for the codimension-two case. Through careful consideration of the results of our computations, an argument is formulated which shows that spinodal isothermal solution branches arising in this model cannot be reproduced numerically. Furthermore, we show that the existence of an upper bound on the density that can be realized on a vapor isothermal solution branch, which must be present at a spinodal, causes the existence of at least one fold bifurcation along that vapor branch when density is used as the bifurcation parameter. This provides an explanation for previous inconclusive attempts to compute solutions using Newton\textendash Picard methods that are popular in the physical chemistry literature.}
}

@article{bedollaMachineLearningCondensed2020,
  title = {Machine Learning for Condensed Matter Physics},
  author = {Bedolla, Edwin and Padierna, Luis Carlos and {Casta{\~n}eda-Priego}, Ram{\'o}n},
  year = {2020},
  month = nov,
  journal = {Journal of Physics: Condensed Matter},
  volume = {33},
  number = {5},
  pages = {053001},
  publisher = {{IOP Publishing}},
  issn = {0953-8984},
  doi = {10.1088/1361-648X/abb895},
  abstract = {Condensed matter physics (CMP) seeks to understand the microscopic interactions of matter at the quantum and atomistic levels, and describes how these interactions result in both mesoscopic and macroscopic properties. CMP overlaps with many other important branches of science, such as chemistry, materials science, statistical physics, and high-performance computing. With the advancements in modern machine learning (ML) technology, a keen interest in applying these algorithms to further CMP research has created a compelling new area of research at the intersection of both fields. In this review, we aim to explore the main areas within CMP, which have successfully applied ML techniques to further research, such as the description and use of ML schemes for potential energy surfaces, the characterization of topological phases of matter in lattice systems, the prediction of phase transitions in off-lattice and atomistic simulations, the interpretation of ML theories with physics-inspired frameworks and the enhancement of simulation methods with ML algorithms. We also discuss in detail the main challenges and drawbacks of using ML methods on CMP problems, as well as some perspectives for future developments.},
  langid = {english}
}

@article{behlerGeneralizedNeuralNetworkRepresentation2007a,
  title = {Generalized {{Neural}}-{{Network Representation}} of {{High}}-{{Dimensional Potential}}-{{Energy Surfaces}}},
  author = {Behler, J{\"o}rg and Parrinello, Michele},
  year = {2007},
  month = apr,
  journal = {Physical Review Letters},
  volume = {98},
  number = {14},
  pages = {146401},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.98.146401},
  abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems.}
}

@article{behlerPerspectiveMachineLearning2016a,
  title = {Perspective: Machine Learning Potentials for Atomistic Simulations},
  shorttitle = {Perspective},
  author = {Behler, J{\"o}rg},
  year = {2016},
  month = nov,
  journal = {The Journal of Chemical Physics},
  volume = {145},
  number = {17},
  pages = {170901},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.4966192},
  abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.}
}

@article{bianchiPredictingPatchyParticle2012,
  title = {Predicting Patchy Particle Crystals: Variable Box Shape Simulations and Evolutionary Algorithms},
  shorttitle = {Predicting Patchy Particle Crystals},
  author = {Bianchi, Emanuela and Doppelbauer, G{\"u}nther and Filion, Laura and Dijkstra, Marjolein and Kahl, Gerhard},
  year = {2012},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {136},
  number = {21},
  pages = {214102},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.4722477}
}

@article{boattiniAveragingLocalStructure2021,
  title = {Averaging {{Local Structure}} to {{Predict}} the {{Dynamic Propensity}} in {{Supercooled Liquids}}},
  author = {Boattini, Emanuele and Smallenburg, Frank and Filion, Laura},
  year = {2021},
  month = aug,
  journal = {Physical Review Letters},
  volume = {127},
  number = {8},
  pages = {088007},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.127.088007},
  abstract = {Predicting the local dynamics of supercooled liquids based purely on local structure is a key challenge in our quest for understanding glassy materials. Recent years have seen an explosion of methods for making such a prediction, often via the application of increasingly complex machine learning techniques. The best predictions so far have involved so-called Graph Neural Networks (GNNs) whose accuracy comes at a cost of models that involve on the order of 105 fit parameters. In this Letter, we propose that the key structural ingredient to the GNN method is its ability to consider not only the local structure around a central particle, but also averaged structural features centered around nearby particles. We demonstrate that this insight can be exploited to design a significantly more efficient model that provides essentially the same predictive power at a fraction of the computational complexity (approximately 1000 fit parameters), and demonstrate its success by fitting the dynamic propensity of Kob-Andersen and binary hard-sphere mixtures. We then use this to make predictions regarding the importance of radial and angular descriptors in the dynamics of both models.}
}

@article{boattiniUnsupervisedLearningLocal2019a,
  title = {Unsupervised Learning for Local Structure Detection in Colloidal Systems},
  author = {Boattini, Emanuele and Dijkstra, Marjolein and Filion, Laura},
  year = {2019},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {151},
  number = {15},
  pages = {154901},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.5118867}
}

@article{bomontRenormalizationIndirectCorrelation2001,
  title = {Renormalization of the Indirect Correlation Function to Extract the Bridge Function of Simple Fluids},
  author = {Bomont, J. M. and Bretonnet, J. L.},
  year = {2001},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {114},
  number = {9},
  pages = {4141--4148},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1344610}
}

@article{boothEfficientSolutionLiquid1999,
  title = {Efficient Solution of Liquid State Integral Equations Using the {{Newton}}-{{GMRES}} Algorithm},
  author = {Booth, Michael J. and Schlijper, A. G. and Scales, L. E. and Haymet, A. D. J.},
  year = {1999},
  month = jun,
  journal = {Computer Physics Communications},
  volume = {119},
  number = {2},
  pages = {122--134},
  issn = {0010-4655},
  doi = {10.1016/S0010-4655(99)00186-1},
  abstract = {We present examples of the accurate, robust and efficient solution of Ornstein-Zernike type integral equations which describe the structure of both homogeneous and inhomogeneous fluids. In this work we use the Newton-GMRES algorithm as implemented in the public-domain nonlinear Krylov solvers NKSOL [P. Brown, Y. Saad, SIAM J. Sci. Stat. Comput. 11 (1990) 450] and NITSOL [M. Pernice, H.F. Walker, SIAM J. Sci. Comput. 19 (1998) 302]. We compare and contrast this method with more traditional approaches in the literature, using Picard iteration (successive-substitution) and hybrid Newton-Raphson and Picard methods, and a recent vector extrapolation method [H.H.H. Homeier, S. Rast, H. Krienke, Comput. Phys. Commun. 92 (1995) 188]. We find that both the performance and ease of implementation of these nonlinear solvers recommend them for the solution of this class of problem.},
  langid = {english},
  keywords = {GMRES,Inhomogeneous fluids,Krylov,Newton,Nonlinear integral equation,Numerical solution,Ornstein,Raphson,Zernike}
}

@article{carleoMachineLearningPhysical2019a,
  title = {Machine Learning and the Physical Sciences},
  author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and {Vogt-Maranto}, Leslie and Zdeborov{\'a}, Lenka},
  year = {2019},
  month = dec,
  journal = {Reviews of Modern Physics},
  volume = {91},
  number = {4},
  pages = {045002},
  publisher = {{American Physical Society}},
  doi = {10.1103/RevModPhys.91.045002},
  abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.}
}

@article{carrasquillaMachineLearningPhases2017a,
  title = {Machine Learning Phases of Matter},
  author = {Carrasquilla, Juan and Melko, Roger G.},
  year = {2017},
  month = may,
  journal = {Nature Physics},
  volume = {13},
  number = {5},
  pages = {431--434},
  publisher = {{Nature Publishing Group}},
  issn = {1745-2481},
  doi = {10.1038/nphys4035},
  abstract = {The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order.},
  copyright = {2017 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Phase transitions and critical phenomena;Statistical physics Subject\_term\_id: phase-transitions-and-critical-phenomena;statistical-physics}
}

@article{carvalhoIndirectSolutionOrnsteinZernike2020,
  title = {Indirect {{Solution}} of {{Ornstein}}-{{Zernike Equation Using}} the {{Hopfield Neural Network Method}}},
  author = {Carvalho, F. S. and Braga, J. P.},
  year = {2020},
  month = oct,
  journal = {Brazilian Journal of Physics},
  volume = {50},
  number = {5},
  pages = {489--494},
  issn = {1678-4448},
  doi = {10.1007/s13538-020-00769-4},
  abstract = {Microscopic information, such as the pair distribution and direct correlation functions, can be obtained from experimental data. From these correlation functions, thermodynamical quantities and the potential interaction function can be recovered. Derivations of Ornstein-Zernike equation and Hopfield Neural Network method are given first, as a theoretical background to follow the present work. From these two frameworks, structural information, such as the radial distribution (g(r)) and direct correlation (C(r)) functions, were retrieved from neutron scattering experimental data. The problem was solved considering simple initial conditions, which does not require any previous information about the system, making it clear the robustness of the Hopfield Neural Network method. The pair interaction potential was estimated in the Percus-Yevick (PY) and hypernetted chain (HNC) approximations and a poor agreement, compared with the Lennard-Jones 6-12 potential, was observed for both cases, suggesting the necessity of a more accurate closure relation to describe the system. In this sense, the Hopfield Neural Network together with experimental information provides an alternative approach to solve the Ornstein-Zernike equations, avoiding the limitations imposed by the closure relation.},
  langid = {english}
}

@article{catsMachinelearningFreeenergyFunctionals2021,
  title = {Machine-Learning Free-Energy Functionals Using Density Profiles from Simulations},
  author = {Cats, Peter and Kuipers, Sander and {de Wind}, Sacha and {van Damme}, Robin and Coli, Gabriele M. and Dijkstra, Marjolein and {van Roij}, Ren{\'e}},
  year = {2021},
  month = mar,
  journal = {APL Materials},
  volume = {9},
  number = {3},
  pages = {031109},
  publisher = {{American Institute of Physics}},
  doi = {10.1063/5.0042558},
  abstract = {The formally exact framework of equilibrium Density Functional Theory (DFT) is capable of simultaneously and consistently describing thermodynamic and structural properties of interacting many-body systems in arbitrary external potentials. In practice, however, DFT hinges on approximate (free-)energy functionals from which density profiles (and hence the thermodynamic potential) follow via an Euler\textendash Lagrange equation. Here, we explore a relatively simple Machine-Learning (ML) approach to improve the standard mean-field approximation of the excess Helmholtz free-energy functional of a 3D Lennard-Jones system at a supercritical temperature. The learning set consists of density profiles from grand-canonical Monte Carlo simulations of this system at varying chemical potentials and external potentials in a planar geometry only. Using the DFT formalism, we nevertheless can extract not only very accurate 3D bulk equations of state but also radial distribution functions using the Percus test-particle method. Unfortunately, our ML approach did not provide very reliable Ornstein\textendash Zernike direct correlation functions for small distances.}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english}
}

@article{degennesSoftMatter1992,
  title = {Soft Matter},
  author = {{de Gennes}, P. G.},
  year = {1992},
  month = jul,
  journal = {Reviews of Modern Physics},
  volume = {64},
  number = {3},
  pages = {645--648},
  publisher = {{American Physical Society}},
  doi = {10.1103/RevModPhys.64.645},
  abstract = {DOI:https://doi.org/10.1103/RevModPhys.64.645}
}

@article{dijkstraPhaseDiagramHighly1999,
  title = {Phase Diagram of Highly Asymmetric Binary Hard-Sphere Mixtures},
  author = {Dijkstra, Marjolein and {van Roij}, Ren{\'e} and Evans, Robert},
  year = {1999},
  month = may,
  journal = {Physical Review E},
  volume = {59},
  number = {5},
  pages = {5744--5771},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.59.5744},
  abstract = {We study the phase behavior and structure of highly asymmetric binary hard-sphere mixtures. By first integrating out the degrees of freedom of the small spheres in the partition function we derive a formal expression for the effective Hamiltonian of the large spheres. Then using an explicit pairwise (depletion) potential approximation to this effective Hamiltonian in computer simulations, we determine fluid-solid coexistence for size ratios q=0.033,0.05,0.1,0.2, and 1.0. The resulting two-phase region becomes very broad in packing fractions of the large spheres as q becomes very small. We find a stable, isostructural solid-solid transition for q{$<\sptilde$}0.05 and a fluid-fluid transition for q{$<\sptilde$}0.10. However, the latter remains metastable with respect to the fluid-solid transition for all size ratios we investigate. In the limit \textrightarrow q0 the phase diagram mimics that of the sticky-sphere system. As expected, the radial distribution function g(r) and the structure factor S(k) of the effective one-component system show no sharp signature of the onset of the freezing transition and we find that at most points on the fluid-solid boundary the value of S(k) at its first peak is much lower than the value given by the Hansen-Verlet freezing criterion. Direct simulations of the true binary mixture of hard spheres were performed for q{$>\sptilde$}0.05 in order to test the predictions from the effective Hamiltonian. For those packing fractions of the small spheres where direct simulations are possible, we find remarkably good agreement between the phase boundaries calculated from the two approaches\textemdash even up to the symmetric limit q=1 and for very high packings of the large spheres, where the solid-solid transition occurs. In both limits one might expect that an approximation which neglects higher-body terms should fail, but our results support the notion that the main features of the phase equilibria of asymmetric binary hard-sphere mixtures are accounted for by the effective pairwise depletion potential description. We also compare our results with those of other theoretical treatments and experiments on colloidal hard-sphere mixtures.}
}

@article{dijkstraPredictiveModellingMachine2021a,
  title = {From Predictive Modelling to Machine Learning and Reverse Engineering of Colloidal Self-Assembly},
  author = {Dijkstra, Marjolein and Luijten, Erik},
  year = {2021},
  month = jun,
  journal = {Nature Materials},
  volume = {20},
  number = {6},
  pages = {762--773},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4660},
  doi = {10.1038/s41563-021-01014-2},
  abstract = {An overwhelming diversity of colloidal building blocks with distinct sizes, materials and tunable interaction potentials are now available for colloidal self-assembly. The application space for materials composed of these building blocks is vast. To make progress in the rational design of new self-assembled materials, it is desirable to guide the experimental synthesis efforts by computational modelling. Here, we discuss computer simulation methods and strategies used for the design of soft materials created through bottom-up self-assembly of colloids and nanoparticles. We describe simulation techniques for investigating the self-assembly behaviour of colloidal suspensions, including crystal structure prediction methods, phase diagram calculations and enhanced sampling techniques, as well as their limitations. We also discuss the recent surge of interest in machine learning and reverse-engineering methods. Although their implementation in the colloidal realm is still in its infancy, we anticipate that these data-science tools offer new paradigms in understanding, predicting and (inverse) design of novel colloidal materials.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Materials science;Nanoscale materials;Statistical physics, thermodynamics and nonlinear dynamics Subject\_term\_id: materials-science;nanoscale-materials;statistical-physics-thermodynamics-and-nonlinear-dynamics}
}

@article{dunjkoMachineLearningArtificial2018,
  title = {Machine Learning \& Artificial Intelligence in the Quantum Domain: A Review of Recent Progress},
  shorttitle = {Machine Learning \& Artificial Intelligence in the Quantum Domain},
  author = {Dunjko, Vedran and Briegel, Hans J.},
  year = {2018},
  month = jun,
  journal = {Reports on Progress in Physics},
  volume = {81},
  number = {7},
  pages = {074001},
  publisher = {{IOP Publishing}},
  issn = {0034-4885},
  doi = {10.1088/1361-6633/aab406},
  abstract = {Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research\textemdash quantum information versus machine learning (ML) and artificial intelligence (AI)\textemdash have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our `big data' world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement\textemdash exploring what ML/AI can do for quantum physics and vice versa\textemdash researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.},
  langid = {english}
}

@article{edelmanHowManyZeros1995,
  title = {How Many Zeros of a Random Polynomial Are Real?},
  author = {Edelman, Alan and Kostlan, Eric},
  year = {1995},
  month = jan,
  journal = {Bulletin of the American Mathematical Society},
  volume = {32},
  number = {1},
  pages = {1--38},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-1995-00571-9},
  abstract = {We provide an elementary geometric derivation of the Kac integral formula for the expected number of real zeros of a random polynomial with independent standard normally distributed coefficients. We show that the expected number of real zeros is simply the length of the moment curve (1, t, ... , t") projected onto the surface of the unit sphere, divided by n . The probability density of the real zeros is proportional to how fast this curve is traced out. We then relax Kac's assumptions by considering a variety of random sums, series, and distributions, and we also illustrate such ideas as integral geometry and the Fubini-Study metric.},
  langid = {english}
}

@book{egelstaffIntroductionLiquidState2012,
  title = {An {{Introduction}} to the {{Liquid State}}},
  author = {Egelstaff, P.},
  year = {2012},
  month = dec,
  publisher = {{Elsevier}},
  abstract = {An Introduction to the Liquid State focuses on the atomic motions and positions of liquids. Particularly given importance in this book are internal motion of molecules as a whole and the motion of atoms in a monatomic liquid. Divided into 16 chapters, the book opens by outlining the general properties of liquids, including a comparison of liquid argon and liquid sodium, discussions on theories and methods of studying the liquid state, and thermodynamic relationships. The book proceeds by defining the molecular distribution functions and equation of state, the potential function for non-conducting liquids and metals, and measurement of pair distribution function. Numerical analyses and representations are provided to simplify the functions of equations. The book discusses equilibrium properties wherein calculations on the state of gases and fluids are presented. The text also underlines space and time dependent correlation functions. Given emphasis in this part are neutron scattering, electromagnetic radiation, and various radiation scattering techniques. Other concerns discussed are diffusion and single particle motion, velocity of correlation function, diffusion and viscosity coefficients, liquid-gas critical point, and a comparison of classical and quantum liquids. The selection is a valuable source of information for readers wanting to study the composition and reactions of liquids.},
  isbn = {978-0-323-15903-6},
  langid = {english},
  keywords = {Science / Physics / General}
}

@article{evansSimpleLiquidsColloids2019,
  title = {From Simple Liquids to Colloids and Soft Matter},
  author = {Evans, Robert and Frenkel, Daan and Dijkstra, Marjolein},
  year = {2019},
  month = feb,
  journal = {Physics Today},
  volume = {72},
  number = {2},
  pages = {38--39},
  publisher = {{American Institute of Physics}},
  issn = {0031-9228},
  doi = {10.1063/PT.3.4135}
}

@book{frenkelUnderstandingMolecularSimulation2001,
  title = {Understanding {{Molecular Simulation}}: From {{Algorithms}} to {{Applications}}},
  shorttitle = {Understanding {{Molecular Simulation}}},
  author = {Frenkel, Daan and Smit, B.},
  year = {2001},
  month = oct,
  publisher = {{Elsevier}},
  abstract = {Understanding Molecular Simulation: From Algorithms to Applications explains the physics behind the \&quot;recipes\&quot; of molecular simulation for materials science. Computer simulators are continuously confronted with questions concerning the choice of a particular technique for a given application. A wide variety of tools exist, so the choice of technique requires a good understanding of the basic principles. More importantly, such understanding may greatly improve the efficiency of a simulation program. The implementation of simulation methods is illustrated in pseudocodes and their practical use in the case studies used in the text. Since the first edition only five years ago, the simulation world has changed significantly -- current techniques have matured and new ones have appeared. This new edition deals with these new developments; in particular, there are sections on:  Transition path sampling and diffusive barrier crossing to simulaterare eventsDissipative particle dynamic as a course-grained simulation techniqueNovel schemes to compute the long-ranged forcesHamiltonian and non-Hamiltonian dynamics in the context constant-temperature and constant-pressure molecular dynamics simulationsMultiple-time step algorithms as an alternative for constraintsDefects in solidsThe pruned-enriched Rosenbluth sampling, recoil-growth, and concerted rotations for complex moleculesParallel tempering for glassy Hamiltonians Examples are included that highlight current applications and the codes of case studies are available on the World Wide Web. Several new examples have been added since the first edition to illustrate recent applications. Questions are included in this new edition. No prior knowledge of computer simulation is assumed.},
  googlebooks = {5qTzldS9ROIC},
  isbn = {978-0-08-051998-2},
  langid = {english},
  keywords = {Computers / Design; Graphics \& Media / Graphics Tools,Science / Physics / Atomic \& Molecular,Science / Physics / Mathematical \& Computational,Technology \& Engineering / Materials Science / General}
}

@article{gaoImplementingNelderMeadSimplex2012,
  title = {Implementing the {{Nelder}}-{{Mead}} Simplex Algorithm with~Adaptive Parameters},
  author = {Gao, Fuchang and Han, Lixing},
  year = {2012},
  month = jan,
  journal = {Computational Optimization and Applications},
  volume = {51},
  number = {1},
  pages = {259--277},
  issn = {1573-2894},
  doi = {10.1007/s10589-010-9329-3},
  abstract = {In this paper, we first prove that the expansion and contraction steps of the Nelder-Mead simplex algorithm possess a descent property when the objective function is uniformly convex. This property provides some new insights on why the standard Nelder-Mead algorithm becomes inefficient in high dimensions. We then propose an implementation of the Nelder-Mead method in which the expansion, contraction, and shrink parameters depend on the dimension of the optimization problem. Our numerical experiments show that the new implementation outperforms the standard Nelder-Mead method for high dimensional problems.},
  langid = {english}
}

@article{gelbartNewScienceComplex1996,
  title = {The ``{{New}}'' {{Science}} of ``{{Complex Fluids}}''},
  author = {Gelbart, William M. and {Ben-Shaul}, Avinoam},
  year = {1996},
  month = jan,
  journal = {The Journal of Physical Chemistry},
  volume = {100},
  number = {31},
  pages = {13169--13189},
  publisher = {{American Chemical Society}},
  issn = {0022-3654},
  doi = {10.1021/jp9606570},
  abstract = {We present an overview of the modern study of complex fluids which, because of the overwhelming breadth and richness of this field, unavoidably neglects many interesting systems and research developments. In proposing a definition of the field, we discuss first the special role played by phenomenological theory and the limitations of molecular-level description. The remainder of the article is organized into sections which treat model colloids, micellized surfactant solutions, interfacial films and microemulsions, bilayers and membranes, and new materials. In each instance we try to provide a physical basis for the special nature of interactions and long-range ordering transitions in these novel colloidal and thin layer systems. At the heart of understanding these highly varied phenomena lie the curvature dependence of surface energies and the coupling between self-assembly on small length scales and phase changes at large ones.}
}

@book{gibbsElementaryPrinciplesStatistical2014,
  title = {Elementary {{Principles}} in {{Statistical Mechanics}}},
  author = {Gibbs, J. Willard},
  year = {2014},
  month = dec,
  publisher = {{Courier Corporation}},
  abstract = {Written by J. Willard Gibbs, the most distinguished American mathematical physicist of the nineteenth century, this book was the first to bring together and arrange in logical order the works of Clausius, Maxwell, Boltzmann, and Gibbs himself. The lucid, advanced-level text remains a valuable collection of fundamental equations and principles. Topics include the general problem and the fundamental equation of statistical mechanics, the canonical distribution of the average energy values in a canonical ensemble of systems, and formulas for evaluating important functions of the energies of a system. Additional discussions cover maximum and minimal properties of distribution in phase, a valuable comparison of statistical mechanics with thermodynamics, and many other subjects.},
  googlebooks = {tB15BAAAQBAJ},
  isbn = {978-0-486-78995-8},
  langid = {english},
  keywords = {History / General,Science / Mechanics / General,Science / Physics / General}
}

@article{gillanNewMethodSolving1979,
  title = {A New Method of Solving the Liquid Structure Integral Equations},
  author = {Gillan, M. J.},
  year = {1979},
  month = dec,
  journal = {Molecular Physics},
  volume = {38},
  number = {6},
  pages = {1781--1794},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268977900102861},
  abstract = {We present a new method of obtaining numerical solutions to the Percus-Yevick and hypernetted chain equations for liquid structure. The method, which is rapidly convergent and very stable, is a hybrid of the traditional iterative scheme and the Newton-Raphson technique. We show by numerical tests for typical potentials that the method gives well-converged solutions in 20 or 30 iterations even for very high densities. The number of iterations needed is insensitive to the choice of initial estimate, even if this is extremely inaccurate.},
  annotation = {\_eprint: https://doi.org/10.1080/00268977900102861}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
  langid = {english}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
  langid = {english}
}

@article{goodallDatadrivenApproximationsBridge2021,
  title = {Data-Driven Approximations to the Bridge Function Yield Improved Closures for the {{Ornstein}}\textendash{{Zernike}} Equation},
  author = {Goodall, Rhys E. A. and Lee, Alpha A.},
  year = {2021},
  month = jun,
  journal = {Soft Matter},
  volume = {17},
  number = {21},
  pages = {5393--5400},
  publisher = {{The Royal Society of Chemistry}},
  issn = {1744-6848},
  doi = {10.1039/D1SM00402F},
  abstract = {A key challenge for soft materials design and coarse-graining simulations is determining interaction potentials between components that give rise to desired condensed-phase structures. In theory, the Ornstein\textendash Zernike equation provides an elegant framework for solving this inverse problem. Pioneering work in liquid state theory derived analytical closures for the framework. However, these analytical closures are approximations, valid only for specific classes of interaction potentials. In this work, we combine the physics of liquid state theory with machine learning to infer a closure directly from simulation data. The resulting closure is more accurate than commonly used closures across a broad range of interaction potentials.},
  langid = {english}
}

@article{goodallInferenceUniversalOrnsteinZernike,
  title = {Inference of a {{Universal Ornstein}}-{{Zernike Closure Relationship}} with {{Machine Learning}}},
  author = {Goodall, Rhys E A and Lee, Alpha A},
  pages = {6},
  abstract = {The Ornstein-Zernike framework provides an elegant route for solving the inverse problem of determining a pairwise interaction potential for a liquid given its structure. However, in order to realise the potential of the formalism superior closure relationships are required. Current approximate closure relationships have been shown to have restricted universality and give rise to thermodynamic inconsistencies. In this work rather than attempting to analytically derive a new closure relationship we return to the point of the approximation and investigate whether machine learning can be used to infer a universal closure for the framework directly from simulation data. We show preliminary results that indicate this is a fruitful approach and identify areas for further work.},
  langid = {english}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  googlebooks = {omivDQAAQBAJ},
  isbn = {978-0-262-33737-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science}
}

@article{grzybowskiSelfassemblyCrystalsCells2009,
  title = {Self-Assembly: From Crystals to Cells},
  shorttitle = {Self-Assembly},
  author = {Grzybowski, Bartosz A. and Wilmer, Christopher E. and Kim, Jiwon and Browne, Kevin P. and Bishop, Kyle J. M.},
  year = {2009},
  month = mar,
  journal = {Soft Matter},
  volume = {5},
  number = {6},
  pages = {1110--1128},
  publisher = {{The Royal Society of Chemistry}},
  issn = {1744-6848},
  doi = {10.1039/B819321P},
  abstract = {Self-assembly (SA) is the process in which a system's components\textemdash be it molecules, polymers, colloids, or macroscopic particles\textemdash organize into ordered and/or functional structures without human intervention. The main challenge in SA research is the ability to ``program'' the properties of the individual pieces such that they organize into a desired structure. Although a general strategy for doing so is still elusive, heuristic rules can be formulated that guide design of SA under various conditions and thermodynamic constraints. This Review examines SA in both the equilibrium and non-equilibrium/dynamic systems and discusses different SA modalities: energy driven, entropy-driven, templated, and field-directed. Non-equilibrium SA is discussed as a route to reconfigurable (``adaptive'') materials, and its connection to biological systems is emphasized.},
  langid = {english}
}

@book{hammingNumericalMethodsScientists2012,
  title = {Numerical {{Methods}} for {{Scientists}} and {{Engineers}}},
  author = {Hamming, Richard},
  year = {2012},
  month = apr,
  publisher = {{Courier Corporation}},
  abstract = {Numerical analysis is a subject of extreme interest to mathematicians and computer scientists, who will welcome this first inexpensive paperback edition of a groundbreaking classic text on the subject. In an introductory chapter on numerical methods and their relevance to computing, well-known mathematician Richard Hamming (\&quot;the Hamming code,\&quot; \&quot;the Hamming distance,\&quot; and \&quot;Hamming window,\&quot; etc.), suggests that the purpose of computing is insight, not merely numbers. In that connection he outlines five main ideas that aim at producing meaningful numbers that will be read and used, but will also lead to greater understanding of how the choice of a particular formula or algorithm influences not only the computing but our understanding of the results obtained.The five main ideas involve (1) insuring that in computing there is an intimate connection between the source of the problem and the usability of the answers (2) avoiding isolated formulas and algorithms in favor of a systematic study of alternate ways of doing the problem (3) avoidance of roundoff (4) overcoming the problem of truncation error (5) insuring the stability of a feedback system.In this second edition, Professor Hamming (Naval Postgraduate School, Monterey, California) extensively rearranged, rewrote and enlarged the material. Moreover, this book is unique in its emphasis on the frequency approach and its use in the solution of problems. Contents include:I. Fundamentals and AlgorithmsII. Polynomial Approximation- Classical TheoryIll. Fourier Approximation- Modern TheoryIV. Exponential Approximation ... and moreHighly regarded by experts in the field, this is a book with unlimited applications for undergraduate and graduate students of mathematics, science and engineering. Professionals and researchers will find it a valuable reference they will turn to again and again.},
  googlebooks = {Z2owE\_0LQukC},
  isbn = {978-0-486-13482-6},
  langid = {english},
  keywords = {Mathematics / Mathematical Analysis}
}

@book{hansenTheorySimpleLiquids2013,
  title = {Theory of {{Simple Liquids}}: With {{Applications}} to {{Soft Matter}}},
  shorttitle = {Theory of {{Simple Liquids}}},
  author = {Hansen, Jean-Pierre and McDonald, I. R.},
  year = {2013},
  month = aug,
  publisher = {{Academic Press}},
  abstract = {Comprehensive coverage of topics in the theory of classical liquids Widely regarded as the standard text in its field, Theory of Simple Liquids gives an advanced but self-contained account of liquid state theory within the unifying framework provided by classical statistical mechanics. The structure of this revised and updated Fourth Edition is similar to that of the previous one but there are significant shifts in emphasis and much new material has been added. Major changes and Key Features in content include: Expansion of existing sections on simulation methods, liquid-vapour coexistence, the hierarchical reference theory of criticality, and the dynamics of super-cooled liquids.New sections on binary fluid mixtures, surface tension, wetting, the asymptotic decay of pair correlations, fluids in porous media, the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely new chapter on applications to 'soft matter' of a combination of liquid state theory and coarse graining strategies, with sections on polymer solutions and polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic liquid crystals, colloidal dynamics, and on clustering and gelation. Expansion of existing sections on simulation methods, liquid-vapour coexistence, the hierarchian reference of criticality, and the dynamics of super-cooled liquids.New sections on binary fluid mixtures, surface tension, wetting, the asymptotic decay of pair correlations, fluids in porous media, the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely new chapter on applications to 'soft matter' of a combination of liquid state theory and coarse graining strategies, with sections on polymer solutions and polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic liquid crystals, colloidal dynamics, and on clustering and gelation.},
  googlebooks = {pbJfOUqZVSgC},
  isbn = {978-0-12-387033-9},
  langid = {english},
  keywords = {Science / Physics / Condensed Matter}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}: Data {{Mining}}, {{Inference}}, and {{Prediction}}, {{Second Edition}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  month = aug,
  publisher = {{Springer Science \& Business Media}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  googlebooks = {tVIjmNS3Ob8C},
  isbn = {978-0-387-84858-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Data Analytics,Computers / Information Technology,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology,Science / Life Sciences / General}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  year = {1991},
  month = jan,
  journal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(91)90009-T},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp({$\mu$}) performance criteria, for arbitrary finite input environment measures {$\mu$}, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  langid = {english},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation}
}

@book{hornMatrixAnalysis2012,
  title = {Matrix {{Analysis}}},
  author = {Horn, Roger A. and Johnson, Charles R.},
  year = {2012},
  month = oct,
  publisher = {{Cambridge University Press}},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  googlebooks = {O7sgAwAAQBAJ},
  isbn = {978-1-139-78888-5},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Economics / General,Mathematics / Algebra / Abstract,Mathematics / Algebra / General,Mathematics / Geometry / Algebraic}
}

@inproceedings{houghZerosGaussianAnalytic2009,
  title = {Zeros of {{Gaussian Analytic Functions}} and {{Determinantal Point Processes}}},
  booktitle = {University {{Lecture Series}}},
  author = {Hough, J. and Krishnapur, Manjunath and Peres, Y. and Vir{\'a}g, B.},
  year = {2009},
  doi = {10.1090/ULECT/051},
  abstract = {The book examines in some depth two important classes of point processes, determinantal processes and 'Gaussian zeros', i.e., zeros of random analytic functions with Gaussian coefficients. These processes share a property of 'point-repulsion', where distinct points are less likely to fall close to each other than in processes, such as the Poisson process, that arise from independent sampling. Nevertheless, the treatment in the book emphasizes the use of independence: for random power series, the independence of coefficients is key; for determinantal processes, the number of points in a domain is a sum of independent indicators, and this yields a satisfying explanation of the central limit theorem (CLT) for this point count. Another unifying theme of the book is invariance of considered point processes under natural transformation groups. The book strives for balance between general theory and concrete examples. On the one hand, it presents a primer on modern techniques on the interface of probability and analysis. On the other hand, a wealth of determinantal processes of intrinsic interest are analyzed; these arise from random spanning trees and eigenvalues of random matrices, as well as from special power series with determinantal zeros. The material in the book formed the basis of a graduate course given at the IAS-Park City Summer School in 2007; the only background knowledge assumed can be acquired in first-year graduate courses in analysis and probability.}
}

@book{huangStatisticalMechanics1987,
  title = {Statistical {{Mechanics}}},
  author = {Huang, Kerson},
  year = {1987},
  month = may,
  publisher = {{Wiley}},
  abstract = {Unlike most other texts on the subject, this clear, concise introduction to the theory of microscopic bodies treats the modern theory of critical phenomena. Provides up-to-date coverage of recent major advances, including a self-contained description of thermodynamics and the classical kinetic theory of gases, interesting applications such as superfluids and the quantum Hall effect, several current research applications, The last three chapters are devoted to the Landau-Wilson approach to critical phenomena. Many new problems and illustrations have been added to this edition.},
  googlebooks = {M8PvAAAAMAAJ},
  isbn = {978-0-471-81518-1},
  langid = {english},
  keywords = {Science / Mechanics / General,Science / Mechanics / Thermodynamics}
}

@article{jadrichProbabilisticInverseDesign2017,
  title = {Probabilistic Inverse Design for Self-Assembling Materials},
  author = {Jadrich, R. B. and Lindquist, B. A. and Truskett, T. M.},
  year = {2017},
  month = may,
  journal = {The Journal of Chemical Physics},
  volume = {146},
  number = {18},
  pages = {184103},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.4981796}
}

@article{jadrichUnsupervisedMachineLearning2018,
  title = {Unsupervised Machine Learning for Detection of Phase Transitions in Off-Lattice Systems. {{I}}. {{Foundations}}},
  author = {Jadrich, R. B. and Lindquist, B. A. and Truskett, T. M.},
  year = {2018},
  month = nov,
  journal = {The Journal of Chemical Physics},
  volume = {149},
  number = {19},
  pages = {194109},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.5049849}
}

@article{karniadakisPhysicsinformedMachineLearning2021a,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Applied mathematics;Computational science Subject\_term\_id: applied-mathematics;computational-science}
}

@article{kellerIntegralEquationsMachine2019,
  title = {Integral Equations and Machine Learning},
  author = {Keller, Alexander and Dahm, Ken},
  year = {2019},
  month = jul,
  journal = {Mathematics and Computers in Simulation},
  series = {Special Issue on the {{Eleventh International Conference}} on {{Monte Carlo Methods}} and {{Applications}} ({{MCM}} 2017), Held in {{Montreal}}, {{Canada}}, {{July}} 03-07, 2017},
  volume = {161},
  pages = {2--12},
  issn = {0378-4754},
  doi = {10.1016/j.matcom.2019.01.010},
  abstract = {As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Efficiency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artificial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.},
  langid = {english},
  keywords = {Artificial neural networks,Integral equations,Light transport simulation,Monte Carlo and quasi-Monte Carlo methods,Reinforcement learning}
}

@article{kelleyFastSolverOrnstein2004,
  title = {A Fast Solver for the {{Ornstein}}\textendash{{Zernike}} Equations},
  author = {Kelley, C. T. and Pettitt, B. Montgomery},
  year = {2004},
  month = jul,
  journal = {Journal of Computational Physics},
  volume = {197},
  number = {2},
  pages = {491--501},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2003.12.006},
  abstract = {In this paper, we report on the design and analysis of a multilevel method for the solution of the Ornstein\textendash Zernike Equations and related systems of integro-algebraic equations. Our approach is based on an extension of the Atkinson\textendash Brakhage method, with Newton-GMRES used as the coarse mesh solver. We report on several numerical experiments to illustrate the effectiveness of the method. The problems chosen are related to simple short ranged fluids with continuous potentials. Speedups over traditional methods for a given accuracy are reported. The new multilevel method is roughly six times faster than Newton-GMRES and 40 times faster than Picard.},
  langid = {english}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: A {{Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@book{kittelElementaryStatisticalPhysics2004,
  title = {Elementary {{Statistical Physics}}},
  author = {Kittel, Charles},
  year = {2004},
  month = mar,
  publisher = {{Courier Corporation}},
  abstract = {Noteworthy for the philosophical subtlety of its foundations and the elegance of its problem-solving methods, statistical mechanics can be employed in a broad range of applications \textemdash{} among them, astrophysics, biology, chemistry, nuclear and solid state physics, communications engineering, metallurgy, and mathematics. Geared toward graduate students in physics, this text covers such important topics as stochastic processes and transport theory in order to provide students with a working knowledge of statistical mechanics. To explain the fundamentals of his subject, the author uses the method of ensembles developed by J. Willard Gibbs. Topics include the properties of the Fermi-Dirac and Bose-Einstein distributions; the interrelated subjects of fluctuations, thermal noise, and Brownian movement; and the thermodynamics of irreversible processes. Negative temperature, magnetic energy, density matrix methods, and the Kramers-Kronig causality relations are treated briefly. Most sections include illustrative problems. Appendix. 28 figures. 1 table.},
  googlebooks = {5sd9SAoRjgQC},
  isbn = {978-0-486-43514-5},
  langid = {english},
  keywords = {Science / Physics / General}
}

@article{kolafaBridgeFunctionHard2002,
  title = {The Bridge Function of Hard Spheres by Direct Inversion of Computer Simulation Data},
  author = {KOLAFA, JI{\v R}{\'I} and LAB{\'I}K, STANISLAV and MALIJEVSK{\'Y}, ANATOL},
  year = {2002},
  month = aug,
  journal = {Molecular Physics},
  volume = {100},
  number = {16},
  pages = {2629--2640},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268970210136357},
  abstract = {The bridge function of the hard sphere fluid has been calculated from our new highly accurate Monte Carlo and molecular dynamics simulation data on the radial distribution function using the (inverted) Ornstein-Zernike equation. Both the systematic errors (finite size, grid size, tail) and statistical errors are analysed in detail and ways to suppress them are proposed. Uncertainties in the resulting values of B(r) are about 0.001. In contrast with many previous findings the bridge function is both positive and negative.},
  annotation = {\_eprint: https://doi.org/10.1080/00268970210136357}
}

@article{kwakEvaluationBridgefunctionDiagrams2005,
  title = {Evaluation of Bridge-Function Diagrams via {{Mayer}}-Sampling {{Monte Carlo}} Simulation},
  author = {Kwak, Sang Kyu and Kofke, David A.},
  year = {2005},
  month = mar,
  journal = {The Journal of Chemical Physics},
  volume = {122},
  number = {10},
  pages = {104508},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1860559}
}

@article{labikEfficientGaussNewtonlikeMethod1994,
  title = {An {{Efficient Gauss}}-{{Newton}}-like {{Method}} for the {{Numerical Solution}} of the {{Ornstein}}-{{Zernike Integral Equation}} for a {{Class}} of {{Fluid Models}}},
  author = {Lab{\'{\i}}k, Stanislav and Posp{\'{\i}}{\v s}il, Roman and Malijevsk{\'y}, Anatol and Smith, William Robert},
  year = {1994},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {115},
  number = {1},
  pages = {12--21},
  issn = {0021-9991},
  doi = {10.1006/jcph.1994.1174},
  abstract = {A numerical algorithm for solving the Ornstein-Zernike (OZ) integral equation of statistical mechanics is described for the class of fluids composed of molecules with axially symmetric interactions. Since the OZ equation is a nonlinear second-kind Fredholm equation whose key feature for the class of problems of interest is the highly computationally intensive nature of the kernel, the general approach employed in this paper is thus potentially useful for similar problems with this characteristic. The algorithm achieves a high degree of computational efficiency by combining iterative linearization of the most complex portion of the kernel with a combination of Newton-Raphson and Picard iteration methods for the resulting approximate equation. This approach makes the algorithm analogous to the approach of the classical Gauss-Newton method for nonlinear regression, and we call our method the GN algorithm. An example calculation is given illustrating the use of the algorithm for the hard prolate ellipsoid fluid and its results are compared directly with those of the Picard iteration method. The GN algorithm is four to ten times as fast as the Picard method, and we present evidence that it is the most efficient general method currently available.},
  langid = {english}
}

@article{labikRapidlyConvergentMethod1985,
  title = {A Rapidly Convergent Method of Solving the {{OZ}} Equation},
  author = {Lab{\'i}k, Stanislav and Malijevsk{\'y}, Anatol and Vo{\v n}ka, Petr},
  year = {1985},
  month = oct,
  journal = {Molecular Physics},
  volume = {56},
  number = {3},
  pages = {709--715},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268978500102651},
  abstract = {A new method is proposed for solving numerically the Ornstein-Zernike equation for systems with a spherically symmetrical pair-potential. The method is based on expansion of the function {$\Gamma$}(r)=r[h(r) - c(r)] in suitable basis functions and on a combination of Newton-Raphson and direct iterations. Tests on the PY and HNC approximations for hard spheres and Lennard-Jones fluid have shown that the proposed method is three to nine times as rapid as the related and so far the most efficient method of Gillan. Other advantages besides the speed are low sensitivity to the choice of initial estimate and a relatively simple computational scheme.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978500102651}
}

@article{lechnerAccurateDeterminationCrystal2008,
  title = {Accurate Determination of Crystal Structures Based on Averaged Local Bond Order Parameters},
  author = {Lechner, Wolfgang and Dellago, Christoph},
  year = {2008},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {129},
  number = {11},
  pages = {114707},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.2977970}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computer science;Mathematics and computing Subject\_term\_id: computer-science;mathematics-and-computing}
}

@article{leunissenIonicColloidalCrystals2005,
  title = {Ionic Colloidal Crystals of Oppositely Charged Particles},
  author = {Leunissen, Mirjam E. and Christova, Christina G. and Hynninen, Antti-Pekka and Royall, C. Patrick and Campbell, Andrew I. and Imhof, Arnout and Dijkstra, Marjolein and {van Roij}, Ren{\'e} and {van Blaaderen}, Alfons},
  year = {2005},
  month = sep,
  journal = {Nature},
  volume = {437},
  number = {7056},
  pages = {235--240},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature03946},
  abstract = {Colloidal suspensions are widely used to study processes such as melting, freezing1,2,3 and glass transitions4,5. This is because they display the same phase behaviour as atoms or molecules, with the nano- to micrometre size of the colloidal particles making it possible to observe them directly in real space3,4. Another attractive feature is that different types of colloidal interactions, such as long-range repulsive1,3, short-range attractive5, hard-sphere-like2,3,4 and dipolar3, can be realized and give rise to equilibrium phases. However, spherically symmetric, long-range attractions (that is, ionic interactions) have so far always resulted in irreversible colloidal aggregation6. Here we show that the electrostatic interaction between oppositely charged particles can be tuned such that large ionic colloidal crystals form readily, with our theory and simulations confirming the stability of these structures. We find that in contrast to atomic systems, the stoichiometry of our colloidal crystals is not dictated by charge neutrality; this allows us to obtain a remarkable diversity of new binary structures. An external electric field melts the crystals, confirming that the constituent particles are indeed oppositely charged. Colloidal model systems can thus be used to study the phase behaviour of ionic species. We also expect that our approach to controlling opposite-charge interactions will facilitate the production of binary crystals of micrometre-sized particles, which could find use as advanced materials for photonic applications7.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research}
}

@article{libbrechtMachineLearningApplications2015,
  title = {Machine Learning Applications in Genetics and Genomics},
  author = {Libbrecht, Maxwell W. and Noble, William Stafford},
  year = {2015},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {16},
  number = {6},
  pages = {321--332},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0064},
  doi = {10.1038/nrg3920},
  abstract = {The field of machine learning includes the development and application of computer algorithms that improve with experience.Machine learning methods can be divided into supervised, semi-supervised and unsupervised methods. Supervised methods are trained on examples with labels (for example, 'gene' or 'not gene') and are then used to predict these labels on other examples, whereas unsupervised methods find patterns in data sets without the use of labels. Semi-supervised methods combine these two approaches, leveraging patterns in unlabelled data to improve power in the prediction of labels.Different machine learning methods may be required for an application, depending on whether one is interested in interpreting the output model or is simply concerned with predictive power. Generative models, which posit a probabilistic distribution over input data, are generally best for interpretability, whereas discriminative models, which seek only to model labels, are generally best for predictive power.Prior information can be added to a model in order to train the model more effectively when it is provided with limited data, to limit the complexity of the model or to incorporate data that are not used by the model directly. Prior information can be incorporated explicitly in a probabilistic model or implicitly through the choice of features or similarity measures.The choice of an appropriate performance measure depends strongly on the application task. Machine learning methods are most effective when they optimize an appropriate performance measure.Network estimation methods are appropriate when the data contain complex dependencies among examples. These methods work best when they take into account the confounding effects of indirect relationships.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Genomics;Machine learning;Statistical methods Subject\_term\_id: genomics;machine-learning;statistical-methods}
}

@article{liMolecularDynamicsOntheFly2015,
  title = {Molecular {{Dynamics}} with {{On}}-the-{{Fly Machine Learning}} of {{Quantum}}-{{Mechanical Forces}}},
  author = {Li, Zhenwei and Kermode, James R. and De Vita, Alessandro},
  year = {2015},
  month = mar,
  journal = {Physical Review Letters},
  volume = {114},
  number = {9},
  pages = {096405},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.114.096405},
  abstract = {We present a molecular dynamics scheme which combines first-principles and machine-learning (ML) techniques in a single information-efficient approach. Forces on atoms are either predicted by Bayesian inference or, if necessary, computed by on-the-fly quantum-mechanical (QM) calculations and added to a growing ML database, whose completeness is, thus, never required. As a result, the scheme is accurate and general, while progressively fewer QM calls are needed when a new chemical process is encountered for the second and subsequent times, as demonstrated by tests on crystalline and molten silicon.}
}

@article{lindquistCommunicationInverseDesign2016,
  title = {Communication: Inverse Design for Self-Assembly via on-the-Fly Optimization},
  shorttitle = {Communication},
  author = {Lindquist, Beth A. and Jadrich, Ryan B. and Truskett, Thomas M.},
  year = {2016},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {145},
  number = {11},
  pages = {111101},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.4962754},
  abstract = {Inverse methods of statistical mechanics have facilitated the discovery of pair potentials that stabilize a wide variety of targeted lattices at zero temperature. However, such methods are complicated by the need to compare, within the optimization framework, the energy of the desired lattice to all possibly relevant competing structures, which are not generally known in advance. Furthermore, ground-state stability does not guarantee that the target will readily assemble from the fluid upon cooling from higher temperature. Here, we introduce a molecular dynamics simulation-based, optimization design strategy that iteratively and systematically refines the pair interaction according to the fluid and crystalline structural ensembles encountered during the assembly process. We successfully apply this probabilistic, machine-learning approach to the design of repulsive, isotropic pair potentials that assemble into honeycomb, kagome, square, rectangular, truncated square, and truncated hexagonal lattices.}
}

@article{llano-restrepoBridgeFunctionCavity1992,
  title = {Bridge Function and Cavity Correlation Function for the {{Lennard}}-{{Jones}} Fluid from Simulation},
  author = {Llano-Restrepo, Mario and Chapman, Walter G.},
  year = {1992},
  month = aug,
  journal = {The Journal of Chemical Physics},
  volume = {97},
  number = {3},
  pages = {2046--2054},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.463142}
}

@article{lombaOrnsteinZernikeEquationsSimulation1993,
  title = {Ornstein-{{Zernike}} Equations and Simulation Results for Hard-Sphere Fluids Adsorbed in Porous Media},
  author = {Lomba, Enrique and Given, James A. and Stell, George and Weis, Jean Jacques and Levesque, Dominique},
  year = {1993},
  month = jul,
  journal = {Physical Review E},
  volume = {48},
  number = {1},
  pages = {233--244},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.48.233},
  abstract = {In this paper we solve the replica Ornstein-Zernike (ROZ) equations in the hypernetted-chain (HNC), Percus-Yevick (PY), and reference Percus-Yevick (RPY) approximations for partly quenched systems. The ROZ equations, which apply to the general class of partly quenched systems, are here applied to a class of models for porous media. These models involve two species of particles: an annealed or equilibrated species, which is used to model the fluid phase, and a quenched or frozen species, whose excluded-volume interactions constitute the matrix in which the fluid is adsorbed. We study two models for the quenched species of particles: a hard-sphere matrix, for which the fluid-fluid, matrix-matrix, and matrix-fluid sphere diameters {$\sigma$}11, {$\sigma$}00, and {$\sigma$}01 are additive, and a matrix of randomly overlapping particles (which still interact with the fluid particle as hard spheres) that gives a ``random'' matrix with interconnected pore structure. For the random-matrix case we study a ratio {$\sigma$}01/{$\sigma$}11 of 2.5, which is a demanding one for the theories. The HNC and RPY results represent significant improvements over the PY result when compared with the Monte Carlo simulations we have generated for this study, with the HNC result yielding the best results overall among those studied. A phenomenological percolating-fluid approximation is also found to be of comparable accuracy to the HNC results over a significant range of matrix and fluid densities. In the hard-sphere matrix case, the RPY is the best of the theories that we have considered.}
}

@misc{MachineLearningApplications,
  title = {Machine Learning Applications in Drug Development - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S2001037019303988}
}

@article{malijevskyBridgeFunctionHard1987,
  title = {The Bridge Function for Hard Spheres},
  author = {Malijevsk{\'y}, Anatol and Lab{\'i}k, Stanislav},
  year = {1987},
  month = feb,
  journal = {Molecular Physics},
  volume = {60},
  number = {3},
  pages = {663--669},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268978700100441},
  abstract = {The paper presents an empirical formula for expressing the bridge function (the sum of elementary graphs) in terms of the interparticle separation and the density. The formulae is fully consistent with the best computer-simulation thermodynamic and structural data for hard spheres in the fluid region. It can serve as both a direct and convenient testing ground for the integral-equation theories of hard spheres and an input to the reference-hypernetted chain approximation for simple fluids.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978700100441}
}

@article{mcdonaldCalculationThermodynamicProperties1967,
  title = {Calculation of Thermodynamic Properties of Liquid Argon from {{Lennard}}-{{Jones}} Parameters by a {{Monte Carlo}} Method},
  author = {McDonald, I. R. and Singer, K.},
  year = {1967},
  month = jan,
  journal = {Discussions of the Faraday Society},
  volume = {43},
  number = {0},
  pages = {40--49},
  publisher = {{The Royal Society of Chemistry}},
  issn = {0366-9033},
  doi = {10.1039/DF9674300040},
  abstract = {Extensions of the Monte Carlo method of Metropolis et al. are described which permit both isochoric and isothermal extrapolations of Monte Carlo data. Thermodynamic properties are calculated for liquid argon from Lennard-Jones parameters for a wide V, T-range. The agreement between calculated and experimental values is, on the whole, satisfactory.},
  langid = {english}
}

@book{mcquarrieStatisticalMechanics2000,
  title = {Statistical {{Mechanics}}},
  author = {McQuarrie, Donald A.},
  year = {2000},
  month = jun,
  publisher = {{University Science Books}},
  abstract = {The canonical ensemble - Other ensembles and fluctuations - Boltzmann statistics, fermi-dirac statistics, and bose-einstein statistics - Ideal monatomic gas - Ideal diatomic - Classical statistical mechanics - Ideal polyatomic - Chemical equilibrium - Quantum statistics - Crystals - Imperfect gases - Distribution functions in classical monatomic liquids - Perturbation theories of liquids - Solutions of strong electrolytes - Kinetic theory of gases and molecular collisions - Continuum mechanics - Kinetic theory of-gases and the boltzmann equation - Transport processes in dilute gases - Theory of brownian motion - The time-correlation function formalism.},
  googlebooks = {itcpPnDnJM0C},
  isbn = {978-1-891389-15-3},
  langid = {english},
  keywords = {Science / Chemistry / Physical \& Theoretical,Science / Mechanics / General,Science / Physics / General,Science / Physics / Mathematical \& Computational}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, S.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-40065-5},
  abstract = {Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
  isbn = {978-0-387-30303-1},
  langid = {english}
}

@inproceedings{parkMinimumWidthUniversal2020,
  title = {Minimum {{Width}} for {{Universal Approximation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Park, Sejun and Yun, Chulhee and Lee, Jaeho and Shin, Jinwoo},
  year = {2020},
  month = sep,
  abstract = {The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width...},
  langid = {english}
}

@article{peplowAlgorithmsComputationSolutions2006,
  title = {Algorithms for the Computation of Solutions of the {{Ornstein}}-{{Zernike}} Equation},
  author = {Peplow, A. T. and Beardmore, R. E. and Bresme, F.},
  year = {2006},
  month = oct,
  journal = {Physical Review E},
  volume = {74},
  number = {4},
  pages = {046705},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.74.046705},
  abstract = {We introduce a robust and efficient methodology to solve the Ornstein-Zernike integral equation using the pseudoarc length (PAL) continuation method that reformulates the integral equation in an equivalent but nonstandard form. This enables the computation of solutions in regions where the compressibility experiences large changes or where the existence of multiple solutions and so-called branch points prevents Newton's method from converging. We illustrate the use of the algorithm with a difficult problem that arises in the numerical solution of integral equations, namely the evaluation of the so-called no-solution line of the Ornstein-Zernike hypernetted chain (HNC) integral equation for the Lennard-Jones potential. We are able to use the PAL algorithm to solve the integral equation along this line and to connect physical and nonphysical solution branches (both isotherms and isochores) where appropriate. We also show that PAL continuation can compute solutions within the no-solution region that cannot be computed when Newton and Picard methods are applied directly to the integral equation. While many solutions that we find are new, some correspond to states with negative compressibility and consequently are not physical.}
}

@article{radovicMachineLearningEnergy2018,
  title = {Machine Learning at the Energy and Intensity Frontiers of Particle Physics},
  author = {Radovic, Alexander and Williams, Mike and Rousseau, David and Kagan, Michael and Bonacorsi, Daniele and Himmel, Alexander and Aurisano, Adam and Terao, Kazuhiro and Wongjirad, Taritree},
  year = {2018},
  month = aug,
  journal = {Nature},
  volume = {560},
  number = {7716},
  pages = {41--48},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0361-2},
  abstract = {Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics.},
  copyright = {2018 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computer science;Experimental particle physics Subject\_term\_id: computer-science;experimental-particle-physics}
}

@article{redaMachineLearningApplications2020,
  title = {Machine Learning Applications in Drug Development},
  author = {R{\'e}da, Cl{\'e}mence and Kaufmann, Emilie and {Delahaye-Duriez}, Andr{\'e}e},
  year = {2020},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {18},
  pages = {241--252},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2019.12.006},
  abstract = {Due to the huge amount of biological and medical data available today, along with well-established machine learning algorithms, the design of largely automated drug development pipelines can now be envisioned. These pipelines may guide, or speed up, drug discovery; provide a better understanding of diseases and associated biological phenomena; help planning preclinical wet-lab experiments, and even future clinical trials. This automation of the drug development process might be key to the current issue of low productivity rate that pharmaceutical companies currently face. In this survey, we will particularly focus on two classes of methods: sequential learning and recommender systems, which are active biomedical fields of research.},
  langid = {english},
  keywords = {Adaptive clinical trial,Bayesian optimization,Collaborative filtering,Drug discovery,Drug repurposing,Multi-armed bandit}
}

@article{rogersIonicLiquidsSolvents2003,
  title = {Ionic {{Liquids}}--{{Solvents}} of the {{Future}}?},
  author = {Rogers, Robin D. and Seddon, Kenneth R.},
  year = {2003},
  month = oct,
  journal = {Science},
  volume = {302},
  number = {5646},
  pages = {792--793},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1090313}
}

@article{rogersNewThermodynamicallyConsistent1984,
  title = {New, Thermodynamically Consistent, Integral Equation for Simple Fluids},
  author = {Rogers, Forrest J. and Young, David A.},
  year = {1984},
  month = aug,
  journal = {Physical Review A},
  volume = {30},
  number = {2},
  pages = {999--1007},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevA.30.999},
  abstract = {A new integral equation in which the hypernetted-chain and Percus-Yevick approximations are "mixed" as a function of interparticle separation is described. An adjustable parameter {$\alpha$} in the mixing function is used to enforce thermodynamic consistency. For simple 1rn potential fluids, {$\alpha$} is constant for all densities, and the solutions of the integral equations are in very good agreement with Monte Carlo calculations. For the one-component plasma, {$\alpha$} is a slowly varying function of density, but the agreement between calculated solutions and Monte Carlo is also good. This approach has definite advantages over previous thermodynamically consistent equations.}
}

@article{rubinsteinOptimizationComputerSimulation1997,
  title = {Optimization of Computer Simulation Models with Rare Events},
  author = {Rubinstein, Reuven Y.},
  year = {1997},
  month = may,
  journal = {European Journal of Operational Research},
  volume = {99},
  number = {1},
  pages = {89--112},
  issn = {0377-2217},
  doi = {10.1016/S0377-2217(96)00385-2},
  abstract = {Discrete event simulation systems (DESS) are widely used in many diverse areas such as computer-communication networks, flexible manufacturing systems, project evaluation and review techniques (PERT), and flow networks. Because of their complexity, such systems are typically analyzed via Monte Carlo simulation methods. This paper deals with optimization of complex computer simulation models involving rare events. A classic example is to find an optimal (s, S) policy in a multi-item, multicommodity inventory system, when quality standards require the backlog probability to be extremely small. Our approach is based on change of the probability measure techniques, also called likelihood ratio (LR) and importance sampling (IS) methods. Unfortunately, for arbitrary probability measures the LR estimators and the resulting optimal solution often tend to be unstable and may have large variances. Therefore, the choice of the corresponding importance sampling distribution and in particular its parameters in an optimal way is an important task. We consider the case where the IS distribution comes from the same parametric family as the original (true) one and use the stochastic counterpart method to handle simulation based optimization models. More specifically, we use a two-stage procedure: at the first stage we identify (estimate) the optimal parameter vector at the IS distribution, while at the second stage we estimate the optimal solution of the underlying constrained optimization problem. Particular emphasis will be placed on estimation of rare events and on integration of the associated performance function into stochastic optimization programs. Supporting numerical results are provided as well.},
  langid = {english},
  keywords = {Inventory,Optimization,Score function,Sensitivity analysis,Simulation}
}

@article{schmidtDistillingFreeFormNatural2009,
  title = {Distilling {{Free}}-{{Form Natural Laws}} from {{Experimental Data}}},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2009},
  month = apr,
  journal = {Science},
  volume = {324},
  number = {5923},
  pages = {81--85},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1165893},
  abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the ``alphabet'' used to describe those systems. An algorithm has been developed to search for natural laws of physics in large data sets. An algorithm has been developed to search for natural laws of physics in large data sets.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {19342586}
}

@article{schoenholzStructuralApproachRelaxation2016,
  title = {A Structural Approach to Relaxation in Glassy Liquids},
  author = {Schoenholz, S. S. and Cubuk, E. D. and Sussman, D. M. and Kaxiras, E. and Liu, A. J.},
  year = {2016},
  month = may,
  journal = {Nature Physics},
  volume = {12},
  number = {5},
  pages = {469--471},
  publisher = {{Nature Publishing Group}},
  issn = {1745-2481},
  doi = {10.1038/nphys3644},
  abstract = {The relation between structure and dynamics in glasses is not fully understood. A new approach based on machine learning now reveals a correlation between softness\textemdash a structural property\textemdash and glassy dynamics.},
  copyright = {2016 Nature Publishing Group},
  langid = {english}
}

@article{scholl-paschingerSelfconsistentOrnsteinZernike2003,
  title = {Self-Consistent {{Ornstein}}\textendash{{Zernike}} Approximation for a Binary Symmetric Fluid Mixture},
  author = {{Sch{\"o}ll-Paschinger}, Elisabeth and Kahl, Gerhard},
  year = {2003},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {118},
  number = {16},
  pages = {7414--7424},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.1557053}
}

@article{shermanInverseMethodsDesign2020a,
  title = {Inverse Methods for Design of Soft Materials},
  author = {Sherman, Zachary M. and Howard, Michael P. and Lindquist, Beth A. and Jadrich, Ryan B. and Truskett, Thomas M.},
  year = {2020},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {152},
  number = {14},
  pages = {140902},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.5145177},
  abstract = {Functional soft materials, comprising colloidal and molecular building blocks that self-organize into complex structures as a result of their tunable interactions, enable a wide array of technological applications. Inverse methods provide a systematic means for navigating their inherently high-dimensional design spaces to create materials with targeted properties. While multiple physically motivated inverse strategies have been successfully implemented in silico, their translation to guiding experimental materials discovery has thus far been limited to a handful of proof-of-concept studies. In this perspective, we discuss recent advances in inverse methods for design of soft materials that address two challenges: (1) methodological limitations that prevent such approaches from satisfying design constraints and (2) computational challenges that limit the size and complexity of systems that can be addressed. Strategies that leverage machine learning have proven particularly effective, including methods to discover order parameters that characterize complex structural motifs and schemes to efficiently compute macroscopic properties from the underlying structure. We also highlight promising opportunities to improve the experimental realizability of materials designed computationally, including discovery of materials with functionality at multiple thermodynamic states, design of externally directed assembly protocols that are simple to implement in experiments, and strategies to improve the accuracy and computational efficiency of experimentally relevant models.}
}

@article{steinhardtBondorientationalOrderLiquids1983,
  title = {Bond-Orientational Order in Liquids and Glasses},
  author = {Steinhardt, Paul J. and Nelson, David R. and Ronchetti, Marco},
  year = {1983},
  month = jul,
  journal = {Physical Review B},
  volume = {28},
  number = {2},
  pages = {784--805},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevB.28.784},
  abstract = {Bond-orientational order in molecular-dynamics simulations of supercooled liquids and in models of metallic glasses is studied. Quadratic and third-order invariants formed from bond spherical harmonics allow quantitative measures of cluster symmetries in these systems. A state with short-range translational order, but extended correlations in the orientations of particle clusters, starts to develop about 10\% below the equilibrium melting temperature in a supercooled Lennard-Jones liquid. The order is predominantly icosahedral, although there is also a cubic component which we attribute to the periodic boundary conditions. Results are obtained for liquids cooled in an icosahedral pair potential as well. Only a modest amount of orientational order appears in a relaxed Finney dense-random-packing model. In contrast, we find essentially perfect icosahedral bond correlations in alternative "amorphon" cluster models of glass structure.}
}

@book{steinwartSupportVectorMachines2008,
  title = {Support {{Vector Machines}}},
  author = {Steinwart, Ingo and Christmann, Andreas},
  year = {2008},
  month = sep,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Every mathematical discipline goes through three periods of development: the naive, the formal, and the critical. David Hilbert The goal of this book is to explain the principles that made support vector machines (SVMs) a successful modeling and prediction tool for a variety of applications. We try to achieve this by presenting the basic ideas of SVMs together with the latest developments and current research questions in a uni?ed style. In a nutshell, we identify at least three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and last but not least their computational e?ciency compared with several other methods. Although there are several roots and precursors of SVMs, these methods gained particular momentum during the last 15 years since Vapnik (1995, 1998) published his well-known textbooks on statistical learning theory with aspecialemphasisonsupportvectormachines. Sincethen,the?eldofmachine learninghaswitnessedintenseactivityinthestudyofSVMs,whichhasspread moreandmoretootherdisciplinessuchasstatisticsandmathematics. Thusit seems fair to say that several communities are currently working on support vector machines and on related kernel-based methods. Although there are many interactions between these communities, we think that there is still roomforadditionalfruitfulinteractionandwouldbegladifthistextbookwere found helpful in stimulating further research. Many of the results presented in this book have previously been scattered in the journal literature or are still under review. As a consequence, these results have been accessible only to a relativelysmallnumberofspecialists,sometimesprobablyonlytopeoplefrom one community but not the others.},
  googlebooks = {HUnqnrpYt4IC},
  isbn = {978-0-387-77242-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Data Science / Data Analytics,Computers / Information Technology,Computers / Mathematical \& Statistical Software,Computers / Optical Data Processing,Mathematics / Discrete Mathematics,Technology \& Engineering / Electronics / General,Technology \& Engineering / Imaging Systems}
}

@article{tangAnalyticalSolutionOrnsteinZernike1995,
  title = {Analytical Solution of the {{Ornstein}}-{{Zernike}} Equation for Mixtures},
  author = {Tang, Yiping and Lu, Benjamin C.-Y.},
  year = {1995},
  month = jan,
  journal = {Molecular Physics},
  volume = {84},
  number = {1},
  pages = {89--103},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268979500100061},
  abstract = {Solution of the Ornstein-Zernike equation under the Percus-Yevick or the mean spherical approximation is presented analytically in a matrix form. The new solution is an extension of the general Ornstein-Zernike solution suggested recently for pure fluids. The development is based on further application of the Hilbert transform and multiple-dimensional space analysis. In addition to the potential matrix, only a hard core correlation function matrix and its inverse are involved in the expression. The solution achieved in this work is explicit and is applicable to any arbitrary potential functions with an additive hard core. The first-order solution for two Yukawa mixtures has been compared with the full solution reported in the literature to serve as an example.},
  annotation = {\_eprint: https://doi.org/10.1080/00268979500100061}
}

@book{tolmanPrinciplesStatisticalMechanics1979,
  title = {The {{Principles}} of {{Statistical Mechanics}}},
  author = {Tolman, Richard Chace},
  year = {1979},
  month = jan,
  publisher = {{Courier Corporation}},
  abstract = {Classic treatment of a subject essential to contemporary physics. Classical and quantum statistical mechanics, plus application to thermodynamic behavior.},
  googlebooks = {4TqQZo962s0C},
  isbn = {978-0-486-63896-6},
  langid = {english},
  keywords = {Science / Mechanics / General,Science / Physics / General,Technology \& Engineering / General}
}

@article{torrieMonteCarloCalculation1977,
  title = {Monte {{Carlo}} Calculation of y(r) for the Hard-Sphere Fluid},
  author = {Torrie, G. and Patey, G. N.},
  year = {1977},
  month = dec,
  journal = {Molecular Physics},
  volume = {34},
  number = {6},
  pages = {1623--1628},
  publisher = {{Taylor \& Francis}},
  issn = {0026-8976},
  doi = {10.1080/00268977700102821},
  abstract = {The function y(r) = exp {$\beta$}u(r)g(r) is calculated for hard spheres in the region r {$<$} {$\sigma$} using umbrella-sampling Monte Carlo techniques. The resulting values are found to be well represented over the entire range 0 {$<$} r {$<$} {$\sigma$} by a simple function proposed by Grundke and Henderson.},
  annotation = {\_eprint: https://doi.org/10.1080/00268977700102821}
}

@article{tsedneeClosureOrnsteinZernikeEquation2019,
  title = {Closure for the {{Ornstein}}-{{Zernike}} Equation with Pressure and Free Energy Consistency},
  author = {Tsednee, Tsogbayar and Luchko, Tyler},
  year = {2019},
  month = mar,
  journal = {Physical Review E},
  volume = {99},
  number = {3},
  pages = {032130},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.99.032130},
  abstract = {The Ornstein-Zernike (OZ) integral equation theory is a powerful approach to simple liquids due to its low computational cost and the fact that, when combined with an appropriate closure equation, the theory is thermodynamically complete. However, approximate closures proposed to date exhibit pressure or free energy inconsistencies that produce inaccurate or ambiguous results, limiting the usefulness of the Ornstein-Zernike approach. To address this problem, we combine methods to enforce both pressure and free energy consistency to create a new closure approximation and test it for a single-component Lennard-Jones fluid. The closure is a simple power series in the direct and total correlation functions for which we have derived analytical formulas for the excess Helmholtz free energy and chemical potential. These expressions contain a partial molar volumelike term, similar to excess chemical potential correction terms recently developed. Using our bridge approximation, we have calculated the pressure, Helmholtz free energy, and chemical potential for the Lennard-Jones fluid using the Kirkwood charging, thermodynamic integration techniques, and analytic expressions. These results are compared with those from the hypernetted chain equation and the Verlet-modified closure against Monte Carlo and equations-of-state data for reduced densities of {$\rho{_\ast}<$}1 and temperatures of T{${_\ast}$}=1.5, 2.74, and 5. Our closure shows consistency among all thermodynamic paths, except for one expression of the Gibbs-Duhem relation, whereas the hypernetted chain equation and the Verlet-modified closure exhibit consistency between only a few relations. Accuracy of the closure is comparable to the Verlet-modified closure and a significant improvement to results obtained from the hypernetted chain equation.}
}

@article{vandammeClassifyingCrystalsRounded2020,
  title = {Classifying {{Crystals}} of {{Rounded Tetrahedra}} and {{Determining Their Order Parameters Using Dimensionality Reduction}}},
  author = {{van Damme}, Robin and Coli, Gabriele M. and {van Roij}, Ren{\'e} and Dijkstra, Marjolein},
  year = {2020},
  month = nov,
  journal = {ACS Nano},
  volume = {14},
  number = {11},
  pages = {15144--15153},
  publisher = {{American Chemical Society}},
  issn = {1936-0851},
  doi = {10.1021/acsnano.0c05288},
  abstract = {Using simulations we study the phase behavior of a family of hard spherotetrahedra, a shape that interpolates between tetrahedra and spheres. We identify 13 close-packed structures, some with packings that are significantly denser than previously reported. Twelve of these are crystals with unit cells of N = 2 or N = 4 particles, but in the shape regime of slightly rounded tetrahedra we find that the densest structure is a quasicrystal approximant with a unit cell of N = 82 particles. All 13 structures are also stable below close packing, together with an additional 14th plastic crystal phase at the sphere side of the phase diagram, and upon sufficient dilution to packing fractions below 50\textendash 60\% all structures melt. Interestingly, however, upon compressing the fluid phase, self-assembly takes place spontaneously only at the tetrahedron and the sphere side of the family but not in an intermediate regime of tetrahedra with rounded edges. We describe the local environment of each particle by a set of l-fold bond orientational order parameters {$\overline q$}l, which we use in an extensive principal component analysis. We find that the total packing fraction as well as several particular linear combinations of {$\overline q$}l rather than individual {$\overline q$}l's are optimally distinctive, specifically the differences {$\overline q$}4 \textendash{} {$\overline q$}6 for separating tetragonal from hexagonal structures and {$\overline q$}4\textendash{$\overline q$}8 for distinguishing tetragonal structures. We argue that these characteristic combinations are also useful as reliable order parameters in nucleation studies, enhanced sampling techniques, or inverse-design methods involving odd-shaped particles in general.}
}

@article{verletComputerExperimentsClassical1967a,
  title = {Computer "{{Experiments}}" on {{Classical Fluids}}. {{I}}. {{Thermodynamical Properties}} of {{Lennard}}-{{Jones Molecules}}},
  author = {Verlet, Loup},
  year = {1967},
  month = jul,
  journal = {Physical Review},
  volume = {159},
  number = {1},
  pages = {98--103},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRev.159.98},
  abstract = {The equation of motion of a system of 864 particles interacting through a Lennard-Jones potential has been integrated for various values of the temperature and density, relative, generally, to a fluid state. The equilibrium properties have been calculated and are shown to agree very well with the corresponding properties of argon. It is concluded that, to a good approximation, the equilibrium state of argon can be described through a two-body potential.}
}

@article{vompeBridgeFunctionExpansion1994,
  title = {The Bridge Function Expansion and the Self-consistency Problem of the {{Ornstein}}\textendash{{Zernike}} Equation Solution},
  author = {Vompe, A. G. and Martynov, G. A.},
  year = {1994},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {100},
  number = {7},
  pages = {5249--5258},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.467189}
}

@book{yuIntroductionEvolutionaryAlgorithms2010,
  title = {Introduction to {{Evolutionary Algorithms}}},
  author = {Yu, Xinjie and Gen, Mitsuo},
  year = {2010},
  month = jun,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Evolutionary algorithms (EAs) are becoming increasingly attractive for researchers from various disciplines, such as operations research, computer science, industrial engineering, electrical engineering, social science, economics, etc. This book presents an insightful, comprehensive, and up-to-date treatment of EAs, such as genetic algorithms, differential evolution, evolution strategy, constraint optimization, multimodal optimization, multiobjective optimization, combinatorial optimization, evolvable hardware, estimation of distribution algorithms, ant colony optimization, particle swarm optimization, artificial immune systems, artificial life, genetic programming, etc. It emphasises the initiative ideas of the algorithm, contains discussions in the contexts, and suggests further readings and possible research projects. All the methods form a pedagogical way to make EAs easy and interesting. This textbook also introduces the applications of EAs as many as possible. At least one real-life application is introduced by the end of almost every chapter. The authors focus on the kernel part of applications, such as how to model real-life problems, how to encode and decode the individuals, how to design effective search operators according to the chromosome structures, etc. This textbook adopts pedagogical ways of making EAs easy and interesting. Its methods include an introduction at the beginning of each chapter, emphasising the initiative, discussions in the contexts, summaries at the end of every chapter, suggested further reading, exercises, and possible research projects. Introduction to Evolutionary Algorithms will enable students to: \textbullet{} establish a strong background on evolutionary algorithms; \textbullet{} appreciate the cutting edge of EAs; \textbullet{} perform their own research projects by simulating the application introduced in the book; and \textbullet{} apply their intuitive ideas to academic search. This book is aimed at senior undergraduate students or first-year graduate students as a textbook or self-study material.},
  googlebooks = {rHQf\_2Dx2ucC},
  isbn = {978-1-84996-129-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Simulation,Computers / Data Science / Data Modeling \& Design,Computers / Information Technology,Computers / Programming / Algorithms,Language Arts \& Disciplines / Library \& Information Science / General,Technology \& Engineering / Automation,Technology \& Engineering / Engineering (General)}
}

@article{zerahEfficientNewtonMethod1985,
  title = {An Efficient Newton's Method for the Numerical Solution of Fluid Integral Equations},
  author = {Zerah, Gilles},
  year = {1985},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {61},
  number = {2},
  pages = {280--285},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(85)90087-7},
  abstract = {We propose a stable, straightforward algorithm for the numerical solution of integral equations for fluid pair distribution functions. The integral equation is not solved by Picard's standard iterative procedure but by Newton's method of solution of non-linear equations. The large matrix appearing in Newton's method is inverted by a conjugate gradient procedure used as a rapidly converging iterative method.},
  langid = {english}
}

@article{zerahSelfConsistentIntegral1986,
  title = {Self-consistent Integral Equations for Fluid Pair Distribution Functions: Another Attempt},
  shorttitle = {Self-consistent Integral Equations for Fluid Pair Distribution Functions},
  author = {Zerah, Gilles and Hansen, Jean-Pierre},
  year = {1986},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {84},
  number = {4},
  pages = {2336--2343},
  publisher = {{American Institute of Physics}},
  issn = {0021-9606},
  doi = {10.1063/1.450397}
}

@article{zhouUniversalityDeepConvolutional2020,
  title = {Universality of Deep Convolutional Neural Networks},
  author = {Zhou, Ding-Xuan},
  year = {2020},
  month = mar,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {48},
  number = {2},
  pages = {787--794},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2019.06.004},
  abstract = {Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains. Deep neural network architectures and computational issues have been well studied in machine learning. But there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network architectures such as deep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough. This answers an open question in learning theory. Our quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data. Our study also demonstrates the role of convolutions in deep CNNs.},
  langid = {english},
  keywords = {Approximation theory,Convolutional neural network,Deep learning,Universality}
}

@article{zhuPhysicsconstrainedDeepLearning2019,
  title = {Physics-Constrained Deep Learning for High-Dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data},
  author = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Physics},
  volume = {394},
  pages = {56--81},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2019.05.024},
  abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.},
  langid = {english},
  keywords = {Conditional generative model,Normalizing flow,Physics-constrained,Reverse KL divergence,Surrogate modeling,Uncertainty quantification}
}


