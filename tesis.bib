
@article{a.goodallDatadrivenApproximationsBridge2021,
  title = {Data-Driven Approximations to the Bridge Function Yield Improved Closures for the Ornstein\textendash Zernike Equation},
  author = {A. Goodall, Rhys E. and A. Lee, Alpha},
  year = {2021},
  journal = {Soft Matter},
  volume = {17},
  number = {21},
  pages = {5393--5400},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/D1SM00402F},
  langid = {english}
}

@article{abdollahiVNetEndtoEndFully2020,
  title = {VNet: An End-to-End Fully Convolutional Neural Network for Road Extraction From High-Resolution Remote Sensing Data},
  shorttitle = {VNet},
  author = {Abdollahi, Abolfazl and Pradhan, Biswajeet and Alamri, Abdullah},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {179424--179436},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3026658},
  abstract = {One of the most important tasks in the advanced transportation systems is road extraction. Extracting road region from high-resolution remote sensing imagery is challenging due to complicated background such as buildings, trees shadows, pedestrians and vehicles and rural road networks that have heterogeneous forms with low interclass and high intraclass differences. Recently, deep learning-based techniques have presented a notable enhancement in the image segmentation results, however, most of them still cannot preserve boundary information and obtain high-resolution road segmentation map when processing the remote sensing imagery. In the present study, we introduce a new deep learning-based convolutional network called VNet model to produce a high-resolution road segmentation map. Moreover, a new dual loss function called cross-entropy-dice-loss (CEDL) is defined that synthesize cross-entropy (CE) and dice loss (DL) and consider both local information (CE) and global information (DL) to decrease the class imbalance influence and improve the road extraction results. The proposed VNet+CEDL model is implemented on two various road datasets called Massachusetts and Ottawa datasets. The suggested VNet+CEDL approach achieved an average F1 accuracy of 90.64\% for Massachusetts dataset and 92.41\% for Ottawa dataset. When compared to other state-of-the-art deep learning-based frameworks like FCN, Segnet and Unet, the proposed approach could improve the results to 1.09\%, 2.45\% and 0.39\%, for Massachusetts dataset and 7.21\%, 1.86\% and 2.68\%, for Ottawa dataset. Also, we compared the proposed method with the state-of-the-art road extraction techniques, and the results proved that the proposed technique outperformed other deep learning-based techniques in road extraction.},
  keywords = {CEDL,Convolutional neural networks,Data mining,Feature extraction,Image segmentation,remote sensing,Remote sensing,road extraction,Roads,Training,VNet network}
}

@incollection{adamNoFreeLunch2019,
  title = {No Free Lunch Theorem: A Review},
  shorttitle = {No Free Lunch Theorem},
  booktitle = {Approximation and Optimization : Algorithms, Complexity and Applications},
  author = {Adam, Stavros P. and Alexandropoulos, Stamatios-Aggelos N. and Pardalos, Panos M. and Vrahatis, Michael N.},
  editor = {Demetriou, Ioannis C. and Pardalos, Panos M.},
  year = {2019},
  series = {Springer Optimization and Its Applications},
  pages = {57--82},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-12767-1_5},
  abstract = {The ``No Free Lunch'' theorem states that, averaged over all optimization problems, without re-sampling, all optimization algorithms perform equally well. Optimization, search, and supervised learning are the areas that have benefited more from this important theoretical concept. Formulation of the initial No Free Lunch theorem, very soon, gave rise to a number of research works which resulted in a suite of theorems that define an entire research field with significant results in other scientific areas where successfully exploring a search space is an essential and critical task. The objective of this paper is to go through the main research efforts that contributed to this research field, reveal the main issues, and disclose those points that are helpful in understanding the hypotheses, the restrictions, or even the inability of applying No Free Lunch theorems.},
  isbn = {978-3-030-12767-1},
  langid = {english}
}

@article{agostinelliLearningActivationFunctions2015,
  title = {Learning Activation Functions to Improve Deep Neural Networks},
  author = {Agostinelli, Forest and Hoffman, Matthew and Sadowski, Peter and Baldi, Pierre},
  year = {2015},
  month = apr,
  journal = {arXiv:1412.6830 [cs, stat]},
  eprint = {1412.6830},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51\%), CIFAR-100 (30.83\%), and a benchmark from high-energy physics involving Higgs boson decay modes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{alderPhaseTransitionHard1957a,
  title = {Phase Transition for a Hard Sphere System},
  author = {Alder, B. J. and Wainwright, T. E.},
  year = {1957},
  month = nov,
  journal = {The Journal of Chemical Physics},
  volume = {27},
  number = {5},
  pages = {1208--1209},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1743957}
}

@book{allenComputerSimulationLiquids2017,
  title = {Computer Simulation of Liquids},
  author = {Allen, Michael P. and Tildesley, Dominic J.},
  year = {2017},
  month = aug,
  publisher = {Oxford University Press},
  abstract = {This book provides a practical guide to molecular dynamics and Monte Carlo simulation techniques used in the modelling of simple and complex liquids. Computer simulation is an essential tool in studying the chemistry and physics of condensed matter, complementing and reinforcing both experiment and theory. Simulations provide detailed information about structure and dynamics, essential to understand the many fluid systems that play a key role in our daily lives: polymers, gels, colloidal suspensions, liquid crystals, biological membranes, and glasses. The second edition of this pioneering book aims to explain how simulation programs work, how to use them, and how to interpret the results, with examples of the latest research in this rapidly evolving field. Accompanying programs in Fortran and Python provide practical, hands-on, illustrations of the ideas in the text.},
  googlebooks = {WFExDwAAQBAJ},
  isbn = {978-0-19-252470-6},
  langid = {english},
  keywords = {Computers / Computer Simulation,Science / Mechanics / Fluids,Science / Physics / General}
}

@article{amariNaturalGradientWorks1998,
  title = {Natural Gradient Works Efficiently in Learning},
  author = {Amari, Shun-ichi},
  year = {1998},
  month = feb,
  journal = {Neural Computation},
  volume = {10},
  number = {2},
  pages = {251--276},
  issn = {0899-7667},
  doi = {10.1162/089976698300017746},
  abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.}
}

@inproceedings{amariWhyNaturalGradient1998,
  title = {Why Natural Gradient?},
  booktitle = {Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)},
  author = {Amari, S. and Douglas, S.C.},
  year = {1998},
  month = may,
  volume = {2},
  pages = {1213-1216 vol.2},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1998.675489},
  abstract = {Gradient adaptation is a useful technique for adjusting a set of parameters to minimize a cost function. While often easy to implement, the convergence speed of gradient adaptation can be slow when the slope of the cost function varies widely for small changes in the parameters. In this paper, we outline an alternative technique, termed natural gradient adaptation, that overcomes the poor convergence properties of gradient adaptation in many cases. The natural gradient is based on differential geometry and employs knowledge of the Riemannian structure of the parameter space to adjust the gradient search direction. Unlike Newton's method, natural gradient adaptation does not assume a locally-quadratic cost function. Moreover, for maximum likelihood estimation tasks, natural gradient adaptation is asymptotically Fisher-efficient. A simple example illustrates the desirable properties of natural gradient adaptation.},
  keywords = {Adaptive filters,Cities and towns,Convergence,Cost function,Geometry,Information systems,Iterative methods,Newton method,Optimization methods,Parameter estimation}
}

@article{antaFastMethodSolving1995,
  title = {A Fast Method of Solving the Hypernetted-Chain Equation for Molecular Lennard-Jones Fluids},
  author = {Anta, J. A. and Lomba, E. and Mart{\'i}n, C. and Lombardero, M. and Lado, F.},
  year = {1995},
  month = mar,
  journal = {Molecular Physics},
  volume = {84},
  number = {4},
  pages = {743--755},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268979500100511},
  abstract = {A fast and stable procedure for solving hypernetted-chain type integral equations for molecular Lennard-Jones fluids is presented. The method is a hybrid algorithm based on the combination of multidimensional angular integration of the closure relation and a linearization technique devised by Fries and Patey (1985, Molec. Phys., 55, 751). The combination of the two techniques leads to a remarkable reduction in the CPU time required to evaluate the closure relation in these systems, which is usually the most time-consuming task. As an application of the method, phase coexistence curves have been calculated for two-centre Lennard-Jones fluids with and without point dipoles.},
  annotation = {\_eprint: https://doi.org/10.1080/00268979500100511}
}

@inproceedings{APSAPSMarcha,
  title = {APS -APS March Meeting 2020 - Event - Inverse Design of Soft Materials: Crystals, Quasi Crystals, Liquid Crystals},
  shorttitle = {APS -APS March Meeting 2020 - Event - Inverse Design of Soft Materials},
  booktitle = {Bulletin of the American Physical Society},
  volume = {Volume 65, Number 1},
  publisher = {American Physical Society}
}

@inproceedings{arulkumaranAlphaStarEvolutionaryComputation2019,
  title = {AlphaStar: An Evolutionary Computation Perspective},
  shorttitle = {AlphaStar},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  author = {Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
  year = {2019},
  month = jul,
  series = {GECCO '19},
  pages = {314--315},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3319619.3321894},
  abstract = {In January 2019, DeepMind revealed AlphaStar to the world---the first artificial intelligence (AI) system to beat a professional player at the game of StarCraft II---representing a milestone in the progress of AI. AlphaStar draws on many areas of AI research, including deep learning, reinforcement learning, game theory, and evolutionary computation (EC). In this paper we analyze AlphaStar primarily through the lens of EC, presenting a new look at the system and relating it to many concepts in the field. We highlight some of its most interesting aspects---the use of Lamarckian evolution, competitive co-evolution, and quality diversity. In doing so, we hope to provide a bridge between the wider EC community and one of the most significant AI systems developed in recent times.},
  isbn = {978-1-4503-6748-6},
  keywords = {co-evolution,lamarckian evolution,quality diversity}
}

@article{asadyUtilizingArtificialNeural2014,
  title = {Utilizing Artificial Neural Network Approach for Solving Two-Dimensional Integral Equations},
  author = {Asady, B. and Hakimzadegan, F. and Nazarlue, R.},
  year = {2014},
  month = apr,
  journal = {Mathematical Sciences},
  volume = {8},
  number = {1},
  pages = {117},
  issn = {2251-7456},
  doi = {10.1007/s40096-014-0117-6},
  abstract = {This paper surveys the artificial neural networks approach. Researchers believe that these networks have the wide range of applicability, they can treat complicated problems as well. The work described here discusses an efficient computational method that can treat complicated problems. The paper intends to introduce an efficient computational method which can be applied to approximate solution of the linear two-dimensional Fredholm integral equation of the second kind. For this aim, a perceptron model based on artificial neural networks is introduced. At first, the unknown bivariate function is replaced by a multilayer perceptron neural net and also a cost function to be minimized is defined. Then a famous learning technique, namely, the steepest descent method, is employed to adjust the parameters (the weights and biases) to optimize their behavior. The article also examines application of the method which turns to be so accurate and efficient. It concludes with a survey of an example in order to investigate the accuracy of the proposed method.},
  langid = {english}
}

@inproceedings{backSurveyEvolutionStrategies1991,
  title = {A Survey of Evolution Strategies},
  booktitle = {Proceedings of the Fourth International Conference on Genetic Algorithms},
  author = {B{\"a}ck, Thomas and Hoffmeister, Frank and Schwefel, Hans-Paul},
  year = {1991},
  pages = {2--9},
  publisher = {Morgan Kaufmann},
  abstract = {Similar to Genetic Algorithms, Evolution Strategies (ESs) are algorithms which imitate the principles of natural evolution as a method to solve parameter optimization problems. The development of Evolution Strategies from the first mutation--selection scheme to the refined (\textasciimacron,)--ES including the general concept of self--adaptation of the strategy parameters for the mutation variances as well as their covariances are described. 1 Introduction  The idea to use principles of organic evolution processes as rules for optimum seeking procedures emerged independently on both sides of the Atlantic ocean more than two decades ago. Both approaches rely upon imitating the collective learning paradigm of natural populations, based upon Darwin's observations and the modern synthetic theory of evolution. In the USA Holland introduced Genetic Algorithms  in the 60ies, embedded into the general framework of adaptation [Hol75]. He also mentioned the applicability to parameter optimization which was fir...}
}

@article{baezUsingSecondVirial2018,
  title = {Using the Second Virial Coefficient as Physical Criterion to Map the Hard-Sphere Potential onto a Continuous Potential},
  author = {B{\'a}ez, C{\'e}sar Alejandro and {Torres-Carbajal}, Alexis and {Casta{\~n}eda-Priego}, Ram{\'o}n and {Villada-Balbuena}, Alejandro and {M{\'e}ndez-Alcaraz}, Jos{\'e} Miguel and {Herrera-Velarde}, Salvador},
  year = {2018},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {149},
  number = {16},
  pages = {164907},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.5049568}
}

@article{basheerArtificialNeuralNetworks2000,
  title = {Artificial Neural Networks: Fundamentals, Computing, Design, and Application},
  shorttitle = {Artificial Neural Networks},
  author = {Basheer, I. A and Hajmeer, M},
  year = {2000},
  month = dec,
  journal = {Journal of Microbiological Methods},
  series = {Neural Computting in Micrbiology},
  volume = {43},
  number = {1},
  pages = {3--31},
  issn = {0167-7012},
  doi = {10.1016/S0167-7012(00)00201-3},
  abstract = {Artificial neural networks (ANNs) are relatively new computational tools that have found extensive utilization in solving many complex real-world problems. The attractiveness of ANNs comes from their remarkable information processing characteristics pertinent mainly to nonlinearity, high parallelism, fault and noise tolerance, and learning and generalization capabilities. This paper aims to familiarize the reader with ANN-based computing (neurocomputing) and to serve as a useful companion practical guide and toolkit for the ANNs modeler along the course of ANN project development. The history of the evolution of neurocomputing and its relation to the field of neurobiology is briefly discussed. ANNs are compared to both expert systems and statistical regression and their advantages and limitations are outlined. A bird's eye review of the various types of ANNs and the related learning rules is presented, with special emphasis on backpropagation (BP) ANNs theory and design. A generalized methodology for developing successful ANNs projects from conceptualization, to design, to implementation, is described. The most common problems that BPANNs developers face during training are summarized in conjunction with possible causes and remedies. Finally, as a practical application, BPANNs were used to model the microbial growth curves of S. flexneri. The developed model was reasonably accurate in simulating both training and test time-dependent growth curves as affected by temperature and pH.},
  langid = {english},
  keywords = {Artificial neural networks,Backpropagation,Growth curves,History,Modeling}
}

@article{baydinAutomaticDifferentiationMachine2018,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {Automatic Differentiation in Machine Learning},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {153},
  pages = {1--43},
  issn = {1533-7928},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply \^aautodiff\^a, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names \^adynamic computational graphs\^a and \^adifferentiable programming\^a. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms \^aautodiff\^a, \^aautomatic differentiation\^a, and \^asymbolic differentiation\^a as these are encountered more and more in machine learning settings.}
}

@book{bealsAnalysisIntroduction2004,
  title = {Analysis: An Introduction},
  shorttitle = {Analysis},
  author = {Beals, Richard},
  year = {2004},
  month = sep,
  publisher = {Cambridge University Press},
  abstract = {This self-contained text, suitable for advanced undergraduates, provides an extensive introduction to mathematical analysis, from the fundamentals to more advanced material. It begins with the properties of the real numbers and continues with a rigorous treatment of sequences, series, metric spaces, and calculus in one variable. Further subjects include Lebesgue measure and integration on the line, Fourier analysis, and differential equations. In addition to this core material, the book includes a number of interesting applications of the subject matter to areas both within and outside the field of mathematics. The aim throughout is to strike a balance between being too austere or too sketchy, and being so detailed as to obscure the essential ideas. A large number of examples and 500 exercises allow the reader to test understanding, practise mathematical exposition and provide a window into further topics.},
  googlebooks = {cXAqJUYqXx0C},
  isbn = {978-0-521-60047-7},
  langid = {english},
  keywords = {Mathematics / Complex Analysis,Mathematics / Functional Analysis,Mathematics / Logic,Mathematics / Mathematical Analysis}
}

@article{beardmoreNumericalBifurcationAnalysis2007,
  title = {A Numerical Bifurcation Analysis of the Ornstein\textendash Zernike Equation with Hypernetted Chain Closure},
  author = {Beardmore, R. E. and Peplow, A. T. and Bresme, F.},
  year = {2007},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {29},
  number = {6},
  pages = {2442--2463},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {1064-8275},
  doi = {10.1137/060650659},
  abstract = {We study the codimension-one and -two bifurcations of the Ornstein\textendash Zernike equation with hypernetted chain (HNC) closure with Lennard\textendash Jones intermolecular interaction potential. The main purpose of the paper is to present the results of a numerical study undertaken using a suite of algorithms implemented in MATLAB and based on pseudo arc-length continuation for the codimension-one case and a Newton-GMRES method for the codimension-two case. Through careful consideration of the results of our computations, an argument is formulated which shows that spinodal isothermal solution branches arising in this model cannot be reproduced numerically. Furthermore, we show that the existence of an upper bound on the density that can be realized on a vapor isothermal solution branch, which must be present at a spinodal, causes the existence of at least one fold bifurcation along that vapor branch when density is used as the bifurcation parameter. This provides an explanation for previous inconclusive attempts to compute solutions using Newton\textendash Picard methods that are popular in the physical chemistry literature.}
}

@article{bedollaMachineLearningCondensed2020,
  title = {Machine Learning for Condensed Matter Physics},
  author = {Bedolla, Edwin and Padierna, Luis Carlos and {Casta{\~n}eda-Priego}, Ram{\'o}n},
  year = {2020},
  month = nov,
  journal = {Journal of Physics: Condensed Matter},
  volume = {33},
  number = {5},
  pages = {053001},
  publisher = {IOP Publishing},
  issn = {0953-8984},
  doi = {10.1088/1361-648X/abb895},
  abstract = {Condensed matter physics (CMP) seeks to understand the microscopic interactions of matter at the quantum and atomistic levels, and describes how these interactions result in both mesoscopic and macroscopic properties. CMP overlaps with many other important branches of science, such as chemistry, materials science, statistical physics, and high-performance computing. With the advancements in modern machine learning (ML) technology, a keen interest in applying these algorithms to further CMP research has created a compelling new area of research at the intersection of both fields. In this review, we aim to explore the main areas within CMP, which have successfully applied ML techniques to further research, such as the description and use of ML schemes for potential energy surfaces, the characterization of topological phases of matter in lattice systems, the prediction of phase transitions in off-lattice and atomistic simulations, the interpretation of ML theories with physics-inspired frameworks and the enhancement of simulation methods with ML algorithms. We also discuss in detail the main challenges and drawbacks of using ML methods on CMP problems, as well as some perspectives for future developments.},
  langid = {english}
}

@article{behlerGeneralizedNeuralNetworkRepresentation2007a,
  title = {Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces},
  author = {Behler, J{\"o}rg and Parrinello, Michele},
  year = {2007},
  month = apr,
  journal = {Physical Review Letters},
  volume = {98},
  number = {14},
  pages = {146401},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.98.146401},
  abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems.}
}

@article{behlerPerspectiveMachineLearning2016a,
  title = {Perspective: Machine Learning Potentials for Atomistic Simulations},
  shorttitle = {Perspective},
  author = {Behler, J{\"o}rg},
  year = {2016},
  month = nov,
  journal = {The Journal of Chemical Physics},
  volume = {145},
  number = {17},
  pages = {170901},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4966192},
  abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.}
}

@article{bernerModernMathematicsDeep2021,
  title = {The Modern Mathematics of Deep Learning},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2021},
  month = may,
  journal = {arXiv:2105.04026 [cs, stat]},
  eprint = {2105.04026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bianchiPredictingPatchyParticle2012,
  title = {Predicting Patchy Particle Crystals: Variable Box Shape Simulations and Evolutionary Algorithms},
  shorttitle = {Predicting Patchy Particle Crystals},
  author = {Bianchi, Emanuela and Doppelbauer, G{\"u}nther and Filion, Laura and Dijkstra, Marjolein and Kahl, Gerhard},
  year = {2012},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {136},
  number = {21},
  pages = {214102},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4722477}
}

@article{biswasTanhSoftDynamicTrainable2021,
  title = {TanhSoft\textemdash Dynamic Trainable Activation Functions for Faster Learning and Better Performance},
  author = {Biswas, Koushik and Kumar, Sandeep and Banerjee, Shilpak and Pandey, Ashish Kumar},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {120613--120623},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3105355},
  abstract = {Deep learning, at its core, contains functions that are the composition of a linear transformation with a nonlinear function known as the activation function. In the past few years, there is an increasing interest in the construction of novel activation functions resulting in better learning. In this work, we propose three novel activation functions with learnable parameters, namely TanhSoft-1, TanhSoft-2, and TanhSoft-3, which are shown to outperform several well-known activation functions. For instance, replacing ReLU with TanhSoft-1, TanhSoft-2, and Tanhsot-3 improves top-1 classification accuracy by 6.06\%, 5.75\%, and 5.38\% respectively on VGG-16(with batch-normalization), by 3.02\%, 3.25\% and 2.93\% respectively on PreActResNet-34 in CIFAR-100 dataset, by 1.76\%, 1.93\%, and 1.82\% respectively on WideResNet 28-10 in Tiny ImageNet dataset. TanhSoft-1, TanhSoft-2, and Tanhsot-3 outperformed ReLU on mean average precision (mAP) by 0.7\%, 0.8\%, and 0.6\% respectively in object detection problem on SSD 300 model in Pascal VOC dataset.},
  keywords = {activation function,Biological neural networks,Deep learning,Licenses,neural network,Neurons,Object detection,Task analysis,Training}
}

@article{boattiniAveragingLocalStructure2021,
  title = {Averaging Local Structure to Predict the Dynamic Propensity in Supercooled Liquids},
  author = {Boattini, Emanuele and Smallenburg, Frank and Filion, Laura},
  year = {2021},
  month = aug,
  journal = {Physical Review Letters},
  volume = {127},
  number = {8},
  pages = {088007},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.127.088007},
  abstract = {Predicting the local dynamics of supercooled liquids based purely on local structure is a key challenge in our quest for understanding glassy materials. Recent years have seen an explosion of methods for making such a prediction, often via the application of increasingly complex machine learning techniques. The best predictions so far have involved so-called Graph Neural Networks (GNNs) whose accuracy comes at a cost of models that involve on the order of 105 fit parameters. In this Letter, we propose that the key structural ingredient to the GNN method is its ability to consider not only the local structure around a central particle, but also averaged structural features centered around nearby particles. We demonstrate that this insight can be exploited to design a significantly more efficient model that provides essentially the same predictive power at a fraction of the computational complexity (approximately 1000 fit parameters), and demonstrate its success by fitting the dynamic propensity of Kob-Andersen and binary hard-sphere mixtures. We then use this to make predictions regarding the importance of radial and angular descriptors in the dynamics of both models.}
}

@article{boattiniModelingManybodyInteractions2020,
  title = {Modeling of Many-Body Interactions between Elastic Spheres through Symmetry Functions},
  author = {Boattini, Emanuele and Bezem, Nina and Punnathanam, Sudeep N. and Smallenburg, Frank and Filion, Laura},
  year = {2020},
  month = aug,
  journal = {The Journal of Chemical Physics},
  volume = {153},
  number = {6},
  pages = {064902},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/5.0015606}
}

@article{boattiniUnsupervisedLearningLocal2019a,
  title = {Unsupervised Learning for Local Structure Detection in Colloidal Systems},
  author = {Boattini, Emanuele and Dijkstra, Marjolein and Filion, Laura},
  year = {2019},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {151},
  number = {15},
  pages = {154901},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.5118867}
}

@article{bomontRenormalizationIndirectCorrelation2001,
  title = {Renormalization of the Indirect Correlation Function to Extract the Bridge Function of Simple Fluids},
  author = {Bomont, J. M. and Bretonnet, J. L.},
  year = {2001},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {114},
  number = {9},
  pages = {4141--4148},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1344610}
}

@incollection{bontempiMachineLearningStrategies2013,
  title = {Machine Learning Strategies for Time Series Forecasting},
  booktitle = {Business Intelligence: Second European Summer School, EBISS 2012, Brussels, Belgium, July 15-21, 2012, Tutorial Lectures},
  author = {Bontempi, Gianluca and Ben Taieb, Souhaib and Le Borgne, Yann-A{\"e}l},
  editor = {Aufaure, Marie-Aude and Zim{\'a}nyi, Esteban},
  year = {2013},
  series = {Lecture Notes in Business Information Processing},
  pages = {62--77},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-36318-4_3},
  abstract = {The increasing availability of large amounts of historical data and the need of performing accurate forecasting of future behavior in several scientific and applied domains demands the definition of robust and efficient techniques able to infer from observations the stochastic dependency between past and future. The forecasting domain has been influenced, from the 1960s on, by linear statistical methods such as ARIMA models. More recently, machine learning models have drawn attention and have established themselves as serious contenders to classical statistical models in the forecasting community. This chapter presents an overview of machine learning techniques in time series forecasting by focusing on three aspects: the formalization of one-step forecasting problems as supervised learning tasks, the discussion of local learning techniques as an effective tool for dealing with temporal data and the role of the forecasting strategy when we move from one-step to multiple-step forecasting.},
  isbn = {978-3-642-36318-4},
  langid = {english},
  keywords = {lazy learning,local learning,machine learning,MIMO,Time series forecasting}
}

@article{boothEfficientSolutionLiquid1999,
  title = {Efficient Solution of Liquid State Integral Equations Using the Newton-GMRES Algorithm},
  author = {Booth, Michael J. and Schlijper, A. G. and Scales, L. E. and Haymet, A. D. J.},
  year = {1999},
  month = jun,
  journal = {Computer Physics Communications},
  volume = {119},
  number = {2},
  pages = {122--134},
  issn = {0010-4655},
  doi = {10.1016/S0010-4655(99)00186-1},
  abstract = {We present examples of the accurate, robust and efficient solution of Ornstein-Zernike type integral equations which describe the structure of both homogeneous and inhomogeneous fluids. In this work we use the Newton-GMRES algorithm as implemented in the public-domain nonlinear Krylov solvers NKSOL [P. Brown, Y. Saad, SIAM J. Sci. Stat. Comput. 11 (1990) 450] and NITSOL [M. Pernice, H.F. Walker, SIAM J. Sci. Comput. 19 (1998) 302]. We compare and contrast this method with more traditional approaches in the literature, using Picard iteration (successive-substitution) and hybrid Newton-Raphson and Picard methods, and a recent vector extrapolation method [H.H.H. Homeier, S. Rast, H. Krienke, Comput. Phys. Commun. 92 (1995) 188]. We find that both the performance and ease of implementation of these nonlinear solvers recommend them for the solution of this class of problem.},
  langid = {english},
  keywords = {GMRES,Inhomogeneous fluids,Krylov,Newton,Nonlinear integral equation,Numerical solution,Ornstein,Raphson,Zernike}
}

@article{byrdTrustRegionAlgorithm1987,
  title = {A Trust Region Algorithm for Nonlinearly Constrained Optimization},
  author = {Byrd, Richard H. and Schnabel, Robert B. and Shultz, Gerald A.},
  year = {1987},
  month = oct,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {24},
  number = {5},
  pages = {1152--1170},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1429},
  doi = {10.1137/0724076},
  abstract = {We present a trust region-based method for the general nonlinearly equality constrained optimization problem. The method works by iteratively minimizing a quadratic model of the Lagrangian subject to a possibly relaxed linearization of the problem constraints and a trust region constraint. The model minimization may be done approximately with a dogleg-type approach. We show that this method is globally convergent even if singular or indefinite Hessian approximations are made. A second order correction step that brings the iterates closer to the feasible set is described. If sufficiently precise Hessian information is used, this correction step allows us to prove that the method is also locally quadratically convergent, and that the limit satisfies the second order necessary conditions for constrained optimization. An example is given to show that, without this correction, a situation similar to the Maratos effect may occur where the iteration is unable to move away from a saddle point.},
  keywords = {49D37,65K05,constrained optimization,trust region}
}

@article{carleoMachineLearningPhysical2019a,
  title = {Machine Learning and the Physical Sciences},
  author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and {Vogt-Maranto}, Leslie and Zdeborov{\'a}, Lenka},
  year = {2019},
  month = dec,
  journal = {Reviews of Modern Physics},
  volume = {91},
  number = {4},
  pages = {045002},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.91.045002},
  abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.}
}

@article{carrasquillaMachineLearningPhases2017a,
  title = {Machine Learning Phases of Matter},
  author = {Carrasquilla, Juan and Melko, Roger G.},
  year = {2017},
  month = may,
  journal = {Nature Physics},
  volume = {13},
  number = {5},
  pages = {431--434},
  publisher = {Nature Publishing Group},
  issn = {1745-2481},
  doi = {10.1038/nphys4035},
  abstract = {The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order.},
  copyright = {2017 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Phase transitions and critical phenomena;Statistical physics Subject\_term\_id: phase-transitions-and-critical-phenomena;statistical-physics}
}

@article{carvalhoIndirectSolutionOrnsteinZernike2020,
  title = {Indirect Solution of Ornstein-Zernike Equation Using the Hopfield Neural Network Method},
  author = {Carvalho, F. S. and Braga, J. P.},
  year = {2020},
  month = oct,
  journal = {Brazilian Journal of Physics},
  volume = {50},
  number = {5},
  pages = {489--494},
  issn = {1678-4448},
  doi = {10.1007/s13538-020-00769-4},
  abstract = {Microscopic information, such as the pair distribution and direct correlation functions, can be obtained from experimental data. From these correlation functions, thermodynamical quantities and the potential interaction function can be recovered. Derivations of Ornstein-Zernike equation and Hopfield Neural Network method are given first, as a theoretical background to follow the present work. From these two frameworks, structural information, such as the radial distribution (g(r)) and direct correlation (C(r)) functions, were retrieved from neutron scattering experimental data. The problem was solved considering simple initial conditions, which does not require any previous information about the system, making it clear the robustness of the Hopfield Neural Network method. The pair interaction potential was estimated in the Percus-Yevick (PY) and hypernetted chain (HNC) approximations and a poor agreement, compared with the Lennard-Jones 6-12 potential, was observed for both cases, suggesting the necessity of a more accurate closure relation to describe the system. In this sense, the Hopfield Neural Network together with experimental information provides an alternative approach to solve the Ornstein-Zernike equations, avoiding the limitations imposed by the closure relation.},
  langid = {english}
}

@article{catsMachinelearningFreeenergyFunctionals2021,
  title = {Machine-Learning Free-Energy Functionals Using Density Profiles from Simulations},
  author = {Cats, Peter and Kuipers, Sander and {de Wind}, Sacha and {van Damme}, Robin and Coli, Gabriele M. and Dijkstra, Marjolein and {van Roij}, Ren{\'e}},
  year = {2021},
  month = mar,
  journal = {APL Materials},
  volume = {9},
  number = {3},
  pages = {031109},
  publisher = {American Institute of Physics},
  doi = {10.1063/5.0042558},
  abstract = {The formally exact framework of equilibrium Density Functional Theory (DFT) is capable of simultaneously and consistently describing thermodynamic and structural properties of interacting many-body systems in arbitrary external potentials. In practice, however, DFT hinges on approximate (free-)energy functionals from which density profiles (and hence the thermodynamic potential) follow via an Euler\textendash Lagrange equation. Here, we explore a relatively simple Machine-Learning (ML) approach to improve the standard mean-field approximation of the excess Helmholtz free-energy functional of a 3D Lennard-Jones system at a supercritical temperature. The learning set consists of density profiles from grand-canonical Monte Carlo simulations of this system at varying chemical potentials and external potentials in a planar geometry only. Using the DFT formalism, we nevertheless can extract not only very accurate 3D bulk equations of state but also radial distribution functions using the Percus test-particle method. Unfortunately, our ML approach did not provide very reliable Ornstein\textendash Zernike direct correlation functions for small distances.}
}

@article{chenUniversalApproximationNonlinear1995,
  title = {Universal Approximation to Nonlinear Operators by Neural Networks with Arbitrary Activation Functions and Its Application to Dynamical Systems},
  author = {Chen, Tianping and Chen, Hong},
  year = {1995},
  month = jul,
  journal = {IEEE Transactions on Neural Networks},
  volume = {6},
  number = {4},
  pages = {911--917},
  issn = {1941-0093},
  doi = {10.1109/72.392253},
  abstract = {The purpose of this paper is to investigate neural network capability systematically. The main results are: 1) every Tauber-Wiener function is qualified as an activation function in the hidden layer of a three-layered neural network; 2) for a continuous function in S'(R/sup 1/) to be a Tauber-Wiener function, the necessary and sufficient condition is that it is not a polynomial; 3) the capability of approximating nonlinear functionals defined on some compact set of a Banach space and nonlinear operators has been shown; and 4) the possibility by neural computation to approximate the output as a whole (not at a fixed point) of a dynamical system, thus identifying the system.{$<>$}},
  keywords = {Computer networks,H infinity control,Integral equations,Kernel,Mathematics,Neural networks,Nonlinear dynamical systems,Polynomials,Sufficient conditions,Sun}
}

@article{choudhuryIntegralEquationTheory2002,
  title = {Integral Equation Theory of Lennard-Jones Fluids: A Modified Verlet Bridge Function Approach},
  shorttitle = {Integral Equation Theory of Lennard-Jones Fluids},
  author = {Choudhury, Niharendu and Ghosh, Swapan K.},
  year = {2002},
  month = may,
  journal = {The Journal of Chemical Physics},
  volume = {116},
  number = {19},
  pages = {8517--8522},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1467894}
}

@article{cohenLearningCurvesOverparametrized2021,
  title = {Learning Curves for Overparametrized Deep Neural Networks: A Field Theory Perspective},
  shorttitle = {Learning Curves for Overparametrized Deep Neural Networks},
  author = {Cohen, Omry and Malka, Or and Ringel, Zohar},
  year = {2021},
  month = apr,
  journal = {Physical Review Research},
  volume = {3},
  number = {2},
  pages = {023034},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.3.023034},
  abstract = {In the past decade, deep neural networks (DNNs) came to the fore as the leading machine-learning algorithms for a variety of tasks. Their rise was founded on market needs and engineering craftsmanship, the latter based more on trial and error than on theory. While still far behind the application forefront, the theoretical study of DNNs has recently made important advancements in analyzing the highly overparametrized regime where some exact results have been obtained. Leveraging these ideas and adopting a more physicslike approach, here we construct a versatile field theory formalism for supervised deep learning, involving renormalization group, Feynman diagrams, and replicas. In particular, we show that our approach leads to highly accurate predictions of learning curves of truly deep DNNs trained on polynomial regression problems. It also explains in a concrete manner why DNNs generalize well despite being highly overparametrized, this due to an entropic bias to simple functions which, for the case of fully connected DNNs with data sampled on the hypersphere, are low-order polynomials in the input vector. Being a complex interacting system of artificial neurons, we believe that such tools and methodologies borrowed from condensed matter physics would prove essential for obtaining an accurate quantitative understanding of deep learning.}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english}
}

@article{degennesSoftMatter1992,
  title = {Soft Matter},
  author = {{de Gennes}, P. G.},
  year = {1992},
  month = jul,
  journal = {Reviews of Modern Physics},
  volume = {64},
  number = {3},
  pages = {645--648},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.64.645},
  abstract = {DOI:https://doi.org/10.1103/RevModPhys.64.645}
}

@book{dhontIntroductionDynamicsColloids1996,
  title = {An Introduction to Dynamics of Colloids},
  author = {Dhont, J. K. G.},
  year = {1996},
  month = may,
  publisher = {Elsevier},
  abstract = {One of the few textbooks in the field, this volume deals with several aspects of the dynamics of colloids. A self-contained treatise, it fills the gap between research literature and existing books for graduate students and researchers. For readers with a background in chemistry, the first chapter contains a section on frequently used mathematical techniques, as well as statistical mechanics.Some of the topics covered include:\textbullet{} diffusion of free particles on the basis of the Langevin equation\textbullet the separation of time, length and angular scales;\textbullet{} the fundamental Fokker-Planck and Smoluchowski equations derived for interacting particles\textbullet{} friction of spheres and rods, and hydrodynamic interaction of spheres (including three body interactions)\textbullet{} diffusion, sedimentation, critical phenomena and phase separation kinetics\textbullet{} experimental light scattering results.For universities and research departments in industry this textbook makes vital reading.},
  googlebooks = {mmArTF5SJ9oC},
  isbn = {978-0-08-053507-4},
  langid = {english},
  keywords = {Science / Chemistry / Computational \& Molecular Modeling,Science / Chemistry / Physical \& Theoretical,Science / Mechanics / Fluids,Technology \& Engineering / Chemical \& Biochemical}
}

@article{dijkstraPhaseDiagramHighly1999,
  title = {Phase Diagram of Highly Asymmetric Binary Hard-Sphere Mixtures},
  author = {Dijkstra, Marjolein and {van Roij}, Ren{\'e} and Evans, Robert},
  year = {1999},
  month = may,
  journal = {Physical Review E},
  volume = {59},
  number = {5},
  pages = {5744--5771},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.59.5744},
  abstract = {We study the phase behavior and structure of highly asymmetric binary hard-sphere mixtures. By first integrating out the degrees of freedom of the small spheres in the partition function we derive a formal expression for the effective Hamiltonian of the large spheres. Then using an explicit pairwise (depletion) potential approximation to this effective Hamiltonian in computer simulations, we determine fluid-solid coexistence for size ratios q=0.033,0.05,0.1,0.2, and 1.0. The resulting two-phase region becomes very broad in packing fractions of the large spheres as q becomes very small. We find a stable, isostructural solid-solid transition for q{$<\sptilde$}0.05 and a fluid-fluid transition for q{$<\sptilde$}0.10. However, the latter remains metastable with respect to the fluid-solid transition for all size ratios we investigate. In the limit \textrightarrow q0 the phase diagram mimics that of the sticky-sphere system. As expected, the radial distribution function g(r) and the structure factor S(k) of the effective one-component system show no sharp signature of the onset of the freezing transition and we find that at most points on the fluid-solid boundary the value of S(k) at its first peak is much lower than the value given by the Hansen-Verlet freezing criterion. Direct simulations of the true binary mixture of hard spheres were performed for q{$>\sptilde$}0.05 in order to test the predictions from the effective Hamiltonian. For those packing fractions of the small spheres where direct simulations are possible, we find remarkably good agreement between the phase boundaries calculated from the two approaches\textemdash even up to the symmetric limit q=1 and for very high packings of the large spheres, where the solid-solid transition occurs. In both limits one might expect that an approximation which neglects higher-body terms should fail, but our results support the notion that the main features of the phase equilibria of asymmetric binary hard-sphere mixtures are accounted for by the effective pairwise depletion potential description. We also compare our results with those of other theoretical treatments and experiments on colloidal hard-sphere mixtures.}
}

@article{dijkstraPredictiveModellingMachine2021a,
  title = {From Predictive Modelling to Machine Learning and Reverse Engineering of Colloidal Self-Assembly},
  author = {Dijkstra, Marjolein and Luijten, Erik},
  year = {2021},
  month = jun,
  journal = {Nature Materials},
  volume = {20},
  number = {6},
  pages = {762--773},
  publisher = {Nature Publishing Group},
  issn = {1476-4660},
  doi = {10.1038/s41563-021-01014-2},
  abstract = {An overwhelming diversity of colloidal building blocks with distinct sizes, materials and tunable interaction potentials are now available for colloidal self-assembly. The application space for materials composed of these building blocks is vast. To make progress in the rational design of new self-assembled materials, it is desirable to guide the experimental synthesis efforts by computational modelling. Here, we discuss computer simulation methods and strategies used for the design of soft materials created through bottom-up self-assembly of colloids and nanoparticles. We describe simulation techniques for investigating the self-assembly behaviour of colloidal suspensions, including crystal structure prediction methods, phase diagram calculations and enhanced sampling techniques, as well as their limitations. We also discuss the recent surge of interest in machine learning and reverse-engineering methods. Although their implementation in the colloidal realm is still in its infancy, we anticipate that these data-science tools offer new paradigms in understanding, predicting and (inverse) design of novel colloidal materials.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Materials science;Nanoscale materials;Statistical physics, thermodynamics and nonlinear dynamics Subject\_term\_id: materials-science;nanoscale-materials;statistical-physics-thermodynamics-and-nonlinear-dynamics}
}

@article{dorigoAntColonySystem1997,
  title = {Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem},
  shorttitle = {Ant Colony System},
  author = {Dorigo, M. and Gambardella, L.M.},
  year = {1997},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {53--66},
  issn = {1941-0026},
  doi = {10.1109/4235.585892},
  abstract = {This paper introduces the ant colony system (ACS), a distributed algorithm that is applied to the traveling salesman problem (TSP). In the ACS, a set of cooperating agents called ants cooperate to find good solutions to TSPs. Ants cooperate using an indirect form of communication mediated by a pheromone they deposit on the edges of the TSP graph while building solutions. We study the ACS by running experiments to understand its operation. The results show that the ACS outperforms other nature-inspired algorithms such as simulated annealing and evolutionary computation, and we conclude comparing ACS-3-opt, a version of the ACS augmented with a local search procedure, to some of the best performing algorithms for symmetric and asymmetric TSPs.},
  keywords = {Ant colony optimization,Computational modeling,Distributed algorithms,Evolutionary computation,Feedback,Global communication,Legged locomotion,Simulated annealing,Traveling salesman problems}
}

@incollection{duchWhatComputationalIntelligence2007,
  title = {What Is Computational Intelligence and Where Is It Going?},
  booktitle = {Challenges for Computational Intelligence},
  author = {Duch, W{\l}odzis{\l}aw},
  editor = {Duch, W{\l}odzis{\l}aw and Ma{\'n}dziuk, Jacek},
  year = {2007},
  series = {Studies in Computational Intelligence},
  pages = {1--13},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-71984-7_1},
  abstract = {SummaryWhat is Computational Intelligence (CI) and what are its relations with Artificial Intelligence (AI)? A brief survey of the scope of CI journals and books with ``computational intelligence'' in their title shows that at present it is an umbrella for three core technologies (neural, fuzzy and evolutionary), their applications, and selected fashionable pattern recognition methods. At present CI has no comprehensive foundations and is more a bag of tricks than a solid branch of science. The change of focus from methods to challenging problems is advocated, with CI defined as a part of computer and engineering sciences devoted to solution of non-algoritmizable problems. In this view AI is a part of CI focused on problems related to higher cognitive functions, while the rest of the CI community works on problems related to perception and control, or lower cognitive functions. Grand challenges on both sides of this spectrum are addressed.},
  isbn = {978-3-540-71984-7},
  langid = {english},
  keywords = {Computational Intelligence,Fuzzy System,Grand Challenge,High Cognitive Function,Open Access Journal}
}

@article{dunjkoMachineLearningArtificial2018,
  title = {Machine Learning \& Artificial Intelligence in the Quantum Domain: A Review of Recent Progress},
  shorttitle = {Machine Learning \& Artificial Intelligence in the Quantum Domain},
  author = {Dunjko, Vedran and Briegel, Hans J.},
  year = {2018},
  month = jun,
  journal = {Reports on Progress in Physics},
  volume = {81},
  number = {7},
  pages = {074001},
  publisher = {IOP Publishing},
  issn = {0034-4885},
  doi = {10.1088/1361-6633/aab406},
  abstract = {Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research\textemdash quantum information versus machine learning (ML) and artificial intelligence (AI)\textemdash have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our `big data' world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement\textemdash exploring what ML/AI can do for quantum physics and vice versa\textemdash researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.},
  langid = {english}
}

@article{edelmanHowManyZeros1995,
  title = {How Many Zeros of a Random Polynomial Are Real?},
  author = {Edelman, Alan and Kostlan, Eric},
  year = {1995},
  month = jan,
  journal = {Bulletin of the American Mathematical Society},
  volume = {32},
  number = {1},
  pages = {1--38},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-1995-00571-9},
  abstract = {We provide an elementary geometric derivation of the Kac integral formula for the expected number of real zeros of a random polynomial with independent standard normally distributed coefficients. We show that the expected number of real zeros is simply the length of the moment curve (1, t, ... , t") projected onto the surface of the unit sphere, divided by n . The probability density of the real zeros is proportional to how fast this curve is traced out. We then relax Kac's assumptions by considering a variety of random sums, series, and distributions, and we also illustrate such ideas as integral geometry and the Fubini-Study metric.},
  langid = {english}
}

@book{egelstaffIntroductionLiquidState2012,
  title = {An Introduction to the Liquid State},
  author = {Egelstaff, P.},
  year = {2012},
  month = dec,
  publisher = {Elsevier},
  abstract = {An Introduction to the Liquid State focuses on the atomic motions and positions of liquids. Particularly given importance in this book are internal motion of molecules as a whole and the motion of atoms in a monatomic liquid. Divided into 16 chapters, the book opens by outlining the general properties of liquids, including a comparison of liquid argon and liquid sodium, discussions on theories and methods of studying the liquid state, and thermodynamic relationships. The book proceeds by defining the molecular distribution functions and equation of state, the potential function for non-conducting liquids and metals, and measurement of pair distribution function. Numerical analyses and representations are provided to simplify the functions of equations. The book discusses equilibrium properties wherein calculations on the state of gases and fluids are presented. The text also underlines space and time dependent correlation functions. Given emphasis in this part are neutron scattering, electromagnetic radiation, and various radiation scattering techniques. Other concerns discussed are diffusion and single particle motion, velocity of correlation function, diffusion and viscosity coefficients, liquid-gas critical point, and a comparison of classical and quantum liquids. The selection is a valuable source of information for readers wanting to study the composition and reactions of liquids.},
  isbn = {978-0-323-15903-6},
  langid = {english},
  keywords = {Science / Physics / General}
}

@article{evansSimpleLiquidsColloids2019,
  title = {From Simple Liquids to Colloids and Soft Matter},
  author = {Evans, Robert and Frenkel, Daan and Dijkstra, Marjolein},
  year = {2019},
  month = feb,
  journal = {Physics Today},
  volume = {72},
  number = {2},
  pages = {38--39},
  publisher = {American Institute of Physics},
  issn = {0031-9228},
  doi = {10.1063/PT.3.4135}
}

@article{fengPerformanceAnalysisVarious2019,
  title = {Performance Analysis of Various Activation Functions in Artificial Neural Networks},
  author = {Feng, Jianli and Lu, Shengnan},
  year = {2019},
  month = jun,
  volume = {1237},
  pages = {022030},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1237/2/022030},
  abstract = {The development of Artificial Neural Networks (ANNs) has achieved a lot of fruitful results so far, and we know that activation function is one of the principal factors which will affect the performance of the networks. In this work, the role of many different types of activation functions, as well as their respective advantages and disadvantages and applicable fields are discussed, so people can choose the appropriate activation functions to get the superior performance of ANNs.},
  langid = {english}
}

@article{fogelWhatEvolutionaryComputation2000,
  title = {What Is Evolutionary Computation?},
  author = {Fogel, D.B.},
  year = {2000},
  month = feb,
  journal = {IEEE Spectrum},
  volume = {37},
  number = {2},
  pages = {26--32},
  issn = {1939-9340},
  doi = {10.1109/6.819926},
  abstract = {Taking a page from Darwin's 'On the origin of the species', computer scientists have found ways to evolve solutions to complex problems. Harnessing the evolutionary process within a computer provides a means for addressing complex engineering problems-ones involving chaotic disturbances, randomness, and complex nonlinear dynamics-that traditional algorithms have been unable to conquer. Indeed, the field of evolutionary computation is one of the fastest growing areas of computer science and engineering for just this reason; it is addressing many problems that were previously beyond reach, such as rapid design of medicines, flexible solutions to supply-chain management problems, and rapid analysis of battlefield tactics for defense. Potentially, the field may fulfil the dream of artificial intelligence: a computer that can learn on its own and become an expert in any chosen area.},
  keywords = {Accidents,Chaos,Computer science,Cost function,Evolution (biology),Evolutionary computation,Heuristic algorithms,Joining processes,Organisms,Sea measurements}
}

@article{fontenla-romeroNewConvexObjective2010,
  title = {A New Convex Objective Function for the Supervised Learning of Single-Layer Neural Networks},
  author = {{Fontenla-Romero}, Oscar and {Guijarro-Berdi{\~n}as}, Bertha and {P{\'e}rez-S{\'a}nchez}, Beatriz and {Alonso-Betanzos}, Amparo},
  year = {2010},
  month = may,
  journal = {Pattern Recognition},
  volume = {43},
  number = {5},
  pages = {1984--1992},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2009.11.024},
  abstract = {This paper proposes a novel supervised learning method for single-layer feedforward neural networks. This approach uses an alternative objective function to that based on the MSE, which measures the errors before the neuron's nonlinear activation functions instead of after them. In this case, the solution can be easily obtained solving systems of linear equations, i.e., requiring much less computational power than the one associated with the regular methods. A theoretical study is included to proof the approximated equivalence between the global optimum of the objective function based on the regular MSE criterion and the one of the proposed alternative MSE function. Furthermore, it is shown that the presented method has the capability of allowing incremental and distributed learning. An exhaustive experimental study is also presented to verify the soundness and efficiency of the method. This study contains 10 classification and 16 regression problems. In addition, a comparison with other high performance learning algorithms shows that the proposed method exhibits, in average, the highest performance and low-demanding computational requirements.},
  langid = {english},
  keywords = {Convex optimization,Global optimum,Incremental learning,Least squares,Single-layer neural networks,Supervised learning method}
}

@article{forresterRecentAdvancesSurrogatebased2009,
  title = {Recent Advances in Surrogate-Based Optimization},
  author = {Forrester, Alexander I. J. and Keane, Andy J.},
  year = {2009},
  month = jan,
  journal = {Progress in Aerospace Sciences},
  volume = {45},
  number = {1},
  pages = {50--79},
  issn = {0376-0421},
  doi = {10.1016/j.paerosci.2008.11.001},
  abstract = {The evaluation of aerospace designs is synonymous with the use of long running and computationally intensive simulations. This fuels the desire to harness the efficiency of surrogate-based methods in aerospace design optimization. Recent advances in surrogate-based design methodology bring the promise of efficient global optimization closer to reality. We review the present state of the art of constructing surrogate models and their use in optimization strategies. We make extensive use of pictorial examples and, since no method is truly universal, give guidance as to each method's strengths and weaknesses.},
  langid = {english}
}

@book{frenkelUnderstandingMolecularSimulation2001,
  title = {Understanding Molecular Simulation: From Algorithms to Applications},
  shorttitle = {Understanding Molecular Simulation},
  author = {Frenkel, Daan and Smit, B.},
  year = {2001},
  month = oct,
  publisher = {Elsevier},
  abstract = {Understanding Molecular Simulation: From Algorithms to Applications explains the physics behind the \&quot;recipes\&quot; of molecular simulation for materials science. Computer simulators are continuously confronted with questions concerning the choice of a particular technique for a given application. A wide variety of tools exist, so the choice of technique requires a good understanding of the basic principles. More importantly, such understanding may greatly improve the efficiency of a simulation program. The implementation of simulation methods is illustrated in pseudocodes and their practical use in the case studies used in the text. Since the first edition only five years ago, the simulation world has changed significantly -- current techniques have matured and new ones have appeared. This new edition deals with these new developments; in particular, there are sections on:  Transition path sampling and diffusive barrier crossing to simulaterare eventsDissipative particle dynamic as a course-grained simulation techniqueNovel schemes to compute the long-ranged forcesHamiltonian and non-Hamiltonian dynamics in the context constant-temperature and constant-pressure molecular dynamics simulationsMultiple-time step algorithms as an alternative for constraintsDefects in solidsThe pruned-enriched Rosenbluth sampling, recoil-growth, and concerted rotations for complex moleculesParallel tempering for glassy Hamiltonians Examples are included that highlight current applications and the codes of case studies are available on the World Wide Web. Several new examples have been added since the first edition to illustrate recent applications. Questions are included in this new edition. No prior knowledge of computer simulation is assumed.},
  googlebooks = {5qTzldS9ROIC},
  isbn = {978-0-08-051998-2},
  langid = {english},
  keywords = {Computers / Design; Graphics \& Media / Graphics Tools,Science / Physics / Atomic \& Molecular,Science / Physics / Mathematical \& Computational,Technology \& Engineering / Materials Science / General}
}

@inproceedings{fukushimaProposalDistanceweightedExponential2011,
  title = {Proposal of Distance-Weighted Exponential Natural Evolution Strategies},
  booktitle = {2011 IEEE Congress of Evolutionary Computation (CEC)},
  author = {Fukushima, Nobusumi and Nagata, Yuichi and Kobayashi, Sigenobu and Ono, Isao},
  year = {2011},
  month = jun,
  pages = {164--171},
  issn = {1941-0026},
  doi = {10.1109/CEC.2011.5949614},
  abstract = {This paper presents a new evolutionary algorithm for function optimization named the distance-weighted exponential natural evolution strategies (DX-NES). DX-NES remedies two problems of a conventional method, the exponential natural evolution strategies (xNES), that shows good performance when it does not need to move the distribution for sampling individuals down the slope to the optimal point. The first problem of xNES is that the search efficiency deteriorates while the distribution moves down the slope of an ill-scaled function because it degenerates before reaching the optimal point. The second problem is that the settings of learning rates are inappropriate because they do not taking account of some factors affecting the estimate accuracy of the natural gradient. We compared the performance of DX-NES with that of xNES and CMA-ES on typical benchmark functions and confirmed that DX-NES outperformed the xNES on all the benchmark functions and that DX-NES showed better performance than CMA-ES on the almost all functions except the k-tablet function.},
  keywords = {Accuracy,Benchmark testing,Convergence,Covariance matrix,Gaussian distribution,Optimization,Search problems}
}

@article{gaoImplementingNelderMeadSimplex2012,
  title = {Implementing the Nelder-Mead Simplex Algorithm with~Adaptive Parameters},
  author = {Gao, Fuchang and Han, Lixing},
  year = {2012},
  month = jan,
  journal = {Computational Optimization and Applications},
  volume = {51},
  number = {1},
  pages = {259--277},
  issn = {1573-2894},
  doi = {10.1007/s10589-010-9329-3},
  abstract = {In this paper, we first prove that the expansion and contraction steps of the Nelder-Mead simplex algorithm possess a descent property when the objective function is uniformly convex. This property provides some new insights on why the standard Nelder-Mead algorithm becomes inefficient in high dimensions. We then propose an implementation of the Nelder-Mead method in which the expansion, contraction, and shrink parameters depend on the dimension of the optimization problem. Our numerical experiments show that the new implementation outperforms the standard Nelder-Mead method for high dimensional problems.},
  langid = {english}
}

@article{gastSimpleOrderingComplex1998,
  title = {Simple Ordering in Complex Fluids},
  author = {Gast, Alice P. and Russel, William B.},
  year = {1998},
  month = dec,
  journal = {Physics Today},
  volume = {51},
  number = {12},
  pages = {24--30},
  publisher = {American Institute of Physics},
  issn = {0031-9228},
  doi = {10.1063/1.882495}
}

@article{gelbartNewScienceComplex1996,
  title = {The ``New'' Science of ``Complex Fluids''},
  author = {Gelbart, William M. and {Ben-Shaul}, Avinoam},
  year = {1996},
  month = jan,
  journal = {The Journal of Physical Chemistry},
  volume = {100},
  number = {31},
  pages = {13169--13189},
  publisher = {American Chemical Society},
  issn = {0022-3654},
  doi = {10.1021/jp9606570},
  abstract = {We present an overview of the modern study of complex fluids which, because of the overwhelming breadth and richness of this field, unavoidably neglects many interesting systems and research developments. In proposing a definition of the field, we discuss first the special role played by phenomenological theory and the limitations of molecular-level description. The remainder of the article is organized into sections which treat model colloids, micellized surfactant solutions, interfacial films and microemulsions, bilayers and membranes, and new materials. In each instance we try to provide a physical basis for the special nature of interactions and long-range ordering transitions in these novel colloidal and thin layer systems. At the heart of understanding these highly varied phenomena lie the curvature dependence of surface energies and the coupling between self-assembly on small length scales and phase changes at large ones.}
}

@book{gibbsElementaryPrinciplesStatistical2014,
  title = {Elementary Principles in Statistical Mechanics},
  author = {Gibbs, J. Willard},
  year = {2014},
  month = dec,
  publisher = {Courier Corporation},
  abstract = {Written by J. Willard Gibbs, the most distinguished American mathematical physicist of the nineteenth century, this book was the first to bring together and arrange in logical order the works of Clausius, Maxwell, Boltzmann, and Gibbs himself. The lucid, advanced-level text remains a valuable collection of fundamental equations and principles. Topics include the general problem and the fundamental equation of statistical mechanics, the canonical distribution of the average energy values in a canonical ensemble of systems, and formulas for evaluating important functions of the energies of a system. Additional discussions cover maximum and minimal properties of distribution in phase, a valuable comparison of statistical mechanics with thermodynamics, and many other subjects.},
  googlebooks = {tB15BAAAQBAJ},
  isbn = {978-0-486-78995-8},
  langid = {english},
  keywords = {History / General,Science / Mechanics / General,Science / Physics / General}
}

@article{gillanNewMethodSolving1979,
  title = {A New Method of Solving the Liquid Structure Integral Equations},
  author = {Gillan, M. J.},
  year = {1979},
  month = dec,
  journal = {Molecular Physics},
  volume = {38},
  number = {6},
  pages = {1781--1794},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268977900102861},
  abstract = {We present a new method of obtaining numerical solutions to the Percus-Yevick and hypernetted chain equations for liquid structure. The method, which is rapidly convergent and very stable, is a hybrid of the traditional iterative scheme and the Newton-Raphson technique. We show by numerical tests for typical potentials that the method gives well-converged solutions in 20 or 30 iterations even for very high densities. The number of iterations needed is insensitive to the choice of initial estimate, even if this is extremely inaccurate.},
  annotation = {\_eprint: https://doi.org/10.1080/00268977900102861}
}

@inproceedings{glasmachersExponentialNaturalEvolution2010,
  title = {Exponential Natural Evolution Strategies},
  booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
  author = {Glasmachers, Tobias and Schaul, Tom and Yi, Sun and Wierstra, Daan and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = jul,
  series = {GECCO '10},
  pages = {393--400},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1830483.1830557},
  abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization by following the natural gradient of the expected fitness. Like the well-known CMA-ES, the most competitive algorithm in the field, NES comes with important invariance properties. In this paper, we introduce a number of elegant and efficient improvements of the basic NES algorithm. First, we propose to parameterize the positive definite covariance matrix using the exponential map, which allows the covariance matrix to be updated in a vector space. This new technique makes the algorithm completely invariant under linear transformations of the underlying search space, which was previously achieved only in the limit of small step sizes. Second, we compute all updates in the natural coordinate system, such that the natural gradient coincides with the vanilla gradient. This way we avoid the computation of the inverse Fisher information matrix, which is the main computational bottleneck of the original NES algorithm. Our new algorithm, exponential NES (xNES), is significantly simpler than its predecessors. We show that the various update rules in CMA-ES are closely related to the natural gradient updates of xNES. However, xNES is more principled than CMA-ES, as all the update rules needed for covariance matrix adaptation are derived from a single principle. We empirically assess the performance of the new algorithm on standard benchmark functions},
  isbn = {978-1-4503-0072-8},
  keywords = {black box optimization,evolution strategies,natural gradient,unconstrained optimization}
}

@inproceedings{glasmachersExponentialNaturalEvolution2010a,
  title = {Exponential Natural Evolution Strategies},
  booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
  author = {Glasmachers, Tobias and Schaul, Tom and Yi, Sun and Wierstra, Daan and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = jul,
  series = {GECCO '10},
  pages = {393--400},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1830483.1830557},
  abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization by following the natural gradient of the expected fitness. Like the well-known CMA-ES, the most competitive algorithm in the field, NES comes with important invariance properties. In this paper, we introduce a number of elegant and efficient improvements of the basic NES algorithm. First, we propose to parameterize the positive definite covariance matrix using the exponential map, which allows the covariance matrix to be updated in a vector space. This new technique makes the algorithm completely invariant under linear transformations of the underlying search space, which was previously achieved only in the limit of small step sizes. Second, we compute all updates in the natural coordinate system, such that the natural gradient coincides with the vanilla gradient. This way we avoid the computation of the inverse Fisher information matrix, which is the main computational bottleneck of the original NES algorithm. Our new algorithm, exponential NES (xNES), is significantly simpler than its predecessors. We show that the various update rules in CMA-ES are closely related to the natural gradient updates of xNES. However, xNES is more principled than CMA-ES, as all the update rules needed for covariance matrix adaptation are derived from a single principle. We empirically assess the performance of the new algorithm on standard benchmark functions},
  isbn = {978-1-4503-0072-8},
  keywords = {black box optimization,evolution strategies,natural gradient,unconstrained optimization}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep Sparse Rectifier Neural Networks},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {JMLR Workshop and Conference Proceedings},
  issn = {1938-7228},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
  langid = {english}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {JMLR Workshop and Conference Proceedings},
  issn = {1938-7228},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
  langid = {english}
}

@book{goldsteinClassicalMechanics2002,
  title = {Classical Mechanics},
  author = {Goldstein, Herbert and Poole, Charles P. and Safko, John L.},
  year = {2002},
  publisher = {Addison Wesley},
  abstract = {For thirty years this has been the acknowledged standard in advanced classical mechanics courses. This text enables students to make connections between classical and modern physics. In this edition, Beams Medal winner Charles Poole and John Safko have updated the text to include the latest topics, applications, and notation, to reflect today's physics curriculum. They introduce students to the increasingly important role that nonlinearities play in contemporary applications of classical mechanics. Numerical exercises help students to develop skills in how to use computer techniques to solve problems in physics. Mathematical techniques are presented in detail so that the text remains fully accessible to students who have not had an intermediate course in classical mechanics.},
  googlebooks = {tJCuQgAACAAJ},
  isbn = {978-0-201-65702-9},
  langid = {english},
  keywords = {Science / Physics / General}
}

@article{goodallDatadrivenApproximationsBridge2021,
  title = {Data-Driven Approximations to the Bridge Function Yield Improved Closures for the Ornstein\textendash Zernike Equation},
  author = {Goodall, Rhys E. A. and Lee, Alpha A.},
  year = {2021},
  month = jun,
  journal = {Soft Matter},
  volume = {17},
  number = {21},
  pages = {5393--5400},
  publisher = {The Royal Society of Chemistry},
  issn = {1744-6848},
  doi = {10.1039/D1SM00402F},
  abstract = {A key challenge for soft materials design and coarse-graining simulations is determining interaction potentials between components that give rise to desired condensed-phase structures. In theory, the Ornstein\textendash Zernike equation provides an elegant framework for solving this inverse problem. Pioneering work in liquid state theory derived analytical closures for the framework. However, these analytical closures are approximations, valid only for specific classes of interaction potentials. In this work, we combine the physics of liquid state theory with machine learning to infer a closure directly from simulation data. The resulting closure is more accurate than commonly used closures across a broad range of interaction potentials.},
  langid = {english}
}

@article{goodallInferenceUniversalOrnsteinZernike,
  title = {Inference of a Universal Ornstein-Zernike Closure Relationship with Machine Learning},
  author = {Goodall, Rhys E A and Lee, Alpha A},
  pages = {6},
  abstract = {The Ornstein-Zernike framework provides an elegant route for solving the inverse problem of determining a pairwise interaction potential for a liquid given its structure. However, in order to realise the potential of the formalism superior closure relationships are required. Current approximate closure relationships have been shown to have restricted universality and give rise to thermodynamic inconsistencies. In this work rather than attempting to analytically derive a new closure relationship we return to the point of the approximation and investigate whether machine learning can be used to infer a universal closure for the framework directly from simulation data. We show preliminary results that indicate this is a fruitful approach and identify areas for further work.},
  langid = {english}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {MIT Press},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  googlebooks = {omivDQAAQBAJ},
  isbn = {978-0-262-33737-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science}
}

@article{grzybowskiSelfassemblyCrystalsCells2009,
  title = {Self-Assembly: From Crystals to Cells},
  shorttitle = {Self-Assembly},
  author = {Grzybowski, Bartosz A. and Wilmer, Christopher E. and Kim, Jiwon and Browne, Kevin P. and Bishop, Kyle J. M.},
  year = {2009},
  month = mar,
  journal = {Soft Matter},
  volume = {5},
  number = {6},
  pages = {1110--1128},
  publisher = {The Royal Society of Chemistry},
  issn = {1744-6848},
  doi = {10.1039/B819321P},
  abstract = {Self-assembly (SA) is the process in which a system's components\textemdash be it molecules, polymers, colloids, or macroscopic particles\textemdash organize into ordered and/or functional structures without human intervention. The main challenge in SA research is the ability to ``program'' the properties of the individual pieces such that they organize into a desired structure. Although a general strategy for doing so is still elusive, heuristic rules can be formulated that guide design of SA under various conditions and thermodynamic constraints. This Review examines SA in both the equilibrium and non-equilibrium/dynamic systems and discusses different SA modalities: energy driven, entropy-driven, templated, and field-directed. Non-equilibrium SA is discussed as a route to reconfigurable (``adaptive'') materials, and its connection to biological systems is emphasized.},
  langid = {english}
}

@book{hammersleyMonteCarloMethods2013,
  title = {Monte Carlo Methods},
  author = {Hammersley, J.},
  year = {2013},
  month = mar,
  publisher = {Springer Science \& Business Media},
  abstract = {This monograph surveys the present state of Monte Carlo methods. we have dallied with certain topics that have interested us Although personally, we hope that our coverage of the subject is reasonably complete; at least we believe that this book and the references in it come near to exhausting the present range of the subject. On the other hand, there are many loose ends; for example we mention various ideas for variance reduction that have never been seriously appli(:d in practice. This is inevitable, and typical of a subject that has remained in its infancy for twenty years or more. We are convinced Qf:ver theless that Monte Carlo methods will one day reach an impressive maturity. The main theoretical content of this book is in Chapter 5; some readers may like to begin with this chapter, referring back to Chapters 2 and 3 when necessary. Chapters 7 to 12 deal with applications of the Monte Carlo method in various fields, and can be read in any order. For the sake of completeness, we cast a very brief glance in Chapter 4 at the direct simulation used in industrial and operational research, where the very simplest Monte Carlo techniques are usually sufficient. We assume that the reader has what might roughly be described as a 'graduate' knowledge of mathematics. The actual mathematical techniques are, with few exceptions, quite elementary, but we have freely used vectors, matrices, and similar mathematical language for the sake of conciseness.},
  googlebooks = {3rDvCAAAQBAJ},
  isbn = {978-94-009-5819-7},
  langid = {english},
  keywords = {Philosophy / General,Science / General,Social Science / General}
}

@book{hammingNumericalMethodsScientists2012,
  title = {Numerical Methods for Scientists and Engineers},
  author = {Hamming, Richard},
  year = {2012},
  month = apr,
  publisher = {Courier Corporation},
  abstract = {Numerical analysis is a subject of extreme interest to mathematicians and computer scientists, who will welcome this first inexpensive paperback edition of a groundbreaking classic text on the subject. In an introductory chapter on numerical methods and their relevance to computing, well-known mathematician Richard Hamming (\&quot;the Hamming code,\&quot; \&quot;the Hamming distance,\&quot; and \&quot;Hamming window,\&quot; etc.), suggests that the purpose of computing is insight, not merely numbers. In that connection he outlines five main ideas that aim at producing meaningful numbers that will be read and used, but will also lead to greater understanding of how the choice of a particular formula or algorithm influences not only the computing but our understanding of the results obtained.The five main ideas involve (1) insuring that in computing there is an intimate connection between the source of the problem and the usability of the answers (2) avoiding isolated formulas and algorithms in favor of a systematic study of alternate ways of doing the problem (3) avoidance of roundoff (4) overcoming the problem of truncation error (5) insuring the stability of a feedback system.In this second edition, Professor Hamming (Naval Postgraduate School, Monterey, California) extensively rearranged, rewrote and enlarged the material. Moreover, this book is unique in its emphasis on the frequency approach and its use in the solution of problems. Contents include:I. Fundamentals and AlgorithmsII. Polynomial Approximation- Classical TheoryIll. Fourier Approximation- Modern TheoryIV. Exponential Approximation ... and moreHighly regarded by experts in the field, this is a book with unlimited applications for undergraduate and graduate students of mathematics, science and engineering. Professionals and researchers will find it a valuable reference they will turn to again and again.},
  googlebooks = {Z2owE\_0LQukC},
  isbn = {978-0-486-13482-6},
  langid = {english},
  keywords = {Mathematics / Mathematical Analysis}
}

@incollection{hansenCMAEvolutionStrategy2006,
  title = {The CMA Evolution Strategy: A Comparing Review},
  shorttitle = {The CMA Evolution Strategy},
  booktitle = {Towards a New Evolutionary Computation: Advances in the Estimation of Distribution Algorithms},
  author = {Hansen, Nikolaus},
  editor = {Lozano, Jose A. and Larra{\~n}aga, Pedro and Inza, I{\~n}aki and Bengoetxea, Endika},
  year = {2006},
  series = {Studies in Fuzziness and Soft Computing},
  pages = {75--102},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-32494-1_4},
  abstract = {SummaryDerived from the concept of self-adaptation in evolution strategies, the CMA (Covariance Matrix Adaptation) adapts the covariance matrix of a multi-variate normal search distribution. The CMA was originally designed to perform well with small populations. In this review, the argument starts out with large population sizes, reflecting recent extensions of the CMA algorithm. Commonalities and differences to continuous Estimation of Distribution Algorithms are analyzed. The aspects of reliability of the estimation, overall step size control, and independence from the coordinate system (invariance) become particularly important in small populations sizes. Consequently, performing the adaptation task with small populations is more intricate.},
  isbn = {978-3-540-32494-2},
  langid = {english},
  keywords = {Covariance Matrix,Distribution Algorithm,Evolution Path,Search Point,Step Length}
}

@book{hansenTheorySimpleLiquids2013,
  title = {Theory of Simple Liquids: With Applications to Soft Matter},
  shorttitle = {Theory of Simple Liquids},
  author = {Hansen, Jean-Pierre and McDonald, I. R.},
  year = {2013},
  month = aug,
  publisher = {Academic Press},
  abstract = {Comprehensive coverage of topics in the theory of classical liquids Widely regarded as the standard text in its field, Theory of Simple Liquids gives an advanced but self-contained account of liquid state theory within the unifying framework provided by classical statistical mechanics. The structure of this revised and updated Fourth Edition is similar to that of the previous one but there are significant shifts in emphasis and much new material has been added. Major changes and Key Features in content include: Expansion of existing sections on simulation methods, liquid-vapour coexistence, the hierarchical reference theory of criticality, and the dynamics of super-cooled liquids.New sections on binary fluid mixtures, surface tension, wetting, the asymptotic decay of pair correlations, fluids in porous media, the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely new chapter on applications to 'soft matter' of a combination of liquid state theory and coarse graining strategies, with sections on polymer solutions and polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic liquid crystals, colloidal dynamics, and on clustering and gelation. Expansion of existing sections on simulation methods, liquid-vapour coexistence, the hierarchian reference of criticality, and the dynamics of super-cooled liquids.New sections on binary fluid mixtures, surface tension, wetting, the asymptotic decay of pair correlations, fluids in porous media, the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely new chapter on applications to 'soft matter' of a combination of liquid state theory and coarse graining strategies, with sections on polymer solutions and polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic liquid crystals, colloidal dynamics, and on clustering and gelation.},
  googlebooks = {pbJfOUqZVSgC},
  isbn = {978-0-12-387033-9},
  langid = {english},
  keywords = {Science / Physics / Condensed Matter}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  month = aug,
  publisher = {Springer Science \& Business Media},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  googlebooks = {tVIjmNS3Ob8C},
  isbn = {978-0-387-84858-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Data Analytics,Computers / Information Technology,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology,Science / Life Sciences / General}
}

@article{hooverMeltingTransitionCommunal1968a,
  title = {Melting Transition and Communal Entropy for Hard Spheres},
  author = {Hoover, William G. and Ree, Francis H.},
  year = {1968},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {49},
  number = {8},
  pages = {3609--3617},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1670641}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  year = {1991},
  month = jan,
  journal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(91)90009-T},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp({$\mu$}) performance criteria, for arbitrary finite input environment measures {$\mu$}, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  langid = {english},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation}
}

@book{hornMatrixAnalysis2012,
  title = {Matrix Analysis},
  author = {Horn, Roger A. and Johnson, Charles R.},
  year = {2012},
  month = oct,
  publisher = {Cambridge University Press},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  googlebooks = {O7sgAwAAQBAJ},
  isbn = {978-1-139-78888-5},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Economics / General,Mathematics / Algebra / Abstract,Mathematics / Algebra / General,Mathematics / Geometry / Algebraic}
}

@inproceedings{houghZerosGaussianAnalytic2009,
  title = {Zeros of Gaussian Analytic Functions and Determinantal Point Processes},
  booktitle = {University Lecture Series},
  author = {Hough, J. and Krishnapur, Manjunath and Peres, Y. and Vir{\'a}g, B.},
  year = {2009},
  doi = {10.1090/ULECT/051},
  abstract = {The book examines in some depth two important classes of point processes, determinantal processes and 'Gaussian zeros', i.e., zeros of random analytic functions with Gaussian coefficients. These processes share a property of 'point-repulsion', where distinct points are less likely to fall close to each other than in processes, such as the Poisson process, that arise from independent sampling. Nevertheless, the treatment in the book emphasizes the use of independence: for random power series, the independence of coefficients is key; for determinantal processes, the number of points in a domain is a sum of independent indicators, and this yields a satisfying explanation of the central limit theorem (CLT) for this point count. Another unifying theme of the book is invariance of considered point processes under natural transformation groups. The book strives for balance between general theory and concrete examples. On the one hand, it presents a primer on modern techniques on the interface of probability and analysis. On the other hand, a wealth of determinantal processes of intrinsic interest are analyzed; these arise from random spanning trees and eigenvalues of random matrices, as well as from special power series with determinantal zeros. The material in the book formed the basis of a graduate course given at the IAS-Park City Summer School in 2007; the only background knowledge assumed can be acquired in first-year graduate courses in analysis and probability.}
}

@book{huangStatisticalMechanics1987,
  title = {Statistical Mechanics},
  author = {Huang, Kerson},
  year = {1987},
  month = may,
  publisher = {Wiley},
  abstract = {Unlike most other texts on the subject, this clear, concise introduction to the theory of microscopic bodies treats the modern theory of critical phenomena. Provides up-to-date coverage of recent major advances, including a self-contained description of thermodynamics and the classical kinetic theory of gases, interesting applications such as superfluids and the quantum Hall effect, several current research applications, The last three chapters are devoted to the Landau-Wilson approach to critical phenomena. Many new problems and illustrations have been added to this edition.},
  googlebooks = {M8PvAAAAMAAJ},
  isbn = {978-0-471-81518-1},
  langid = {english},
  keywords = {Science / Mechanics / General,Science / Mechanics / Thermodynamics}
}

@article{ibrahimOverviewSoftComputing2016,
  title = {An Overview of Soft Computing},
  author = {Ibrahim, Dogan},
  year = {2016},
  month = jan,
  journal = {Procedia Computer Science},
  series = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
  volume = {102},
  pages = {34--38},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2016.09.366},
  abstract = {Soft computing, as opposed to traditional computing, deals with approximate models and gives solutions to complex real-life problems. Unlike hard computing, soft computing is tolerant of imprecision, uncertainty, partial truth, and approximations. In effect, the role model for soft computing is the human mind. Soft computing is based on techniques such as fuzzy logic, genetic algorithms, artificial neural networks, machine learning, and expert systems. Although soft computing theory and techniques were first introduced in 1980s, it has now become a major research and study area in automatic control engineering. The techniques of soft computing are nowadays being used successfully in many domestic, commercial, and industrial applications. With the advent of the low-cost and very high performance digital processors and the reduction of the cost of memory chips it is clear that the techniques and application areas of soft computing will continue to expand. This paper gives an overview of the current state of soft computing techniques and describes the advantages and disadvantages of soft computing compared to traditional hard computing techniques.},
  langid = {english},
  keywords = {expert system,fuzzy logic,genetic algorithms,neural networks,Soft computing}
}

@article{jadrichProbabilisticInverseDesign2017,
  title = {Probabilistic Inverse Design for Self-Assembling Materials},
  author = {Jadrich, R. B. and Lindquist, B. A. and Truskett, T. M.},
  year = {2017},
  month = may,
  journal = {The Journal of Chemical Physics},
  volume = {146},
  number = {18},
  pages = {184103},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4981796}
}

@article{jadrichUnsupervisedMachineLearning2018,
  title = {Unsupervised Machine Learning for Detection of Phase Transitions in Off-Lattice Systems. I. Foundations},
  author = {Jadrich, R. B. and Lindquist, B. A. and Truskett, T. M.},
  year = {2018},
  month = nov,
  journal = {The Journal of Chemical Physics},
  volume = {149},
  number = {19},
  pages = {194109},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.5049849}
}

@book{kacprzykSpringerHandbookComputational2015,
  title = {Springer Handbook of Computational Intelligence},
  author = {Kacprzyk, Janusz and Pedrycz, Witold},
  year = {2015},
  month = may,
  publisher = {Springer},
  abstract = {The Springer Handbook for Computational Intelligence is the first book covering the basics, the state-of-the-art and important applications of the dynamic and rapidly expanding discipline of computational intelligence. This comprehensive handbook makes readers familiar with a broad spectrum of approaches to solve various problems in science and technology. Possible approaches include, for example, those being inspired by biology, living organisms and animate systems. Content is organized in seven parts: foundations; fuzzy logic; rough sets; evolutionary computation; neural networks; swarm intelligence and hybrid computational intelligence systems. Each Part is supervised by its own Part Editor(s) so that high-quality content as well as completeness are assured.},
  googlebooks = {gLS4CQAAQBAJ},
  isbn = {978-3-662-43505-2},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Technology \& Engineering / Engineering (General),Technology \& Engineering / General}
}

@article{kaelblingReinforcementLearningSurvey1996,
  title = {Reinforcement Learning: A Survey},
  shorttitle = {Reinforcement Learning},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  year = {1996},
  month = may,
  journal = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  issn = {1076-9757},
  doi = {10.1613/jair.301},
  abstract = {This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.},
  copyright = {Copyright (c)},
  langid = {english}
}

@article{karniadakisPhysicsinformedMachineLearning2021a,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Applied mathematics;Computational science Subject\_term\_id: applied-mathematics;computational-science}
}

@article{kellerIntegralEquationsMachine2019,
  title = {Integral Equations and Machine Learning},
  author = {Keller, Alexander and Dahm, Ken},
  year = {2019},
  month = jul,
  journal = {Mathematics and Computers in Simulation},
  series = {Special Issue on the Eleventh International Conference on Monte Carlo Methods and Applications (MCM 2017), Held in Montreal, Canada, July 03-07, 2017},
  volume = {161},
  pages = {2--12},
  issn = {0378-4754},
  doi = {10.1016/j.matcom.2019.01.010},
  abstract = {As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Efficiency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artificial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.},
  langid = {english},
  keywords = {Artificial neural networks,Integral equations,Light transport simulation,Monte Carlo and quasi-Monte Carlo methods,Reinforcement learning}
}

@article{kelleyFastSolverOrnstein2004,
  title = {A Fast Solver for the Ornstein\textendash Zernike Equations},
  author = {Kelley, C. T. and Pettitt, B. Montgomery},
  year = {2004},
  month = jul,
  journal = {Journal of Computational Physics},
  volume = {197},
  number = {2},
  pages = {491--501},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2003.12.006},
  abstract = {In this paper, we report on the design and analysis of a multilevel method for the solution of the Ornstein\textendash Zernike Equations and related systems of integro-algebraic equations. Our approach is based on an extension of the Atkinson\textendash Brakhage method, with Newton-GMRES used as the coarse mesh solver. We report on several numerical experiments to illustrate the effectiveness of the method. The problems chosen are related to simple short ranged fluids with continuous potentials. Speedups over traditional methods for a given accuracy are reported. The new multilevel method is roughly six times faster than Newton-GMRES and 40 times faster than Picard.},
  langid = {english}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: A Method for Stochastic Optimization},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{kinoshitaInteractionSurfacesSolvophobicity2003,
  title = {Interaction between Surfaces with Solvophobicity or Solvophilicity Immersed in Solvent:\hspace{0.6em}Effects Due to Addition of Solvophobic or Solvophilic Solute},
  shorttitle = {Interaction between Surfaces with Solvophobicity or Solvophilicity Immersed in Solvent},
  author = {Kinoshita, Masahiro},
  year = {2003},
  month = may,
  journal = {The Journal of Chemical Physics},
  volume = {118},
  number = {19},
  pages = {8969--8981},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1566935}
}

@book{kittelElementaryStatisticalPhysics2004,
  title = {Elementary Statistical Physics},
  author = {Kittel, Charles},
  year = {2004},
  month = mar,
  publisher = {Courier Corporation},
  abstract = {Noteworthy for the philosophical subtlety of its foundations and the elegance of its problem-solving methods, statistical mechanics can be employed in a broad range of applications \textemdash{} among them, astrophysics, biology, chemistry, nuclear and solid state physics, communications engineering, metallurgy, and mathematics. Geared toward graduate students in physics, this text covers such important topics as stochastic processes and transport theory in order to provide students with a working knowledge of statistical mechanics. To explain the fundamentals of his subject, the author uses the method of ensembles developed by J. Willard Gibbs. Topics include the properties of the Fermi-Dirac and Bose-Einstein distributions; the interrelated subjects of fluctuations, thermal noise, and Brownian movement; and the thermodynamics of irreversible processes. Negative temperature, magnetic energy, density matrix methods, and the Kramers-Kronig causality relations are treated briefly. Most sections include illustrative problems. Appendix. 28 figures. 1 table.},
  googlebooks = {5sd9SAoRjgQC},
  isbn = {978-0-486-43514-5},
  langid = {english},
  keywords = {Science / Physics / General}
}

@article{kolafaAccurateEquationState2004,
  title = {Accurate Equation of State of the Hard Sphere Fluid in Stable and Metastable Regions},
  author = {Kolafa, Ji{\v r}{\'i} and Lab{\'i}k, Stanislav and Malijevsk{\'y}, Anatol},
  year = {2004},
  month = may,
  journal = {Physical Chemistry Chemical Physics},
  volume = {6},
  number = {9},
  pages = {2335--2340},
  publisher = {The Royal Society of Chemistry},
  issn = {1463-9084},
  doi = {10.1039/B402792B},
  abstract = {New accurate data on the compressibility factor of the hard sphere fluid are obtained by highly optimized molecular dynamics calculations in the range of reduced densities 0.20\textendash 1.03. The relative inaccuracy at the 95\% confidence level is better than 0.00004 for all densities but the last deeply metastable point. This accuracy requires careful examination of finite size effects and other possible sources of errors and applying corrections. The data are fitted to a power series in y/(1 - y), where y is the packing fraction; the coefficients are determined so that virial coefficients B2 to B6 are reproduced. To do this, values of B5 and B6 are accurately recalculated. Virial coefficients up to B11 are then estimated from the equation of state.},
  langid = {english}
}

@article{kolafaBridgeFunctionHard2002,
  title = {The Bridge Function of Hard Spheres by Direct Inversion of Computer Simulation Data},
  author = {KOLAFA, JI{\v R}{\'I} and LAB{\'I}K, STANISLAV and MALIJEVSK{\'Y}, ANATOL},
  year = {2002},
  month = aug,
  journal = {Molecular Physics},
  volume = {100},
  number = {16},
  pages = {2629--2640},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268970210136357},
  abstract = {The bridge function of the hard sphere fluid has been calculated from our new highly accurate Monte Carlo and molecular dynamics simulation data on the radial distribution function using the (inverted) Ornstein-Zernike equation. Both the systematic errors (finite size, grid size, tail) and statistical errors are analysed in detail and ways to suppress them are proposed. Uncertainties in the resulting values of B(r) are about 0.001. In contrast with many previous findings the bridge function is both positive and negative.},
  annotation = {\_eprint: https://doi.org/10.1080/00268970210136357}
}

@book{kornerFourierAnalysis1989,
  title = {Fourier Analysis},
  author = {K{\"o}rner, T. W.},
  year = {1989},
  month = nov,
  publisher = {Cambridge University Press},
  abstract = {Fourier analysis is a subject that was born in physics but grew up in mathematics. Now it is part of the standard repertoire for mathematicians, physicists and engineers. In most books, this diversity of interest is often ignored, but here Dr K\"orner has provided a shop-window for some of the ideas, techniques and elegant results of Fourier analysis, and for their applications. These range from number theory, numerical analysis, control theory and statistics, to earth science, astronomy, and electrical engineering. Each application is placed in perspective by a short essay. The prerequisites are few (the reader with knowledge of second or third year undergraduate mathematics should have no difficulty following the text), and the style is lively and entertaining. In short, this stimulating account will be welcomed by all who like to read about more than the bare bones of a subject. For them this will be a meaty guide to Fourier analysis.},
  googlebooks = {OcZ5iKsGrmoC},
  isbn = {978-0-521-38991-4},
  langid = {english},
  keywords = {Mathematics / Algebra / Abstract,Mathematics / Functional Analysis,Mathematics / Probability \& Statistics / General}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  month = dec,
  series = {NIPS'12},
  pages = {1097--1105},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.}
}

@article{kroeseWhyMonteCarlo2014,
  title = {Why the Monte Carlo Method Is so Important Today},
  author = {Kroese, Dirk P. and Brereton, Tim and Taimre, Thomas and Botev, Zdravko I.},
  year = {2014},
  journal = {WIREs Computational Statistics},
  volume = {6},
  number = {6},
  pages = {386--392},
  issn = {1939-0068},
  doi = {10.1002/wics.1314},
  abstract = {Since the beginning of electronic computing, people have been interested in carrying out random experiments on a computer. Such Monte Carlo techniques are now an essential ingredient in many quantitative investigations. Why is the Monte Carlo method (MCM) so important today? This article explores the reasons why the MCM has evolved from a `last resort' solution to a leading methodology that permeates much of contemporary science, finance, and engineering. WIREs Comput Stat 2014, 6:386\textendash 392. doi: 10.1002/wics.1314 This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Markov Chain Monte Carlo (MCMC) Statistical Models {$>$} Simulation Models},
  langid = {english},
  keywords = {estimation,MCMC,Monte Carlo method,randomized optimization,simulation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1314}
}

@book{kruseComputationalIntelligenceMethodological2016,
  title = {Computational Intelligence: A Methodological Introduction},
  shorttitle = {Computational Intelligence},
  author = {Kruse, Rudolf and Borgelt, Christian and Braune, Christian and Mostaghim, Sanaz and Steinbrecher, Matthias},
  year = {2016},
  month = sep,
  publisher = {Springer},
  abstract = {This textbook provides a clear and logical introduction to the field, covering the fundamental concepts, algorithms and practical implementations behind efforts to develop systems that exhibit intelligent behavior in complex environments. This enhanced second edition has been fully revised and expanded with new content on swarm intelligence, deep learning, fuzzy data analysis, and discrete decision graphs. Features: provides supplementary material at an associated website; contains numerous classroom-tested examples and definitions throughout the text; presents useful insights into all that is necessary for the successful application of computational intelligence methods; explains the theoretical background underpinning proposed solutions to common problems; discusses in great detail the classical areas of artificial neural networks, fuzzy systems and evolutionary algorithms; reviews the latest developments in the field, covering such topics as ant colony optimization and probabilistic graphical models.},
  googlebooks = {An0XDQAAQBAJ},
  isbn = {978-1-4471-7296-3},
  langid = {english},
  keywords = {Computers / Intelligence (AI) \& Semantics,Computers / Software Development \& Engineering / General,Mathematics / Applied}
}

@article{kwakEvaluationBridgefunctionDiagrams2005,
  title = {Evaluation of Bridge-Function Diagrams via Mayer-Sampling Monte Carlo Simulation},
  author = {Kwak, Sang Kyu and Kofke, David A.},
  year = {2005},
  month = mar,
  journal = {The Journal of Chemical Physics},
  volume = {122},
  number = {10},
  pages = {104508},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1860559}
}

@article{labikEfficientGaussNewtonlikeMethod1994,
  title = {An Efficient Gauss-Newton-like Method for the Numerical Solution of the Ornstein-Zernike Integral Equation for a Class of Fluid Models},
  author = {Lab{\'{\i}}k, Stanislav and Posp{\'{\i}}{\v s}il, Roman and Malijevsk{\'y}, Anatol and Smith, William Robert},
  year = {1994},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {115},
  number = {1},
  pages = {12--21},
  issn = {0021-9991},
  doi = {10.1006/jcph.1994.1174},
  abstract = {A numerical algorithm for solving the Ornstein-Zernike (OZ) integral equation of statistical mechanics is described for the class of fluids composed of molecules with axially symmetric interactions. Since the OZ equation is a nonlinear second-kind Fredholm equation whose key feature for the class of problems of interest is the highly computationally intensive nature of the kernel, the general approach employed in this paper is thus potentially useful for similar problems with this characteristic. The algorithm achieves a high degree of computational efficiency by combining iterative linearization of the most complex portion of the kernel with a combination of Newton-Raphson and Picard iteration methods for the resulting approximate equation. This approach makes the algorithm analogous to the approach of the classical Gauss-Newton method for nonlinear regression, and we call our method the GN algorithm. An example calculation is given illustrating the use of the algorithm for the hard prolate ellipsoid fluid and its results are compared directly with those of the Picard iteration method. The GN algorithm is four to ten times as fast as the Picard method, and we present evidence that it is the most efficient general method currently available.},
  langid = {english}
}

@article{labikRapidlyConvergentMethod1985,
  title = {A Rapidly Convergent Method of Solving the OZ Equation},
  author = {Lab{\'i}k, Stanislav and Malijevsk{\'y}, Anatol and Vo{\v n}ka, Petr},
  year = {1985},
  month = oct,
  journal = {Molecular Physics},
  volume = {56},
  number = {3},
  pages = {709--715},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268978500102651},
  abstract = {A new method is proposed for solving numerically the Ornstein-Zernike equation for systems with a spherically symmetrical pair-potential. The method is based on expansion of the function {$\Gamma$}(r)=r[h(r) - c(r)] in suitable basis functions and on a combination of Newton-Raphson and direct iterations. Tests on the PY and HNC approximations for hard spheres and Lennard-Jones fluid have shown that the proposed method is three to nine times as rapid as the related and so far the most efficient method of Gillan. Other advantages besides the speed are low sensitivity to the choice of initial estimate and a relatively simple computational scheme.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978500102651}
}

@article{ladoSolutionsReferencehypernettedchainEquation1983,
  title = {Solutions of the Reference-Hypernetted-Chain Equation with Minimized Free Energy},
  author = {Lado, F. and Foiles, S. M. and Ashcroft, N. W.},
  year = {1983},
  month = oct,
  journal = {Physical Review A},
  volume = {28},
  number = {4},
  pages = {2374--2379},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.28.2374},
  abstract = {We use the Rosenfeld-Ashcroft procedure of modeling the bridge function in the reference\textemdash hypernetted-chain integral equation with its hard-sphere values, and choose the sphere diameter so that the free energy of the system is minimized. The resulting integral equation is solved for both the long-range Coulomb potential and the short-range Lennard-Jones potential. The results are in excellent agreement with Monte Carlo data for the thermodynamics and structure of both systems. The method provides an entirely first-principles approach to the theory of the structure and thermodynamics of simple classical liquids.}
}

@book{landauGuideMonteCarlo2021,
  title = {A Guide to Monte Carlo Simulations in Statistical Physics},
  author = {Landau, David and Binder, Kurt},
  year = {2021},
  month = jul,
  publisher = {Cambridge University Press},
  abstract = {"Dealing with all aspects of Monte Carlo simulation of complex physical systems encountered in condensed-matter physics and statistical mechanics, this book provides an introduction to computer simulations in physics. This fourth edition contains extensive new material describing numerous powerful algorithms not covered in previous editions, in some cases representing new developments that have only recently appeared. Older methodologies whose impact was previously unclear or unappreciated are also introduced, in addition to many small revisions that bring the text and cited literature up to date. This edition also introduces the use of petascale computing facilities in the Monte Carlo arena"--},
  googlebooks = {WW8yEAAAQBAJ},
  isbn = {978-1-108-49014-6},
  langid = {english},
  keywords = {Mathematics / Applied,Science / Physics / Condensed Matter,Science / Physics / General,Science / Physics / Mathematical \& Computational}
}

@article{lechnerAccurateDeterminationCrystal2008,
  title = {Accurate Determination of Crystal Structures Based on Averaged Local Bond Order Parameters},
  author = {Lechner, Wolfgang and Dellago, Christoph},
  year = {2008},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {129},
  number = {11},
  pages = {114707},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.2977970}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computer science;Mathematics and computing Subject\_term\_id: computer-science;mathematics-and-computing}
}

@article{lecunGradientbasedLearningApplied1998a,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis}
}

@article{leunissenIonicColloidalCrystals2005,
  title = {Ionic Colloidal Crystals of Oppositely Charged Particles},
  author = {Leunissen, Mirjam E. and Christova, Christina G. and Hynninen, Antti-Pekka and Royall, C. Patrick and Campbell, Andrew I. and Imhof, Arnout and Dijkstra, Marjolein and {van Roij}, Ren{\'e} and {van Blaaderen}, Alfons},
  year = {2005},
  month = sep,
  journal = {Nature},
  volume = {437},
  number = {7056},
  pages = {235--240},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature03946},
  abstract = {Colloidal suspensions are widely used to study processes such as melting, freezing1,2,3 and glass transitions4,5. This is because they display the same phase behaviour as atoms or molecules, with the nano- to micrometre size of the colloidal particles making it possible to observe them directly in real space3,4. Another attractive feature is that different types of colloidal interactions, such as long-range repulsive1,3, short-range attractive5, hard-sphere-like2,3,4 and dipolar3, can be realized and give rise to equilibrium phases. However, spherically symmetric, long-range attractions (that is, ionic interactions) have so far always resulted in irreversible colloidal aggregation6. Here we show that the electrostatic interaction between oppositely charged particles can be tuned such that large ionic colloidal crystals form readily, with our theory and simulations confirming the stability of these structures. We find that in contrast to atomic systems, the stoichiometry of our colloidal crystals is not dictated by charge neutrality; this allows us to obtain a remarkable diversity of new binary structures. An external electric field melts the crystals, confirming that the constituent particles are indeed oppositely charged. Colloidal model systems can thus be used to study the phase behaviour of ionic species. We also expect that our approach to controlling opposite-charge interactions will facilitate the production of binary crystals of micrometre-sized particles, which could find use as advanced materials for photonic applications7.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research}
}

@article{libbrechtMachineLearningApplications2015,
  title = {Machine Learning Applications in Genetics and Genomics},
  author = {Libbrecht, Maxwell W. and Noble, William Stafford},
  year = {2015},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {16},
  number = {6},
  pages = {321--332},
  publisher = {Nature Publishing Group},
  issn = {1471-0064},
  doi = {10.1038/nrg3920},
  abstract = {The field of machine learning includes the development and application of computer algorithms that improve with experience.Machine learning methods can be divided into supervised, semi-supervised and unsupervised methods. Supervised methods are trained on examples with labels (for example, 'gene' or 'not gene') and are then used to predict these labels on other examples, whereas unsupervised methods find patterns in data sets without the use of labels. Semi-supervised methods combine these two approaches, leveraging patterns in unlabelled data to improve power in the prediction of labels.Different machine learning methods may be required for an application, depending on whether one is interested in interpreting the output model or is simply concerned with predictive power. Generative models, which posit a probabilistic distribution over input data, are generally best for interpretability, whereas discriminative models, which seek only to model labels, are generally best for predictive power.Prior information can be added to a model in order to train the model more effectively when it is provided with limited data, to limit the complexity of the model or to incorporate data that are not used by the model directly. Prior information can be incorporated explicitly in a probabilistic model or implicitly through the choice of features or similarity measures.The choice of an appropriate performance measure depends strongly on the application task. Machine learning methods are most effective when they optimize an appropriate performance measure.Network estimation methods are appropriate when the data contain complex dependencies among examples. These methods work best when they take into account the confounding effects of indirect relationships.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Genomics;Machine learning;Statistical methods Subject\_term\_id: genomics;machine-learning;statistical-methods}
}

@article{liDiversityPromotingObjectiveFunction2016,
  title = {A Diversity-Promoting Objective Function for Neural Conversation Models},
  author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  year = {2016},
  month = jun,
  journal = {arXiv:1510.03055 [cs]},
  eprint = {1510.03055},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., "I don't know") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{liMolecularDynamicsOntheFly2015,
  title = {Molecular Dynamics with On-the-Fly Machine Learning of Quantum-Mechanical Forces},
  author = {Li, Zhenwei and Kermode, James R. and De Vita, Alessandro},
  year = {2015},
  month = mar,
  journal = {Physical Review Letters},
  volume = {114},
  number = {9},
  pages = {096405},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.114.096405},
  abstract = {We present a molecular dynamics scheme which combines first-principles and machine-learning (ML) techniques in a single information-efficient approach. Forces on atoms are either predicted by Bayesian inference or, if necessary, computed by on-the-fly quantum-mechanical (QM) calculations and added to a growing ML database, whose completeness is, thus, never required. As a result, the scheme is accurate and general, while progressively fewer QM calls are needed when a new chemical process is encountered for the second and subsequent times, as demonstrated by tests on crystalline and molten silicon.}
}

@article{lindquistCommunicationInverseDesign2016,
  title = {Communication: Inverse Design for Self-Assembly via on-the-Fly Optimization},
  shorttitle = {Communication},
  author = {Lindquist, Beth A. and Jadrich, Ryan B. and Truskett, Thomas M.},
  year = {2016},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {145},
  number = {11},
  pages = {111101},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4962754},
  abstract = {Inverse methods of statistical mechanics have facilitated the discovery of pair potentials that stabilize a wide variety of targeted lattices at zero temperature. However, such methods are complicated by the need to compare, within the optimization framework, the energy of the desired lattice to all possibly relevant competing structures, which are not generally known in advance. Furthermore, ground-state stability does not guarantee that the target will readily assemble from the fluid upon cooling from higher temperature. Here, we introduce a molecular dynamics simulation-based, optimization design strategy that iteratively and systematically refines the pair interaction according to the fluid and crystalline structural ensembles encountered during the assembly process. We successfully apply this probabilistic, machine-learning approach to the design of repulsive, isotropic pair potentials that assemble into honeycomb, kagome, square, rectangular, truncated square, and truncated hexagonal lattices.}
}

@article{liuCarnahanStarlingTypeEquations2021,
  title = {Carnahan-Starling Type Equations of State for Stable Hard Disk and Hard Sphere Fluids},
  author = {Liu, Hongqin},
  year = {2021},
  month = may,
  journal = {Molecular Physics},
  volume = {119},
  number = {9},
  pages = {e1886364},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268976.2021.1886364},
  abstract = {The well-known Carnahan-Starling (CS) equation of state (EoS) [N.F. Carnahan and K.E. Starling. J. Chem. Phys. 51 (2), 635\textendash 636 (1969). doi:10.1063/1.1672048] for hard sphere (HS) fluid was derived from a quadratic relationship between the integer portions of the virial coefficients, Bn,integer, and their orders, n. In this paper, the method is extended to cover the full virial coefficients, Bn, for the general D-dimensional case. We propose a (D-1)th order polynomial for the virial coefficients starting from the 4th order and EoS's are derived from it. For the hard rod (D = 1) case, the exact solution is obtained. For the stable hard disk fluid (D = 2), the most recent virial coefficients, up to the 10th order, [N. Clisby and B.M. McCoy. J. Stat. Phys. 122 (1), 15\textendash 57 (2006). doi:10.1007/s10955-005-8080-0] and accurate compressibility data [J. Kolafa and M. Rottner. Mol. Phys. 104 (22\textendash 24), 3435\textendash 3441 (2006). doi:10.1080/00268970600967963; J.J. Erpenbeck and M. Luban. Phys. Rev. A. 32 (5), 2920\textendash 2922 (1985). doi:10.1103/PhysRevA.32.2920] are employed to construct and to test the EoS. For the stable hard sphere (D = 3) fluid, a new CS-type EoS is obtained by using the most recent virial coefficients [N. Clisby and B.M. McCoy. J. Stat. Phys. 122 (1), 15\textendash 57 (2006). doi:10.1007/s10955-005-8080-0; A.J. Schultz and D.A. Kofke. Physical Review E. 90 (2) (2014). doi:10.1103/PhysRevE.90.023301], up to the 11th order, along with highly-accurate compressibility data [S. Pieprzyk et al. Phys. Chem. Chem. Phys. 21 (13), 6886\textendash 6899 (2019). doi:10.1039/C9CP00903E; M.N. Bannerman et al. J. Chem. Phys. 132 (8), 084507 (2010). doi:10.1063/1.3328823; J. Kolafa, et al. Phys. Chem. Chem. Phys. 6 (9), 2335\textendash 2340 (2004). doi:10.1039/B402792B]. The simple new EoS's prove to be as accurate as the Pad\'e approximations based on all available virial coefficients, which significantly improve the accuracy of the CS-type EoS in the hard sphere case. This research also reveals that as long as the virial coefficients obey a polynomial function, any EoS derived from it will diverge at the non-physical packing fraction, {$\eta$}=1.},
  keywords = {Equation of state,hard disc,hard sphere,thermodynamics,virial coefficients},
  annotation = {\_eprint: https://doi.org/10.1080/00268976.2021.1886364}
}

@article{llano-restrepoBridgeFunctionCavity1992,
  title = {Bridge Function and Cavity Correlation Function for the Lennard-Jones Fluid from Simulation},
  author = {Llano-Restrepo, Mario and Chapman, Walter G.},
  year = {1992},
  month = aug,
  journal = {The Journal of Chemical Physics},
  volume = {97},
  number = {3},
  pages = {2046--2054},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.463142}
}

@article{lombaOrnsteinZernikeEquationsSimulation1993,
  title = {Ornstein-Zernike Equations and Simulation Results for Hard-Sphere Fluids Adsorbed in Porous Media},
  author = {Lomba, Enrique and Given, James A. and Stell, George and Weis, Jean Jacques and Levesque, Dominique},
  year = {1993},
  month = jul,
  journal = {Physical Review E},
  volume = {48},
  number = {1},
  pages = {233--244},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.48.233},
  abstract = {In this paper we solve the replica Ornstein-Zernike (ROZ) equations in the hypernetted-chain (HNC), Percus-Yevick (PY), and reference Percus-Yevick (RPY) approximations for partly quenched systems. The ROZ equations, which apply to the general class of partly quenched systems, are here applied to a class of models for porous media. These models involve two species of particles: an annealed or equilibrated species, which is used to model the fluid phase, and a quenched or frozen species, whose excluded-volume interactions constitute the matrix in which the fluid is adsorbed. We study two models for the quenched species of particles: a hard-sphere matrix, for which the fluid-fluid, matrix-matrix, and matrix-fluid sphere diameters {$\sigma$}11, {$\sigma$}00, and {$\sigma$}01 are additive, and a matrix of randomly overlapping particles (which still interact with the fluid particle as hard spheres) that gives a ``random'' matrix with interconnected pore structure. For the random-matrix case we study a ratio {$\sigma$}01/{$\sigma$}11 of 2.5, which is a demanding one for the theories. The HNC and RPY results represent significant improvements over the PY result when compared with the Monte Carlo simulations we have generated for this study, with the HNC result yielding the best results overall among those studied. A phenomenological percolating-fluid approximation is also found to be of comparable accuracy to the HNC results over a significant range of matrix and fluid densities. In the hard-sphere matrix case, the RPY is the best of the theories that we have considered.}
}

@article{lopez-sanchezDemixingTransitionStructure2013a,
  title = {Demixing Transition, Structure, and Depletion Forces in Binary Mixtures of Hard-Spheres: The Role of Bridge Functions},
  shorttitle = {Demixing Transition, Structure, and Depletion Forces in Binary Mixtures of Hard-Spheres},
  author = {{L{\'o}pez-S{\'a}nchez}, Erik and {Estrada-{\'A}lvarez}, C{\'e}sar D. and {P{\'e}rez-{\'A}ngel}, Gabriel and {M{\'e}ndez-Alcaraz}, Jos{\'e} Miguel and {Gonz{\'a}lez-Mozuelos}, Pedro and {Casta{\~n}eda-Priego}, Ram{\'o}n},
  year = {2013},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {139},
  number = {10},
  pages = {104908},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4820559}
}

@misc{MachineLearningApplications,
  title = {Machine Learning Applications in Drug Development - ScienceDirect},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S2001037019303988}
}

@article{malijevskyBridgeFunctionHard1987,
  title = {The Bridge Function for Hard Spheres},
  author = {Malijevsk{\'y}, Anatol and Lab{\'i}k, Stanislav},
  year = {1987},
  month = feb,
  journal = {Molecular Physics},
  volume = {60},
  number = {3},
  pages = {663--669},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268978700100441},
  abstract = {The paper presents an empirical formula for expressing the bridge function (the sum of elementary graphs) in terms of the interparticle separation and the density. The formulae is fully consistent with the best computer-simulation thermodynamic and structural data for hard spheres in the fluid region. It can serve as both a direct and convenient testing ground for the integral-equation theories of hard spheres and an input to the reference-hypernetted chain approximation for simple fluids.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978700100441}
}

@article{mccoyAssessmentTimeSeriesMachine2018,
  title = {Assessment of Time-Series Machine Learning Methods for Forecasting Hospital Discharge Volume},
  author = {McCoy, Jr, Thomas H. and Pellegrini, Amelia M. and Perlis, Roy H.},
  year = {2018},
  month = nov,
  journal = {JAMA Network Open},
  volume = {1},
  number = {7},
  pages = {e184087},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2018.4087},
  abstract = {Forecasting the volume of hospital discharges has important implications for resource allocation and represents an opportunity to improve patient safety at periods of elevated risk.To determine the performance of a new time-series machine learning method for forecasting hospital discharge volume compared with simpler methods.A retrospective cohort study of daily hospital discharge volumes at 2 large, New England academic medical centers between January 1, 2005, and December 31, 2014 (hospital 1), or January 1, 2005, and December 31, 2010 (hospital 2), comparing time-series forecasting methods for prediction was performed. Data analysis was conducted from February 28, 2017, to August 30, 2018. Group-level data for all discharges from inpatient units were included. In addition to conventional methods, a technique originally developed for allocating data center resources, and comparison strategies for incorporating prior data and frequency of model updates, was conducted to identify the model application that optimized forecast accuracy.Model calibration as measured by R2 and, secondarily, number of days with errors greater than 1 SD of daily volume.During the forecasted year, hospital 1 had 54 411 discharges (daily mean, 149) and hospital 2 had 47 456 discharges (daily mean, 130). The machine learning method was well calibrated at both sites (R2, 0.843 and 0.726, respectively) and made errors greater than 1 SD of daily volume on only 13 and 22 days, respectively, of the forecast year at the 2 sites. Last-value-carried-forward models performed somewhat less well (calibration R2, 0.781 and 0.596, respectively) with 13 and 46 errors of 1 SD or greater, respectively. More frequent retraining and training sets of longer than 1 year had minimal effects on the machine learning method's performance.Volume of hospital discharges can perhaps be reliably forecasted using simple carry-forward models as well as methods drawn from machine learning. The benefit of the latter does not appear to be dependent on extensive training data and may enable forecasts up to 1 year in advance with superior absolute accuracy to carry-forward models.}
}

@article{mcdonaldCalculationThermodynamicProperties1967,
  title = {Calculation of Thermodynamic Properties of Liquid Argon from Lennard-Jones Parameters by a Monte Carlo Method},
  author = {McDonald, I. R. and Singer, K.},
  year = {1967},
  month = jan,
  journal = {Discussions of the Faraday Society},
  volume = {43},
  number = {0},
  pages = {40--49},
  publisher = {The Royal Society of Chemistry},
  issn = {0366-9033},
  doi = {10.1039/DF9674300040},
  abstract = {Extensions of the Monte Carlo method of Metropolis et al. are described which permit both isochoric and isothermal extrapolations of Monte Carlo data. Thermodynamic properties are calculated for liquid argon from Lennard-Jones parameters for a wide V, T-range. The agreement between calculated and experimental values is, on the whole, satisfactory.},
  langid = {english}
}

@book{mcquarrieStatisticalMechanics2000,
  title = {Statistical Mechanics},
  author = {McQuarrie, Donald A.},
  year = {2000},
  month = jun,
  publisher = {University Science Books},
  abstract = {The canonical ensemble - Other ensembles and fluctuations - Boltzmann statistics, fermi-dirac statistics, and bose-einstein statistics - Ideal monatomic gas - Ideal diatomic - Classical statistical mechanics - Ideal polyatomic - Chemical equilibrium - Quantum statistics - Crystals - Imperfect gases - Distribution functions in classical monatomic liquids - Perturbation theories of liquids - Solutions of strong electrolytes - Kinetic theory of gases and molecular collisions - Continuum mechanics - Kinetic theory of-gases and the boltzmann equation - Transport processes in dilute gases - Theory of brownian motion - The time-correlation function formalism.},
  googlebooks = {itcpPnDnJM0C},
  isbn = {978-1-891389-15-3},
  langid = {english},
  keywords = {Science / Chemistry / Physical \& Theoretical,Science / Mechanics / General,Science / Physics / General,Science / Physics / Mathematical \& Computational}
}

@article{mehligMachineLearningNeural2021,
  title = {Machine Learning with Neural Networks},
  author = {Mehlig, B.},
  year = {2021},
  month = feb,
  journal = {arXiv:1901.05639 [cond-mat, stat]},
  eprint = {1901.05639},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, stat},
  abstract = {Lecture notes for my course on machine learning with neural networks that I have given at Gothenburg University and Chalmers Technical University in Gothenburg, Sweden.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Statistics - Machine Learning}
}

@book{murphyMachineLearningProbabilistic2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  month = aug,
  publisher = {MIT Press},
  abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package\textemdash PMTK (probabilistic modeling toolkit)\textemdash that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  googlebooks = {NZP6AQAAQBAJ},
  isbn = {978-0-262-01802-9},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Machine Theory}
}

@inproceedings{neyshaburRoleOverparametrizationGeneralization2018,
  title = {The Role of Over-Parametrization in Generalization of Neural Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  year = {2018},
  month = sep,
  abstract = {We suggest a generalization bound that could partly explain the improvement in generalization with over-parametrization.},
  langid = {english}
}

@article{ngHypernettedChainSolutions1974,
  title = {Hypernetted Chain Solutions for the Classical One-component Plasma up to {$\Gamma$}=7000},
  author = {Ng, Kin-Chue},
  year = {1974},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {61},
  number = {7},
  pages = {2680--2689},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1682399}
}

@book{nielsenNeuralNetworksDeep2015,
  title = {Neural Networks and Deep Learning},
  author = {Nielsen, Michael},
  year = {2015}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, S.},
  year = {2006},
  series = {Springer Series in Operations Research and Financial Engineering},
  edition = {Second},
  publisher = {Springer-Verlag},
  address = {New York},
  doi = {10.1007/978-0-387-40065-5},
  abstract = {Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
  isbn = {978-0-387-30303-1},
  langid = {english}
}

@inproceedings{nomuraDistanceweightedExponentialNatural2021,
  title = {Distance-Weighted Exponential Natural Evolution Strategy for Implicitly Constrained Black-Box Function Optimization},
  booktitle = {2021 IEEE Congress on Evolutionary Computation (CEC)},
  author = {Nomura, Masahiro and Sakai, Nobuyuki and Fukushima, Nobusumi and Ono, Isao},
  year = {2021},
  month = jun,
  pages = {1099--1106},
  doi = {10.1109/CEC45853.2021.9504865},
  abstract = {This paper presents a natural evolution strategy for implicitly constrained black-box function optimization. The black-box function optimization is challenging because explicit representations of objective functions are not given, and only evaluation values of solutions can be used. In implicitly constrained black-box function problems, constraints are not explicitly given, and only the feasibility of a solution is obtained when the objective function is evaluated. In other words, the amount of constraint violation cannot be obtained, which makes the optimization difficult. Natural Evolution Strategies (NES) is one of the promising frameworks for black-box function optimization. DX-NES is an improved version of xNES which is a promising NES using a multivariate normal distribution as the probability distribution. DX-NES has been reported to show good performance on unconstrained black-box function optimization problems. However, DX-NES has a serious problem in that its performance degrades when applied to implicitly constrained problems. In order to address the problem, we propose DX-NES taking account of Implicit Constraint (DX-NES-IC). In experiments using benchmark problems and a lens system design problem, DX-NES-IC showed better performance than DX-NES, xNES, CMA-ES, and those with the resampling technique in terms of the number of successful trials and that of evaluations, where the resampling technique is a constraint handling method which can be used for implicitly constrained problems.},
  keywords = {Benchmark testing,Estimation,Evolutionary computation,Gaussian distribution,Implicit Constraint,Linear programming,Natural Evolution Strategy,Optimization,Probability distribution}
}

@article{nwankpaActivationFunctionsComparison2018,
  title = {Activation Functions: Comparison of Trends in Practice and Research for Deep Learning},
  shorttitle = {Activation Functions},
  author = {Nwankpa, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.03378 [cs]},
  eprint = {1811.03378},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{ornsteinAccidentalDeviationsDensity1914,
  title = {Accidental Deviations of Density and Opalescence at the Critical Point of a Single Substance},
  author = {Ornstein, L. S. and Zernike, F.},
  year = {1914},
  journal = {Proc. Akad. Sci.},
  volume = {17},
  pages = {793}
}

@inproceedings{parkMinimumWidthUniversal2020,
  title = {Minimum Width for Universal Approximation},
  booktitle = {International Conference on Learning Representations},
  author = {Park, Sejun and Yun, Chulhee and Lee, Jaeho and Shin, Jinwoo},
  year = {2020},
  month = sep,
  abstract = {The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width...},
  langid = {english}
}

@article{peplowAlgorithmsComputationSolutions2006,
  title = {Algorithms for the Computation of Solutions of the Ornstein-Zernike Equation},
  author = {Peplow, A. T. and Beardmore, R. E. and Bresme, F.},
  year = {2006},
  month = oct,
  journal = {Physical Review E},
  volume = {74},
  number = {4},
  pages = {046705},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.74.046705},
  abstract = {We introduce a robust and efficient methodology to solve the Ornstein-Zernike integral equation using the pseudoarc length (PAL) continuation method that reformulates the integral equation in an equivalent but nonstandard form. This enables the computation of solutions in regions where the compressibility experiences large changes or where the existence of multiple solutions and so-called branch points prevents Newton's method from converging. We illustrate the use of the algorithm with a difficult problem that arises in the numerical solution of integral equations, namely the evaluation of the so-called no-solution line of the Ornstein-Zernike hypernetted chain (HNC) integral equation for the Lennard-Jones potential. We are able to use the PAL algorithm to solve the integral equation along this line and to connect physical and nonphysical solution branches (both isotherms and isochores) where appropriate. We also show that PAL continuation can compute solutions within the no-solution region that cannot be computed when Newton and Picard methods are applied directly to the integral equation. While many solutions that we find are new, some correspond to states with negative compressibility and consequently are not physical.}
}

@article{percusAnalysisClassicalStatistical1958,
  title = {Analysis of Classical Statistical Mechanics by Means of Collective Coordinates},
  author = {Percus, Jerome K. and Yevick, George J.},
  year = {1958},
  month = apr,
  journal = {Physical Review},
  volume = {110},
  number = {1},
  pages = {1--13},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.110.1},
  abstract = {The three-dimensional classical many-body system is approximated by the use of collective coordinates, through the assumed knowledge of two-body correlation functions. The resulting approximate statistical state is used to obtain the two-body correlation function. Thus, a self-consistent formulation is available for determining the correlation function. Then, the self-consistent integral equation is solved in virial expansion, and the thermodynamic quantities of the system thereby ascertained. The first three virial coefficients are exactly reproduced, while the fourth is nearly correct, as evidenced by numerical results for the case of hard spheres.}
}

@article{pinkusApproximationTheoryMLP1999,
  title = {Approximation Theory of the MLP Model in Neural Networks},
  author = {Pinkus, Allan},
  year = {1999},
  month = jan,
  journal = {Acta Numerica},
  volume = {8},
  pages = {143--195},
  publisher = {Cambridge University Press},
  issn = {1474-0508, 0962-4929},
  doi = {10.1017/S0962492900002919},
  abstract = {In this survey we discuss various approximation-theoretic problems that arise in the multilayer feedforward perceptron (MLP) model in neural networks. The MLP model is one of the more popular and practical of the many neural network models. Mathematically it is also one of the simpler models. Nonetheless the mathematics of this model is not well understood, and many of these problems are approximation-theoretic in character. Most of the research we will discuss is of very recent vintage. We will report on what has been done and on various unanswered questions. We will not be presenting practical (algorithmic) methods. We will, however, be exploring the capabilities and limitations of this model.},
  langid = {english}
}

@article{pittsHowWeKnow1947,
  title = {How We Know Universals the Perception of Auditory and Visual Forms},
  author = {Pitts, Walter and McCulloch, Warren S.},
  year = {1947},
  month = sep,
  journal = {The bulletin of mathematical biophysics},
  volume = {9},
  number = {3},
  pages = {127--147},
  issn = {1522-9602},
  doi = {10.1007/BF02478291},
  abstract = {Two neural mechanisms are described which exhibit recognition of forms. Both are independent of small perturbations at synapses of excitation, threshold, and synchrony, and are referred to partiular appropriate regions of the nervous system, thus suggesting experimental verification. The first mechanism averages an apparition over a group, and in the treatment of this mechanism it is suggested that scansion plays a significant part. The second mechanism reduces an apparition to a standard selected from among its many legitimate presentations. The former mechanism is exemplified by the recognition of chords regardless of pitch and shapes regardless of size. The latter is exemplified here only in the reflexive mechanism translating apparitions to the fovea. Both are extensions to contemporaneous functions of the knowing of universals heretofore treated by the authors only with respect to sequence in time.},
  langid = {english}
}

@article{powellUOBYQAUnconstrainedOptimization2002,
  title = {UOBYQA: Unconstrained Optimization by Quadratic Approximation},
  shorttitle = {UOBYQA},
  author = {Powell, M.J.D.},
  year = {2002},
  month = may,
  journal = {Mathematical Programming},
  volume = {92},
  number = {3},
  pages = {555--582},
  issn = {1436-4646},
  doi = {10.1007/s101070100290},
  abstract = {UOBYQA is a new algorithm for general unconstrained optimization calculations, that takes account of the curvature of the objective function, F say, by forming quadratic models by interpolation. Therefore, because no first derivatives are required, each model is defined by {$\frac{1}{2}$}(n+1)(n+2) values of F, where n is the number of variables, and the interpolation points must have the property that no nonzero quadratic polynomial vanishes at all of them. A typical iteration of the algorithm generates a new vector of variables, \$\textbackslash widetilde\{\textbackslash underline\{x\}\}\$tsay, either by minimizing the quadratic model subject to a trust region bound, or by a procedure that should improve the accuracy of the model. Then usually F(\$\textbackslash widetilde\{\textbackslash underline\{x\}\}\$t) is obtained, and one of the interpolation points is replaced by \$\textbackslash widetilde\{\textbackslash underline\{x\}\}\$t. Therefore the paper addresses the initial positions of the interpolation points, the adjustment of trust region radii, the calculation of \$\textbackslash widetilde\{\textbackslash underline\{x\}\}\$tin the two cases that have been mentioned, and the selection of the point to be replaced. Further, UOBYQA works with the Lagrange functions of the interpolation equations explicitly, so their coefficients are updated when an interpolation point is moved. The Lagrange functions assist the procedure that improves the model, and also they provide an estimate of the error of the quadratic approximation to F, which allows the algorithm to achieve a fast rate of convergence. These features are discussed and a summary of the algorithm is given. Finally, a Fortran implementation of UOBYQA is applied to several choices of F, in order to investigate accuracy, robustness in the presence of rounding errors, the effects of first derivative discontinuities, and the amount of work. The numerical results are very promising for n{$\leq$}20, but larger values are problematical, because the routine work of an iteration is of fourth order in the number of variables.},
  langid = {english}
}

@article{puseyPhaseBehaviourConcentrated1986,
  title = {Phase Behaviour of Concentrated Suspensions of Nearly Hard Colloidal Spheres},
  author = {Pusey, P. N. and {van Megen}, W.},
  year = {1986},
  month = mar,
  journal = {Nature},
  volume = {320},
  number = {6060},
  pages = {340--342},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/320340a0},
  abstract = {Suspensions of spherical colloidal particles in a liquid show a fascinating variety of phase behaviour which can mimic that of simple atomic liquids and solids. `Colloidal fluids'1\textendash 4, in which there are significant short-range correlations between the positions of neighbouring particles, and `colloidal crystals'5\textendash 7, which have long-range spatial order, have been investigated extensively. We report here a detailed study of the phase diagram of suspensions of colloidal spheres which interact through a steep repulsive potential. With increasing particle concentration we observed a progression from colloidal fluid, to fluid and crystal phases in coexistence, to fully crystallized samples. At the highest concentrations we obtained very viscous samples in which full crystallization had not occurred after several months and in which the particles appeared to be arranged as an amorphous `colloidal glass'. The empirical phase diagram can be reproduced reasonably well by an effective hard-sphere model. The observation of the colloidal glass phase is interesting both in itself and because of possible relevance to the manufacture of high-strength ceramics8.},
  copyright = {1986 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research}
}

@article{radovicMachineLearningEnergy2018,
  title = {Machine Learning at the Energy and Intensity Frontiers of Particle Physics},
  author = {Radovic, Alexander and Williams, Mike and Rousseau, David and Kagan, Michael and Bonacorsi, Daniele and Himmel, Alexander and Aurisano, Adam and Terao, Kazuhiro and Wongjirad, Taritree},
  year = {2018},
  month = aug,
  journal = {Nature},
  volume = {560},
  number = {7716},
  pages = {41--48},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0361-2},
  abstract = {Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics.},
  copyright = {2018 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Computer science;Experimental particle physics Subject\_term\_id: computer-science;experimental-particle-physics}
}

@article{ramachandranSearchingActivationFunctions2017,
  title = {Searching for Activation Functions},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.05941 [cs]},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x \textbackslash cdot \textbackslash text\{sigmoid\}(\textbackslash beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\textbackslash\% for Mobile NASNet-A and 0.6\textbackslash\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I. and Bach, Francis},
  year = {2006},
  publisher = {MIT Press},
  abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines. Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
  googlebooks = {Tr34DwAAQBAJ},
  isbn = {978-0-262-18253-9},
  langid = {english},
  keywords = {Computers / Computer Science,Computers / Machine Theory}
}

@article{razafindralandyReviewGeometricIntegrators2018,
  title = {A Review of Some Geometric Integrators},
  author = {Razafindralandy, Dina and Hamdouni, Aziz and Chhay, Marx},
  year = {2018},
  month = jun,
  journal = {Advanced Modeling and Simulation in Engineering Sciences},
  volume = {5},
  number = {1},
  pages = {16},
  issn = {2213-7467},
  doi = {10.1186/s40323-018-0110-y},
  abstract = {Some of the most important geometric integrators for both ordinary and partial differential equations are reviewed and illustrated with examples in mechanics. The class of Hamiltonian differential systems is recalled and its symplectic structure is highlighted. The associated natural geometric integrators, known as symplectic integrators, are then presented. In particular, their ability to numerically reproduce first integrals with a bounded error over a long time interval is shown. The extension to partial differential Hamiltonian systems and to multisymplectic integrators is presented afterwards. Next, the class of Lagrangian systems is described. It is highlighted that the variational structure carries both the dynamics (Euler\textendash Lagrange equations) and the conservation laws (N\oe ther's theorem). Integrators preserving the variational structure are constructed by mimicking the calculus of variation at the discrete level. We show that this approach leads to numerical schemes which preserve exactly the energy of the system. After that, the Lie group of local symmetries of partial differential equations is recalled. A construction of Lie-symmetry-preserving numerical scheme is then exposed. This is done via the moving frame method. Applications to Burgers equation are shown. The last part is devoted to the Discrete Exterior Calculus, which is a structure-preserving integrator based on differential geometry and exterior calculus. The efficiency of the approach is demonstrated on fluid flow problems with a passive scalar advection.},
  keywords = {Discrete Exterior Calculus,Geometric integration,Lie-symmetry preserving scheme,Multisymplectic,Symplectic integrator,Variational integrator}
}

@article{redaMachineLearningApplications2020,
  title = {Machine Learning Applications in Drug Development},
  author = {R{\'e}da, Cl{\'e}mence and Kaufmann, Emilie and {Delahaye-Duriez}, Andr{\'e}e},
  year = {2020},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {18},
  pages = {241--252},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2019.12.006},
  abstract = {Due to the huge amount of biological and medical data available today, along with well-established machine learning algorithms, the design of largely automated drug development pipelines can now be envisioned. These pipelines may guide, or speed up, drug discovery; provide a better understanding of diseases and associated biological phenomena; help planning preclinical wet-lab experiments, and even future clinical trials. This automation of the drug development process might be key to the current issue of low productivity rate that pharmaceutical companies currently face. In this survey, we will particularly focus on two classes of methods: sequential learning and recommender systems, which are active biomedical fields of research.},
  langid = {english},
  keywords = {Adaptive clinical trial,Bayesian optimization,Collaborative filtering,Drug discovery,Drug repurposing,Multi-armed bandit}
}

@article{redmonYOLOv3IncrementalImprovement2018,
  title = {YOLOv3: An Incremental Improvement},
  shorttitle = {YOLOv3},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.02767 [cs]},
  eprint = {1804.02767},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{riceMathematicalStatisticsData2006,
  title = {Mathematical Statistics and Data Analysis},
  author = {Rice, John A.},
  year = {2006},
  month = apr,
  publisher = {Cengage Learning},
  abstract = {This is the first text in a generation to re-examine the purpose of the mathematical statistics course. The book's approach interweaves traditional topics with data analysis and reflects the use of the computer with close ties to the practice of statistics. The author stresses analysis of data, examines real problems with real data, and motivates the theory. The book's descriptive statistics, graphical displays, and realistic applications stand in strong contrast to traditional texts that are set in abstract settings.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
  isbn = {978-0-534-39942-9},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@article{riosDerivativefreeOptimizationReview2013,
  title = {Derivative-Free Optimization: A Review of Algorithms and Comparison of Software Implementations},
  shorttitle = {Derivative-Free Optimization},
  author = {Rios, Luis Miguel and Sahinidis, Nikolaos V.},
  year = {2013},
  month = jul,
  journal = {Journal of Global Optimization},
  volume = {56},
  number = {3},
  pages = {1247--1293},
  issn = {1573-2916},
  doi = {10.1007/s10898-012-9951-y},
  abstract = {This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.},
  langid = {english}
}

@article{roblesNoteEquationState2014a,
  title = {Note: Equation of State and the Freezing Point in the Hard-Sphere Model},
  shorttitle = {Note},
  author = {Robles, Miguel and {L{\'o}pez de Haro}, Mariano and Santos, Andr{\'e}s},
  year = {2014},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {140},
  number = {13},
  pages = {136101},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4870524}
}

@article{rogersIonicLiquidsSolvents2003,
  title = {Ionic Liquids--Solvents of the Future?},
  author = {Rogers, Robin D. and Seddon, Kenneth R.},
  year = {2003},
  month = oct,
  journal = {Science},
  volume = {302},
  number = {5646},
  pages = {792--793},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1090313}
}

@article{rogersNewThermodynamicallyConsistent1984b,
  title = {New, Thermodynamically Consistent, Integral Equation for Simple Fluids},
  author = {Rogers, Forrest J. and Young, David A.},
  year = {1984},
  month = aug,
  journal = {Physical Review A},
  volume = {30},
  number = {2},
  pages = {999--1007},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.30.999},
  abstract = {A new integral equation in which the hypernetted-chain and Percus-Yevick approximations are "mixed" as a function of interparticle separation is described. An adjustable parameter {$\alpha$} in the mixing function is used to enforce thermodynamic consistency. For simple 1rn potential fluids, {$\alpha$} is constant for all densities, and the solutions of the integral equations are in very good agreement with Monte Carlo calculations. For the one-component plasma, {$\alpha$} is a slowly varying function of density, but the agreement between calculated solutions and Monte Carlo is also good. This approach has definite advantages over previous thermodynamically consistent equations.}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  shorttitle = {U-Net},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention \textendash{} MICCAI 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture Notes in Computer Science},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Brain,Cognition,Memory,Nervous System}
}

@article{rubinsteinOptimizationComputerSimulation1997,
  title = {Optimization of Computer Simulation Models with Rare Events},
  author = {Rubinstein, Reuven Y.},
  year = {1997},
  month = may,
  journal = {European Journal of Operational Research},
  volume = {99},
  number = {1},
  pages = {89--112},
  issn = {0377-2217},
  doi = {10.1016/S0377-2217(96)00385-2},
  abstract = {Discrete event simulation systems (DESS) are widely used in many diverse areas such as computer-communication networks, flexible manufacturing systems, project evaluation and review techniques (PERT), and flow networks. Because of their complexity, such systems are typically analyzed via Monte Carlo simulation methods. This paper deals with optimization of complex computer simulation models involving rare events. A classic example is to find an optimal (s, S) policy in a multi-item, multicommodity inventory system, when quality standards require the backlog probability to be extremely small. Our approach is based on change of the probability measure techniques, also called likelihood ratio (LR) and importance sampling (IS) methods. Unfortunately, for arbitrary probability measures the LR estimators and the resulting optimal solution often tend to be unstable and may have large variances. Therefore, the choice of the corresponding importance sampling distribution and in particular its parameters in an optimal way is an important task. We consider the case where the IS distribution comes from the same parametric family as the original (true) one and use the stochastic counterpart method to handle simulation based optimization models. More specifically, we use a two-stage procedure: at the first stage we identify (estimate) the optimal parameter vector at the IS distribution, while at the second stage we estimate the optimal solution of the underlying constrained optimization problem. Particular emphasis will be placed on estimation of rare events and on integration of the associated performance function into stochastic optimization programs. Supporting numerical results are provided as well.},
  langid = {english},
  keywords = {Inventory,Optimization,Score function,Sensitivity analysis,Simulation}
}

@article{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  journal = {arXiv:1609.04747 [cs]},
  eprint = {1609.04747},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@book{rudinPrinciplesMathematicalAnalysis2013,
  title = {Principles of Mathematical Analysis},
  author = {Rudin, Walter},
  year = {2013},
  publisher = {McGraw-Hill},
  abstract = {The third edition of this well known text continues to provide a solid foundation in mathematical analysis for undergraduate and first-year graduate students. The text begins with a discussion of the real number system as a complete ordered field. (Dedekind's construction is now treated in an appendix to Chapter I.) The topological background needed for the development of convergence, continuity, differentiation and integration is provided in Chapter 2. There is a new section on the gamma function, and many new and interesting exercises are included. -- Publisher description.},
  googlebooks = {bejCsgEACAAJ},
  isbn = {978-1-259-06478-4},
  langid = {english}
}

@article{saadPracticalUseKrylov1984a,
  title = {Practical Use of Some Krylov Subspace Methods for Solving Indefinite and Nonsymmetric Linear Systems},
  author = {Saad, Youcef},
  year = {1984},
  month = mar,
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {5},
  number = {1},
  pages = {203--228},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0196-5204},
  doi = {10.1137/0905015},
  abstract = {The main purpose of this paper is to develop stable versions of some Krylov subspace methods for solving linear systems of equations \$Ax = b\$. As in the case of Paige and Saunders's SYMMLQ [SIAM J. Numer. Anal., 12 (1975), pp. 617\textendash 624], our algorithms are based on stable factorizations of the banded Hessenberg matrix representing the restriction of the linear application A to a Krylov subspace. We will show how an algorithm similar to the SYMMLQ can be derived for nonsymmetric problems and we will describe a more economical algorithm based upon the \$LU\$ factorization with partial pivoting. In the particular case where A is symmetric indefinite the new algorithm is theoretically equivalent to SYMMLQ but slightly more economical. As a consequence, an advantage of the new approach is that nonsymmetric or symmetric indefinite or both nonsymmetric and indefinite systems of linear equations can be handled by a single algorithm.},
  keywords = {conjugate gradients,iterative methods,nonsymmetric systems,numerical linear algebra}
}

@article{schmidtDistillingFreeFormNatural2009,
  title = {Distilling Free-Form Natural Laws from Experimental Data},
  author = {Schmidt, Michael and Lipson, Hod},
  year = {2009},
  month = apr,
  journal = {Science},
  volume = {324},
  number = {5923},
  pages = {81--85},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1165893},
  abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the ``alphabet'' used to describe those systems. An algorithm has been developed to search for natural laws of physics in large data sets. An algorithm has been developed to search for natural laws of physics in large data sets.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {19342586}
}

@article{schoenholzStructuralApproachRelaxation2016,
  title = {A Structural Approach to Relaxation in Glassy Liquids},
  author = {Schoenholz, S. S. and Cubuk, E. D. and Sussman, D. M. and Kaxiras, E. and Liu, A. J.},
  year = {2016},
  month = may,
  journal = {Nature Physics},
  volume = {12},
  number = {5},
  pages = {469--471},
  publisher = {Nature Publishing Group},
  issn = {1745-2481},
  doi = {10.1038/nphys3644},
  abstract = {The relation between structure and dynamics in glasses is not fully understood. A new approach based on machine learning now reveals a correlation between softness\textemdash a structural property\textemdash and glassy dynamics.},
  copyright = {2016 Nature Publishing Group},
  langid = {english}
}

@article{scholl-paschingerSelfconsistentOrnsteinZernike2003,
  title = {Self-Consistent Ornstein\textendash Zernike Approximation for a Binary Symmetric Fluid Mixture},
  author = {{Sch{\"o}ll-Paschinger}, Elisabeth and Kahl, Gerhard},
  year = {2003},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {118},
  number = {16},
  pages = {7414--7424},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.1557053}
}

@article{sezerFinancialTimeSeries2020,
  title = {Financial Time Series Forecasting with Deep Learning : A Systematic Literature Review: 2005\textendash 2019},
  shorttitle = {Financial Time Series Forecasting with Deep Learning},
  author = {Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
  year = {2020},
  month = may,
  journal = {Applied Soft Computing},
  volume = {90},
  pages = {106181},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2020.106181},
  abstract = {Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.},
  langid = {english},
  keywords = {CNN,Computational intelligence,Deep learning,Finance,LSTM,Machine learning,RNN,Time series forecasting}
}

@article{shermanInverseMethodsDesign2020a,
  title = {Inverse Methods for Design of Soft Materials},
  author = {Sherman, Zachary M. and Howard, Michael P. and Lindquist, Beth A. and Jadrich, Ryan B. and Truskett, Thomas M.},
  year = {2020},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {152},
  number = {14},
  pages = {140902},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.5145177},
  abstract = {Functional soft materials, comprising colloidal and molecular building blocks that self-organize into complex structures as a result of their tunable interactions, enable a wide array of technological applications. Inverse methods provide a systematic means for navigating their inherently high-dimensional design spaces to create materials with targeted properties. While multiple physically motivated inverse strategies have been successfully implemented in silico, their translation to guiding experimental materials discovery has thus far been limited to a handful of proof-of-concept studies. In this perspective, we discuss recent advances in inverse methods for design of soft materials that address two challenges: (1) methodological limitations that prevent such approaches from satisfying design constraints and (2) computational challenges that limit the size and complexity of systems that can be addressed. Strategies that leverage machine learning have proven particularly effective, including methods to discover order parameters that characterize complex structural motifs and schemes to efficiently compute macroscopic properties from the underlying structure. We also highlight promising opportunities to improve the experimental realizability of materials designed computationally, including discovery of materials with functionality at multiple thermodynamic states, design of externally directed assembly protocols that are simple to implement in experiments, and strategies to improve the accuracy and computational efficiency of experimentally relevant models.}
}

@book{siddiqueComputationalIntelligenceSynergies2013,
  title = {Computational Intelligence: Synergies of Fuzzy Logic, Neural Networks and Evolutionary Computing},
  shorttitle = {Computational Intelligence},
  author = {Siddique, Nazmul and Adeli, Hojjat},
  year = {2013},
  month = may,
  publisher = {John Wiley \& Sons},
  abstract = {Computational Intelligence: Synergies of Fuzzy Logic, Neural Networks and Evolutionary Computing presents an introduction to some of the cutting edge technological paradigms under the umbrella of computational intelligence. Computational intelligence schemes are investigated with the development of a suitable framework for fuzzy logic, neural networks and evolutionary computing, neuro-fuzzy systems, evolutionary-fuzzy systems and evolutionary neural systems. Applications to linear and non-linear systems are discussed with examples. Key features:  Covers all the aspects of fuzzy, neural and evolutionary approaches with worked out examples, MATLAB\textregistered{} exercises and applications in each chapter Presents the synergies of technologies of computational intelligence such as evolutionary fuzzy neural fuzzy and evolutionary neural systems Considers real world problems in the domain of systems modelling, control and optimization Contains a foreword written by Lotfi Zadeh  Computational Intelligence: Synergies of Fuzzy Logic, Neural Networks and Evolutionary Computing is an ideal text for final year undergraduate, postgraduate and research students in electrical, control, computer, industrial and manufacturing engineering.},
  googlebooks = {CbpbuA0jvVgC},
  isbn = {978-1-118-53481-6},
  langid = {english},
  keywords = {Technology \& Engineering / Manufacturing,Technology \& Engineering / Quality Control}
}

@article{steinhardtBondorientationalOrderLiquids1983,
  title = {Bond-Orientational Order in Liquids and Glasses},
  author = {Steinhardt, Paul J. and Nelson, David R. and Ronchetti, Marco},
  year = {1983},
  month = jul,
  journal = {Physical Review B},
  volume = {28},
  number = {2},
  pages = {784--805},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.28.784},
  abstract = {Bond-orientational order in molecular-dynamics simulations of supercooled liquids and in models of metallic glasses is studied. Quadratic and third-order invariants formed from bond spherical harmonics allow quantitative measures of cluster symmetries in these systems. A state with short-range translational order, but extended correlations in the orientations of particle clusters, starts to develop about 10\% below the equilibrium melting temperature in a supercooled Lennard-Jones liquid. The order is predominantly icosahedral, although there is also a cubic component which we attribute to the periodic boundary conditions. Results are obtained for liquids cooled in an icosahedral pair potential as well. Only a modest amount of orientational order appears in a relaxed Finney dense-random-packing model. In contrast, we find essentially perfect icosahedral bond correlations in alternative "amorphon" cluster models of glass structure.}
}

@book{steinwartSupportVectorMachines2008,
  title = {Support Vector Machines},
  author = {Steinwart, Ingo and Christmann, Andreas},
  year = {2008},
  month = sep,
  publisher = {Springer Science \& Business Media},
  abstract = {Every mathematical discipline goes through three periods of development: the naive, the formal, and the critical. David Hilbert The goal of this book is to explain the principles that made support vector machines (SVMs) a successful modeling and prediction tool for a variety of applications. We try to achieve this by presenting the basic ideas of SVMs together with the latest developments and current research questions in a uni?ed style. In a nutshell, we identify at least three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and last but not least their computational e?ciency compared with several other methods. Although there are several roots and precursors of SVMs, these methods gained particular momentum during the last 15 years since Vapnik (1995, 1998) published his well-known textbooks on statistical learning theory with aspecialemphasisonsupportvectormachines. Sincethen,the?eldofmachine learninghaswitnessedintenseactivityinthestudyofSVMs,whichhasspread moreandmoretootherdisciplinessuchasstatisticsandmathematics. Thusit seems fair to say that several communities are currently working on support vector machines and on related kernel-based methods. Although there are many interactions between these communities, we think that there is still roomforadditionalfruitfulinteractionandwouldbegladifthistextbookwere found helpful in stimulating further research. Many of the results presented in this book have previously been scattered in the journal literature or are still under review. As a consequence, these results have been accessible only to a relativelysmallnumberofspecialists,sometimesprobablyonlytopeoplefrom one community but not the others.},
  googlebooks = {HUnqnrpYt4IC},
  isbn = {978-0-387-77242-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Data Science / Data Analytics,Computers / Information Technology,Computers / Mathematical \& Statistical Software,Computers / Optical Data Processing,Mathematics / Discrete Mathematics,Technology \& Engineering / Electronics / General,Technology \& Engineering / Imaging Systems}
}

@article{stoneApplicationsTheoryBoolean1937,
  title = {Applications of the Theory of Boolean Rings to General Topology},
  author = {Stone, M. H.},
  year = {1937},
  journal = {Transactions of the American Mathematical Society},
  volume = {41},
  number = {3},
  pages = {375--481},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/S0002-9947-1937-1501905-7},
  abstract = {Advancing research. Creating connections.},
  langid = {english}
}

@article{stoneGeneralizedWeierstrassApproximation1948,
  title = {The Generalized Weierstrass Approximation Theorem},
  author = {Stone, M. H.},
  year = {1948},
  journal = {Mathematics Magazine},
  volume = {21},
  number = {4},
  pages = {167--184},
  publisher = {Mathematical Association of America},
  issn = {0025-570X},
  doi = {10.2307/3029750}
}

@book{suttonReinforcementLearningSecond2018,
  title = {Reinforcement Learning, Second Edition: An Introduction},
  shorttitle = {Reinforcement Learning, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  publisher = {MIT Press},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  googlebooks = {uWV0DwAAQBAJ},
  isbn = {978-0-262-35270-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General}
}

@article{tangAnalyticalSolutionOrnsteinZernike1995,
  title = {Analytical Solution of the Ornstein-Zernike Equation for Mixtures},
  author = {Tang, Yiping and Lu, Benjamin C.-Y.},
  year = {1995},
  month = jan,
  journal = {Molecular Physics},
  volume = {84},
  number = {1},
  pages = {89--103},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268979500100061},
  abstract = {Solution of the Ornstein-Zernike equation under the Percus-Yevick or the mean spherical approximation is presented analytically in a matrix form. The new solution is an extension of the general Ornstein-Zernike solution suggested recently for pure fluids. The development is based on further application of the Hilbert transform and multiple-dimensional space analysis. In addition to the potential matrix, only a hard core correlation function matrix and its inverse are involved in the expression. The solution achieved in this work is explicit and is applicable to any arbitrary potential functions with an additive hard core. The first-order solution for two Yukawa mixtures has been compared with the full solution reported in the literature to serve as an example.},
  annotation = {\_eprint: https://doi.org/10.1080/00268979500100061}
}

@book{taoIntroductionMeasureTheory2011,
  title = {An Introduction to Measure Theory},
  author = {Tao, Terence},
  year = {2011},
  month = sep,
  publisher = {American Mathematical Soc.},
  abstract = {This is a graduate text introducing the fundamentals of measure theory and integration theory, which is the foundation of modern real analysis. The text focuses first on the concrete setting of Lebesgue measure and the Lebesgue integral (which in turn is motivated by the more classical concepts of Jordan measure and the Riemann integral), before moving on to abstract measure and integration theory, including the standard convergence theorems, Fubini\&\#39;s theorem, and the Caratheodory extension theorem. Classical differentiation theorems, such as the Lebesgue and Rademacher differentiation theorems, are also covered, as are connections with probability theory. The material is intended to cover a quarter or semester\&\#39;s worth of material for a first graduate course in real analysis. There is an emphasis in the text on tying together the abstract and the concrete sides of the subject, using the latter to illustrate and motivate the former. The central role of key principles (such as Littlewood\&\#39;s three principles) as providing guiding intuition to the subject is also emphasized. There are a large number of exercises throughout that develop key aspects of the theory, and are thus an integral component of the text. As a supplementary section, a discussion of general problem-solving strategies in analysis is also given. The last three sections discuss optional topics related to the main matter of the book.},
  googlebooks = {HoGDAwAAQBAJ},
  isbn = {978-0-8218-6919-2},
  langid = {english},
  keywords = {Mathematics / General}
}

@book{tolmanPrinciplesStatisticalMechanics1979,
  title = {The Principles of Statistical Mechanics},
  author = {Tolman, Richard Chace},
  year = {1979},
  month = jan,
  publisher = {Courier Corporation},
  abstract = {Classic treatment of a subject essential to contemporary physics. Classical and quantum statistical mechanics, plus application to thermodynamic behavior.},
  googlebooks = {4TqQZo962s0C},
  isbn = {978-0-486-63896-6},
  langid = {english},
  keywords = {Science / Mechanics / General,Science / Physics / General,Technology \& Engineering / General}
}

@article{torrieMonteCarloCalculation1977,
  title = {Monte Carlo Calculation of y(r) for the Hard-Sphere Fluid},
  author = {Torrie, G. and Patey, G. N.},
  year = {1977},
  month = dec,
  journal = {Molecular Physics},
  volume = {34},
  number = {6},
  pages = {1623--1628},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268977700102821},
  abstract = {The function y(r) = exp {$\beta$}u(r)g(r) is calculated for hard spheres in the region r {$<$} {$\sigma$} using umbrella-sampling Monte Carlo techniques. The resulting values are found to be well represented over the entire range 0 {$<$} r {$<$} {$\sigma$} by a simple function proposed by Grundke and Henderson.},
  annotation = {\_eprint: https://doi.org/10.1080/00268977700102821}
}

@book{trefethenApproximationTheoryApproximation2013,
  title = {Approximation Theory and Approximation Practice},
  author = {Trefethen, Lloyd N.},
  year = {2013},
  month = jan,
  publisher = {SIAM},
  abstract = {This book presents a twenty-first century approach to classical polynomial and rational approximation theory. The reader will find a strikingly original treatment of the subject, completely unlike any of the existing literature on approximation theory, with a rich set of both computational and theoretical exercises for the classroom. There are many original features that set this book apart: the emphasis is on topics close to numerical algorithms; every idea is illustrated with Chebfun examples; each chapter has an accompanying Matlab file for the reader to download; the text focuses on theorems and methods for analytic functions; original sources are cited rather than textbooks, and each item in the bibliography is accompanied by an editorial comment. This textbook is ideal for advanced undergraduates and graduate students across all of applied mathematics.},
  isbn = {978-1-61197-240-5},
  langid = {english},
  keywords = {Computers / Programming / Algorithms,Mathematics / Calculus,Mathematics / General,Mathematics / Mathematical Analysis,Mathematics / Numerical Analysis}
}

@book{trefethenNumericalLinearAlgebra1997,
  title = {Numerical Linear Algebra},
  author = {Trefethen, Lloyd N. and III, David Bau},
  year = {1997},
  month = jun,
  publisher = {SIAM},
  abstract = {This is a concise, insightful introduction to the field of numerical linear algebra. The clarity and eloquence of the presentation make it popular with teachers and students alike. The text aims to expand the reader\&\#39;s view of the field and to present standard material in a novel way. All of the most important topics in the field are covered with a fresh perspective, including iterative methods for systems of equations and eigenvalue problems and the underlying principles of conditioning and stability. Presentation is in the form of 40 lectures, which each focus on one or two central ideas. The unity between topics is emphasized throughout, with no risk of getting lost in details and technicalities. The book breaks with tradition by beginning with the QR factorization - an important and fresh idea for students, and the thread that connects most of the algorithms of numerical linear algebra.},
  googlebooks = {4Mou5YpRD\_kC},
  isbn = {978-0-89871-361-9},
  langid = {english},
  keywords = {Mathematics / Algebra / General,Mathematics / Algebra / Linear,Mathematics / Applied,Mathematics / Mathematical Analysis,Technology \& Engineering / Engineering (General)}
}

@article{tsedneeClosureOrnsteinZernikeEquation2019,
  title = {Closure for the Ornstein-Zernike Equation with Pressure and Free Energy Consistency},
  author = {Tsednee, Tsogbayar and Luchko, Tyler},
  year = {2019},
  month = mar,
  journal = {Physical Review E},
  volume = {99},
  number = {3},
  pages = {032130},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.99.032130},
  abstract = {The Ornstein-Zernike (OZ) integral equation theory is a powerful approach to simple liquids due to its low computational cost and the fact that, when combined with an appropriate closure equation, the theory is thermodynamically complete. However, approximate closures proposed to date exhibit pressure or free energy inconsistencies that produce inaccurate or ambiguous results, limiting the usefulness of the Ornstein-Zernike approach. To address this problem, we combine methods to enforce both pressure and free energy consistency to create a new closure approximation and test it for a single-component Lennard-Jones fluid. The closure is a simple power series in the direct and total correlation functions for which we have derived analytical formulas for the excess Helmholtz free energy and chemical potential. These expressions contain a partial molar volumelike term, similar to excess chemical potential correction terms recently developed. Using our bridge approximation, we have calculated the pressure, Helmholtz free energy, and chemical potential for the Lennard-Jones fluid using the Kirkwood charging, thermodynamic integration techniques, and analytic expressions. These results are compared with those from the hypernetted chain equation and the Verlet-modified closure against Monte Carlo and equations-of-state data for reduced densities of {$\rho{_\ast}<$}1 and temperatures of T{${_\ast}$}=1.5, 2.74, and 5. Our closure shows consistency among all thermodynamic paths, except for one expression of the Gibbs-Duhem relation, whereas the hypernetted chain equation and the Verlet-modified closure exhibit consistency between only a few relations. Accuracy of the closure is comparable to the Verlet-modified closure and a significant improvement to results obtained from the hypernetted chain equation.}
}

@article{valadez-perezReversibleAggregationColloidal2018,
  title = {Reversible Aggregation and Colloidal Cluster Morphology: The Importance of the Extended Law of Corresponding States},
  shorttitle = {Reversible Aggregation and Colloidal Cluster Morphology},
  author = {{Valadez-P{\'e}rez}, N{\'e}stor E. and Liu, Yun and {Casta{\~n}eda-Priego}, Ram{\'o}n},
  year = {2018},
  month = jun,
  journal = {Physical Review Letters},
  volume = {120},
  number = {24},
  pages = {248004},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.120.248004},
  abstract = {Cluster morphology of spherical particles interacting with a short-range attraction has been extensively studied due to its relevance to many applications, such as the large-scale structure in amorphous materials, phase separation, protein aggregation, and organelle formation in cells. Although it was widely accepted that the range of the attraction solely controls the fractal dimension of clusters, recent experimental results challenged this concept by also showing the importance of the strength of attraction. Using Monte Carlo simulations, we conclusively demonstrate that it is possible to reduce the dependence of the cluster morphology to a single variable, namely, the reduced second virial coefficient, B{${_\ast}$}2, linking the local properties of colloidal systems to the extended law of corresponding states. Furthermore, the cluster size distribution exhibits two well-defined regimes: one identified for small clusters, whose fractal dimension, df, does not depend on the details of the attraction, i.e., small clusters have the same df, and another related to large clusters, whose morphology depends exclusively on B{${_\ast}$}2, i.e., df of large aggregates follows a master curve, which is only a function of B{${_\ast}$}2. This physical scenario is confirmed with the reanalysis of experimental results on colloidal-polymer mixtures.}
}

@article{vandammeClassifyingCrystalsRounded2020,
  title = {Classifying Crystals of Rounded Tetrahedra and Determining Their Order Parameters Using Dimensionality Reduction},
  author = {{van Damme}, Robin and Coli, Gabriele M. and {van Roij}, Ren{\'e} and Dijkstra, Marjolein},
  year = {2020},
  month = nov,
  journal = {ACS Nano},
  volume = {14},
  number = {11},
  pages = {15144--15153},
  publisher = {American Chemical Society},
  issn = {1936-0851},
  doi = {10.1021/acsnano.0c05288},
  abstract = {Using simulations we study the phase behavior of a family of hard spherotetrahedra, a shape that interpolates between tetrahedra and spheres. We identify 13 close-packed structures, some with packings that are significantly denser than previously reported. Twelve of these are crystals with unit cells of N = 2 or N = 4 particles, but in the shape regime of slightly rounded tetrahedra we find that the densest structure is a quasicrystal approximant with a unit cell of N = 82 particles. All 13 structures are also stable below close packing, together with an additional 14th plastic crystal phase at the sphere side of the phase diagram, and upon sufficient dilution to packing fractions below 50\textendash 60\% all structures melt. Interestingly, however, upon compressing the fluid phase, self-assembly takes place spontaneously only at the tetrahedron and the sphere side of the family but not in an intermediate regime of tetrahedra with rounded edges. We describe the local environment of each particle by a set of l-fold bond orientational order parameters {$\overline q$}l, which we use in an extensive principal component analysis. We find that the total packing fraction as well as several particular linear combinations of {$\overline q$}l rather than individual {$\overline q$}l's are optimally distinctive, specifically the differences {$\overline q$}4 \textendash{} {$\overline q$}6 for separating tetragonal from hexagonal structures and {$\overline q$}4\textendash{$\overline q$}8 for distinguishing tetragonal structures. We argue that these characteristic combinations are also useful as reliable order parameters in nucleation studies, enhanced sampling techniques, or inverse-design methods involving odd-shaped particles in general.}
}

@article{vandenberghenCONDORNewParallel2005,
  title = {CONDOR, a New Parallel, Constrained Extension of Powell's UOBYQA Algorithm: Experimental Results and Comparison with the DFO Algorithm},
  shorttitle = {CONDOR, a New Parallel, Constrained Extension of Powell's UOBYQA Algorithm},
  author = {Vanden Berghen, Frank and Bersini, Hugues},
  year = {2005},
  month = sep,
  journal = {Journal of Computational and Applied Mathematics},
  volume = {181},
  number = {1},
  pages = {157--175},
  issn = {0377-0427},
  doi = {10.1016/j.cam.2004.11.029},
  abstract = {This paper presents an algorithmic extension of Powell's UOBYQA algorithm (Unconstrained Optimization BY Quadratical Approximation). We start by summarizing the original algorithm of Powell and by presenting it in a more comprehensible form. Thereafter, we report comparative numerical results between UOBYQA, DFO and a parallel, constrained extension of UOBYQA that will be called in the paper CONDOR (COnstrained, Non-linear, Direct, parallel Optimization using trust Region method for high-computing load function). The experimental results are very encouraging and validate the approach. They open wide possibilities in the field of noisy and high-computing-load objective functions optimization (from 2min to several days) like, for instance, industrial shape optimization based on computation fluid dynamic codes or partial differential equations solvers. Finally, we present a new, easily comprehensible and fully stand-alone implementation in C++ of the parallel algorithm.},
  langid = {english},
  keywords = {High-computing-load,Lagrange interpolation,Noisy optimization,Nonlinear,Optimal shape design,Parallel computing,Trust region method}
}

@article{verletComputerExperimentsClassical1967a,
  title = {Computer "Experiments" on Classical Fluids. I. Thermodynamical Properties of Lennard-Jones Molecules},
  author = {Verlet, Loup},
  year = {1967},
  month = jul,
  journal = {Physical Review},
  volume = {159},
  number = {1},
  pages = {98--103},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.159.98},
  abstract = {The equation of motion of a system of 864 particles interacting through a Lennard-Jones potential has been integrated for various values of the temperature and density, relative, generally, to a fluid state. The equilibrium properties have been calculated and are shown to agree very well with the corresponding properties of argon. It is concluded that, to a good approximation, the equilibrium state of argon can be described through a two-body potential.}
}

@article{verletIntegralEquationsClassical1980,
  title = {Integral Equations for Classical Fluids. I. The Hard Sphere Case},
  author = {Verlet, Loup},
  year = {1980},
  month = sep,
  journal = {Molecular Physics},
  volume = {41},
  number = {1},
  pages = {183--190},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268978000102671},
  abstract = {A semi-phenomenological equation for the radial distribution function of hard spheres is presented and solved. This equation yields results which agree with the `exact' results within 1 per cent. The virial pressure and the pressure obtained from the inverse compressibility formula are consistent within 2 per cent.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978000102671}
}

@article{verletIntegralEquationsClassical1981,
  title = {Integral Equations for Classical Fluids. II. Hard Spheres Again},
  author = {Verlet, Loup},
  year = {1981},
  month = apr,
  journal = {Molecular Physics},
  volume = {42},
  number = {6},
  pages = {1291--1302},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/00268978100100971},
  abstract = {With the help of a Pad\'e technique, it is shown that, given the two requirements that the equation for the hard spheres r.d.f. should involve only convolution integrals (as in the PY and HNC equations) and that it should obey the internal consistency criterion imposed by the equality of the pressures calculated using the virial or the Ornstein-Zernike theorem, an integral equation can be derived without resorting to any postulate regarding its functional form. This equation leads to an equation of state which is satisfactory except in the vicinity of the Alder transition where it under-estimates the pressure by nearly 4 per cent. A detailed examination of the fourth virial coefficient reveals some possible reasons for the validity of the integral equation derived in the present paper.},
  annotation = {\_eprint: https://doi.org/10.1080/00268978100100971}
}

@article{vompeBridgeFunctionExpansion1994,
  title = {The Bridge Function Expansion and the Self-consistency Problem of the Ornstein\textendash Zernike Equation Solution},
  author = {Vompe, A. G. and Martynov, G. A.},
  year = {1994},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {100},
  number = {7},
  pages = {5249--5258},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.467189}
}

@article{wierstraNaturalEvolutionStrategies2014a,
  title = {Natural Evolution Strategies},
  author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J{\textbackslash}"\{u\}rgen},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {27},
  pages = {949--980},
  issn = {1533-7928},
  abstract = {This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others.}
}

@article{xiaoFashionMNISTNovelImage2017a,
  title = {Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  shorttitle = {Fashion-MNIST},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  month = sep,
  journal = {arXiv:1708.07747 [cs, stat]},
  eprint = {1708.07747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{yuIntroductionEvolutionaryAlgorithms2010,
  title = {Introduction to Evolutionary Algorithms},
  author = {Yu, Xinjie and Gen, Mitsuo},
  year = {2010},
  month = jun,
  publisher = {Springer Science \& Business Media},
  abstract = {Evolutionary algorithms (EAs) are becoming increasingly attractive for researchers from various disciplines, such as operations research, computer science, industrial engineering, electrical engineering, social science, economics, etc. This book presents an insightful, comprehensive, and up-to-date treatment of EAs, such as genetic algorithms, differential evolution, evolution strategy, constraint optimization, multimodal optimization, multiobjective optimization, combinatorial optimization, evolvable hardware, estimation of distribution algorithms, ant colony optimization, particle swarm optimization, artificial immune systems, artificial life, genetic programming, etc. It emphasises the initiative ideas of the algorithm, contains discussions in the contexts, and suggests further readings and possible research projects. All the methods form a pedagogical way to make EAs easy and interesting. This textbook also introduces the applications of EAs as many as possible. At least one real-life application is introduced by the end of almost every chapter. The authors focus on the kernel part of applications, such as how to model real-life problems, how to encode and decode the individuals, how to design effective search operators according to the chromosome structures, etc. This textbook adopts pedagogical ways of making EAs easy and interesting. Its methods include an introduction at the beginning of each chapter, emphasising the initiative, discussions in the contexts, summaries at the end of every chapter, suggested further reading, exercises, and possible research projects. Introduction to Evolutionary Algorithms will enable students to: \textbullet{} establish a strong background on evolutionary algorithms; \textbullet{} appreciate the cutting edge of EAs; \textbullet{} perform their own research projects by simulating the application introduced in the book; and \textbullet{} apply their intuitive ideas to academic search. This book is aimed at senior undergraduate students or first-year graduate students as a textbook or self-study material.},
  googlebooks = {rHQf\_2Dx2ucC},
  isbn = {978-1-84996-129-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Simulation,Computers / Data Science / Data Modeling \& Design,Computers / Information Technology,Computers / Programming / Algorithms,Language Arts \& Disciplines / Library \& Information Science / General,Technology \& Engineering / Automation,Technology \& Engineering / Engineering (General)}
}

@article{zerahEfficientNewtonMethod1985,
  title = {An Efficient Newton's Method for the Numerical Solution of Fluid Integral Equations},
  author = {Zerah, Gilles},
  year = {1985},
  month = nov,
  journal = {Journal of Computational Physics},
  volume = {61},
  number = {2},
  pages = {280--285},
  issn = {0021-9991},
  doi = {10.1016/0021-9991(85)90087-7},
  abstract = {We propose a stable, straightforward algorithm for the numerical solution of integral equations for fluid pair distribution functions. The integral equation is not solved by Picard's standard iterative procedure but by Newton's method of solution of non-linear equations. The large matrix appearing in Newton's method is inverted by a conjugate gradient procedure used as a rapidly converging iterative method.},
  langid = {english}
}

@article{zerahSelfConsistentIntegral1986,
  title = {Self-consistent Integral Equations for Fluid Pair Distribution Functions: Another Attempt},
  shorttitle = {Self-consistent Integral Equations for Fluid Pair Distribution Functions},
  author = {Zerah, Gilles and Hansen, Jean-Pierre},
  year = {1986},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {84},
  number = {4},
  pages = {2336--2343},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.450397}
}

@article{zhouUniversalityDeepConvolutional2020,
  title = {Universality of Deep Convolutional Neural Networks},
  author = {Zhou, Ding-Xuan},
  year = {2020},
  month = mar,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {48},
  number = {2},
  pages = {787--794},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2019.06.004},
  abstract = {Deep learning has been widely applied and brought breakthroughs in speech recognition, computer vision, and many other domains. Deep neural network architectures and computational issues have been well studied in machine learning. But there lacks a theoretical foundation for understanding the approximation or generalization ability of deep learning methods generated by the network architectures such as deep convolutional neural networks. Here we show that a deep convolutional neural network (CNN) is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough. This answers an open question in learning theory. Our quantitative estimate, given tightly in terms of the number of free parameters to be computed, verifies the efficiency of deep CNNs in dealing with large dimensional data. Our study also demonstrates the role of convolutions in deep CNNs.},
  langid = {english},
  keywords = {Approximation theory,Convolutional neural network,Deep learning,Universality}
}

@article{zhuPhysicsconstrainedDeepLearning2019,
  title = {Physics-Constrained Deep Learning for High-Dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data},
  author = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Physics},
  volume = {394},
  pages = {56--81},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2019.05.024},
  abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.},
  langid = {english},
  keywords = {Conditional generative model,Normalizing flow,Physics-constrained,Reverse KL divergence,Surrogate modeling,Uncertainty quantification}
}


