\chapter{Introduction}
\label{Cap1}

Since the mainstream adoption of \emph{Machine Learning} (ML) methods
on common tasks such as object recognition, computer vision,
and human-computer interactions~\cite{lecunDeepLearning2015},
scientists have tried to adopt most of these techniques to further research
in their respective fields. From drug development~\cite{redaMachineLearningApplications2020}
to genetics and biotechnology~\cite{libbrechtMachineLearningApplications2015},
multiple applications of ML to current research problems have seen
widespread interest for their generalization and automatic discovery attributes.
It is with the inspiration from these applications that physicists have attempted to use 
such methods in diverse Physics fields~\cite{carleoMachineLearningPhysical2019a,dunjkoMachineLearningArtificial2018,carrasquillaMachineLearningPhases2017a}.

Most of the attempts and successes of using ML methods in Physical sciences come from
the direct application of common ML pipelines and uses, such as \emph{classification},
\emph{regression}, and \emph{unsupervised learning}~\cite{hastieElementsStatisticalLearning2009}, just to name some.
Such is the case of the determination of the
critical point of the Ising model as means of a classification task~\cite{carrasquillaMachineLearningPhases2017a}.
Similar is the case of the use of \emph{computer vision} and \emph{deep learning} 
techniques in particle physics, which have seen great applications when dealing with 
experimental data~\cite{radovicMachineLearningEnergy2018}.
In each of the previous examples, scientists have taken the most common applications
of ML methods and have adjusted them for their respective research problems.
This has the advantage that such ML techniques have been extensively researched
and developed, so physicists know that these methods are robust and useful for
the problems they have been developed for.
However, it turns out that not all ML techniques can be readily applied to the problem
at hand, and physicists should instead try to capitalize on the Physics of the problem and
use it along with the ML method to boost its usefulness, flexibility and accuracy.

It is with this perspective that physicists have preferred to incorporate most of the 
Physics into the ML method, and thus create a new form of
\emph{physics-inspired machine learning}~\cite{karniadakisPhysicsinformedMachineLearning2021a}.
One such example is the Behler-Parrinello neural network approach for energy surfaces
in the Density Functional Theory framework~\cite{behlerGeneralizedNeuralNetworkRepresentation2007a}.
Within such proposal, Behler and Parrinello chose to use some functions whose 
definition and composition are based on the properties of the system studied, which were 
atoms and their components,
and use such information as input to a \emph{regression} scheme to approximate the
energy surface of the studied system. At the moment of publication, this approach
defined a new paradigm of ML application within the physical sciences. It was no longer
the fact that simple learning tasks were used, but by including physical descriptors
in the ML methods, new ways of obtaining the same results were found.
Not only do ML methods provide mostly the same results as the physical framework they
are modeling, they also provide solutions much faster and more efficiently~\cite{zhuPhysicsconstrainedDeepLearning2019}.

Of all the research fields within Physics, in this thesis we would like to focus
our attention to the field of \emph{condensed matter physics}, and in particular, to the
field of Liquid State Theory and Soft Matter. Before we do that, however, we should
mention briefly some of the precursors to the applications of ML to said fields.
A great review by J\"{o}rg Behler~\cite{behlerPerspectiveMachineLearning2016a}
describes in great detail some of these precursors. Most applications have dealt with
materials science, computational chemistry and chemistry, and the computational aspect
of condensed matter physics. It is within these fields that new ways of using ML methods
have been developed from various needs in research. Another prime example is the coupling
of computer simulations and ML methods, such as the work by Li \emph{et al}~\cite{liMolecularDynamicsOntheFly2015}.
In this work, whenever the quantum-mechanical information is needed it is computed with
first-principles calculations. These calculations are then added to a dataset to be used
by ML methods, which in turn are used as approximators for the computer simulation.
This approach not only efficiently uses Physics in its most pure form, but it also
adopts the ML best attributes and uses them to its advantage.
For a more complete overview of some of the most impactful applications, the
review by Bedolla \emph{et al}~\cite{bedollaMachineLearningCondensed2020}
covers these and some other important aspects of ML methods applied to
condensed matter physics.

Although these are a tiny subset of all the modified applications of ML techniques
in physical sciences research, we should note that these show an important aspect
in common between them. If we wish to assimilate the physics of the problem at hand,
variations and modifications to the common ML methods and techniques are needed. 
Exploration and testing, trial and error, are an important part of the search and 
application of ML methods to Physics research. In the case of Liquid State Theory and Soft 
Matter, we can consider that most research is still in the exploration and testing stage,
although some applications have seen great success in specific scenarios.
One such successful application is the use of \emph{Support Vector Machines}\textemdash
an explicit method useful for classication and regression based on kernels and
quadratic optimization~\cite{steinwartSupportVectorMachines2008}\textemdash
in the description of the properties of glassy dynamics~\cite{schoenholzStructuralApproachRelaxation2016}.
The reason for research in Soft Matter and ML still being part of the exploration step is 
that in Soft Matter and Liquid Theory it is hard to find suitable descriptors that 
actually tell useful information from the system or phenomena~\cite{dijkstraPredictiveModellingMachine2021a}.
As such, most of the time a descriptor-based approach might not be feasible for every 
possible system. Instead, we need to explore a diverse range of possibilities when using ML 
methods within the context of Soft Matter and Liquid Theory.

Even though most research is still exploration and testing, there have been interesting
amalgamations and developments in the fields of Soft Matter, with Liquid Theory dragging
behind. In a sense, this is expected, due to the fact that Liquid Theory might be thought
of as \emph{solved}, although there is still research done within the field.
Let us focus first on the developments of Soft Matter. Instead of referring to specific
uses of ML within Soft Matter, it is more fruitful to mention some of the research groups
that have delved deep into using ML methods in Soft Matter. The group of Marjolein
Dijkstra at Utrecht University is an excellent example. Having done impactful research
in the field of Colloidal Soft Matter~\cite{dijkstraPhaseDiagramHighly1999,leunissenIonicColloidalCrystals2005},
the group is now focusing on using all the research and knowledge built and trying to
understand the best way to enforce the physics of the systems into ML techniques.
The group has explored with \emph{evolutionary algorithms} and their uses in patchy
colloids~\cite{bianchiPredictingPatchyParticle2012}. We refer to evolutionary algorithms as
derivative-free optimization algorithms that are useful for nonlinear optimization
problems~\cite{yuIntroductionEvolutionaryAlgorithms2010}.
Unsupervised methods, such as \emph{principal component analysis}~\cite{hastieElementsStatisticalLearning2009},
have been used for choosing the best descriptors in supercooled liquids~\cite{boattiniAveragingLocalStructure2021},
as well as the detection of local structure in colloidal systems~\cite{boattiniUnsupervisedLearningLocal2019a}.
All in all, these methods simplify the process of dealing with these research problems.
ML methods make it simpler and easier to identify structure and attributes from a
system. However, it is important to note that not only do these methods make it simpler,
they can also contribute to finding new things that were previously not as obvious or easy
to see.
Another work from the group is the use of classification methods 
to identify different types of crystal phases using
a mix of supervised and unsupervised methods~\cite{hastieElementsStatisticalLearning2009},
such as in the work by van Damme \emph{et al}~\cite{vandammeClassifyingCrystalsRounded2020}.
This work is quite interesting because it is a great example of using physical descriptors,
such as bond order parameters~\cite{steinhardtBondorientationalOrderLiquids1983,lechnerAccurateDeterminationCrystal2008},
along with ML methods that actively select and distinguish between the best descriptors
for the system.

Another important group that has done several advances in the use of ML within Soft Matter
is the one lead by Thomas Truskett from the University of Texas.
Their work on the use of ML methods for inverse desing of soft materials~\cite{shermanInverseMethodsDesign2020a}
is in similar ways helping out the work by the group of Dijkstra in the same research
problem~\cite{APSAPSMarcha}.
However, the work by the Truskett group is in fact more focused on the definition and
foundations of better descriptors for soft materials and off-lattice systems~\cite{jadrichUnsupervisedMachineLearning2018}.
An important topic that the group explores is the inverse design of self-assembly
systems. \emph{Self-assembly} is the property of soft materials to order their
components, such as particles, atoms or cells, without any external interactions,
into functional structures~\cite{grzybowskiSelfassemblyCrystalsCells2009}.
In this direction, their interest is particularly focused in self-assembly and how this 
phenomena can be dealt with ML methods.
In a particular work by Lindquist \emph{et al} with the Truskett group~\cite{lindquistCommunicationInverseDesign2016},
optimization-based methods were used along with standard molecular dynamics computer
simulation methods to understand how certain materials can self-assemble.
Instead of using optimization-based methods, another work from the Truskett group
deals with probabilistic ML methods for some of the same phenomena~\cite{jadrichProbabilisticInverseDesign2017}.
The difference between both these uses is that probabilistic methods are better at dealing
with probability distributions that stem from the Statistical Mechanics description of the
problem. With the use of specialized frameworks, handling these descriptors are easier
and simpler.


%% TODO: Primero mencionar algo sobre la teoría de líquidos y el hecho de que casi no hay investigacion de ML en esta área. Luego, en un párrafo hablar sobre el resultado de Goodall-Lee sobre la ecuación de OZ, y con esto comenzar la discusión de la tesis. Al final, mencionar cómo está distribuida la tesis, los capítulos y todo eso. (un párrafo para Goodall-Lee, y un párrafo para la distribución del texto)