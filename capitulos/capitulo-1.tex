\chapter{Introduction}
\label{Cap1}

Since the mainstream adoption of \emph{Machine Learning} (ML) methods
on common tasks such as object recognition, computer vision,
and human-computer interactions~\cite{lecunDeepLearning2015},
scientists have tried to adopt most of these techniques to further research
in their respective fields. From drug development~\cite{redaMachineLearningApplications2020}
to genetics and biotechnology~\cite{libbrechtMachineLearningApplications2015},
multiple applications of ML to current research problems have seen
widespread interest for their generalization and automatic discovery attributes.
It is with the inspiration from these applications that physicists have attempted to use 
such methods in diverse Physics fields~\cite{carleoMachineLearningPhysical2019a,dunjkoMachineLearningArtificial2018,carrasquillaMachineLearningPhases2017a}.

Most of the attempts and successes of using ML methods in Physical sciences come from
the direct application of common ML pipelines and uses, such as \emph{classification},
\emph{regression}, and \emph{unsupervised learning}~\cite{hastieElementsStatisticalLearning2009}, just to name some.
Such is the case of the determination of the
critical point of the Ising model as means of a classification task~\cite{carrasquillaMachineLearningPhases2017a}.
Similar is the case of the use of \emph{computer vision} and \emph{deep learning} 
techniques in particle physics, which have seen great applications when dealing with 
experimental data~\cite{radovicMachineLearningEnergy2018}.
In each of the previous examples, scientists have taken the most common applications
of ML methods and have adjusted them for their respective research problems.
This has the advantage that such ML techniques have been extensively researched
and developed, so physicists know that these methods are robust and useful for
the problems they have been developed for.
However, it turns out that not all ML techniques can be readily applied to the problem
at hand, and physicists should instead try to capitalize on the Physics of the problem and
use it along with the ML method to boost its usefulness, flexibility and accuracy.

It is with this perspective that physicists have preferred to incorporate most of the 
Physics into the ML method, and thus create a new form of
\emph{physics-inspired machine learning}~\cite{karniadakisPhysicsinformedMachineLearning2021a}.
One such example is the Behler-Parrinello neural network approach for energy surfaces
in the Density Functional Theory framework~\cite{behlerGeneralizedNeuralNetworkRepresentation2007a}.
Within such proposal, Behler and Parrinello chose to use some functions whose 
definition and composition are based on the properties of the system studied, which were 
atoms and their components,
and use such information as input to a \emph{regression} scheme to approximate the
energy surface of the studied system. At the moment of publication, this approach
defined a new paradigm of ML application within the physical sciences. It was no longer
the fact that simple learning tasks were used, but by including physical descriptors
in the ML methods, new ways of obtaining the same results were found.
Not only do ML methods provide mostly the same results as the physical framework they
are modeling, they also provide solutions much faster and more efficiently~\cite{zhuPhysicsconstrainedDeepLearning2019}.

Of all the research fields within Physics, in this thesis we focus
our attention to the field of \emph{Condensed Matter Physics}, and in particular, to the
field of Liquid State Theory and Soft Matter. Before we do that, however, we should
mention briefly some of the precursors to the applications of ML to those fields.
A review by J\"{o}rg Behler~\cite{behlerPerspectiveMachineLearning2016a}
describes in great detail some of these precursors. Most applications have dealt with
materials science, computational chemistry and chemistry, and the computational aspect
of condensed matter physics. It is within these fields that new ways of using ML methods
have been developed from various needs in research. Another prime example is the coupling
of computer simulations and ML methods, such as the work by Li \emph{et al}~\cite{liMolecularDynamicsOntheFly2015}.
In that work, whenever the quantum-mechanical information is needed it is computed with
first-principles calculations. These calculations are then added to a dataset to be used
by ML methods, which in turn are used as approximators for the computer simulation.
This approach not only efficiently uses Physics in its most pure form, but it also
adopts the ML best attributes and uses them to its advantage.
For a more complete overview of some of the most impactful applications, the
review by Bedolla \emph{et al}~\cite{bedollaMachineLearningCondensed2020}
covers these and some other important aspects of ML methods applied to
condensed matter physics.

Although these applications form a tiny subset of all the modified applications of ML 
techniques in physical sciences research, we should note that these applications show an 
important aspect in common between them. If we wish to assimilate the physics of the 
problem at hand, variations and modifications to the common ML methods and techniques are 
needed. Exploration and testing, trial and error, are an important part of the search and 
application of ML methods to Physics research. In the case of Liquid State Theory and Soft 
Matter, we can consider that most research is still in the exploration and testing stage,
although some applications have seen great success in specific scenarios.
One such successful application is the use of \emph{Support Vector Machines}\textemdash
an explicit method useful for classification and regression based on kernels and
quadratic optimization~\cite{steinwartSupportVectorMachines2008}\textemdash
in the description of the properties of glassy dynamics~\cite{schoenholzStructuralApproachRelaxation2016}.
The reason for research in Soft Matter and ML still being part of the exploration step is 
that in Soft Matter and Liquid Theory it is hard to find suitable descriptors that 
actually tell us useful information from the system or phenomena~\cite{dijkstraPredictiveModellingMachine2021a}.
As such, most of the time a descriptor-based approach might not be feasible for every 
possible system. Instead, we need to explore a diverse range of possibilities when using ML 
methods within the context of Soft Matter and Liquid Theory.

\section{Recent research in Soft Matter and Machine Learning}
Even though most research is still exploration and testing, there have been interesting
amalgamations and developments in the fields of Soft Matter, with Liquid Theory dragging
behind. In a sense, this is expected, due to the fact that Liquid Theory might be thought
of as \emph{solved}, although there is still research done within the field.
Let us focus first on the developments of Soft Matter. Instead of referring to specific
uses of ML within Soft Matter, it is more fruitful to mention some of the research groups
that have delved deep into using ML methods in Soft Matter. The group of Marjolein
Dijkstra at Utrecht University is an excellent example. Having done impactful research
in the field of Colloidal Soft Matter~\cite{dijkstraPhaseDiagramHighly1999,leunissenIonicColloidalCrystals2005},
the group is now focusing on using all the research and knowledge built and trying to
understand the best way to enforce the physics of the systems into ML techniques.
The group has explored with \emph{evolutionary algorithms} and their uses in patchy
colloids~\cite{bianchiPredictingPatchyParticle2012}. We refer to evolutionary algorithms as
derivative-free optimization algorithms that are useful for nonlinear optimization
problems~\cite{yuIntroductionEvolutionaryAlgorithms2010}.
Unsupervised methods, such as \emph{principal component analysis}~\cite{hastieElementsStatisticalLearning2009},
have been used for choosing the best descriptors in supercooled liquids~\cite{boattiniAveragingLocalStructure2021},
as well as the detection of local structure in colloidal systems~\cite{boattiniUnsupervisedLearningLocal2019a}.
All in all, these methods simplify the process of dealing with these research problems.
ML methods make it simpler and easier to identify structure and attributes from a
system. However, it is important to note that not only do these methods make it simpler,
they can also contribute to finding new things that were previously not as obvious or easy
to see.
Another work from the group is the use of classification methods 
to identify different types of crystal phases using
a mix of supervised and unsupervised methods~\cite{hastieElementsStatisticalLearning2009},
such as in the work by van Damme \emph{et al}~\cite{vandammeClassifyingCrystalsRounded2020}.
This work is quite interesting because it is a great example of using physical descriptors,
such as bond order parameters~\cite{steinhardtBondorientationalOrderLiquids1983,lechnerAccurateDeterminationCrystal2008},
along with ML methods that actively select and distinguish between the best descriptors
for the system.

Another important group that has done several advances in the use of ML within Soft Matter
is the one lead by Thomas Truskett from the University of Texas.
Their work on the use of ML methods for inverse design of soft materials~\cite{shermanInverseMethodsDesign2020a}
is in similar ways helping out the work by the group of Dijkstra in the same research
problem~\cite{APSAPSMarcha}.
However, the work by the Truskett group is in fact more focused on the definition and
foundations of better descriptors for soft materials and off-lattice systems~\cite{jadrichUnsupervisedMachineLearning2018}.
An important topic that the group explores is the inverse design of self-assembly
systems. \emph{Self-assembly} is the property of soft materials to order their
components, such as particles, atoms or cells, without any external interactions,
into functional structures~\cite{grzybowskiSelfassemblyCrystalsCells2009}.
In this direction, their interest is particularly focused on self-assembly and how these 
phenomena can be dealt with using ML methods.
In a particular work by Lindquist \emph{et al} with the Truskett group~\cite{lindquistCommunicationInverseDesign2016},
optimization-based methods were used along with standard molecular dynamics computer
simulation methods to understand how certain materials can self-assemble.
Instead of using optimization-based methods, another work from the Truskett group
deals with probabilistic ML methods for some of the same phenomena~\cite{jadrichProbabilisticInverseDesign2017}.
The difference between both these uses is that probabilistic methods are better at dealing
with probability distributions that stem from the Statistical Mechanics description of the
problem. With the use of specialized frameworks, handling these descriptors are easier
and more flexible.

\section{Liquid Theory as a basis for Soft Matter}
Up until this point, we have talked about Soft Matter and its interaction with machine
learning methods, and most of all the way it has been disrupting this research field
with novel ways to do things. Liquid State Theory was briefly mentioned before,
but it is time to state the relation between both research fields.
Why talk about Liquid State Theory and Soft Matter as if these were related somehow?
In a fascinating note by Evans, Frenkel and Dijkstra~\cite{evansSimpleLiquidsColloids2019}
they talk about the precise relation between liquids, their theory, and how by studying
such simple, but also complicated systems, soft matter research arises quite naturally.
Liquids are non-trivial systems, that are also dense, disordered and spatially correlated
but exhibit peculiar characteristics that make them different from crystals, solids,
and gases. The first attempt to understand and apply liquid theory,
along with computer simulations of liquids, was to study the
classic Lennard-Jones system to model Argon~\cite{mcdonaldCalculationThermodynamicProperties1967,verletComputerExperimentsClassical1967a}. 
Computer simulation methods, theoretical frameworks, and experimental setups helped 
understanding the true nature of liquids, giving them a special place in Physics research. 
Physicists were attempting to construct a rigorous framework which were used to 
further create new research fields.
Such fields were then created by the outstanding work of Nobel Laureate Pierre Gilles-de 
Gennes. In his Nobel Lecture~\cite{degennesSoftMatter1992} he mentioned that by studying 
simple systems it was possible to extend what was known of such systems to new, different 
systems that exhibit similar properties. His work on liquid crystals and polymer physics 
opened up a whole new research field, which we now call Soft Matter.
Then again, behind all the bewildering phenomena found in Soft Matter, and all the 
innovative research done to understand it, there exists a solid theory of the most simple 
systems known as liquids. And yet, these systems are not \emph{simple} at all, they
have evolved into active research fields such as ionic liquids~\cite{rogersIonicLiquidsSolvents2003}, complex fluids~\cite{gelbartNewScienceComplex1996},
and many others.

Despite all the research done in Liquid State Theory, most research has been focused on
understanding it through computational and theoretical frameworks. There is not much
interaction between Liquid State Theory and ML research at this point.
Yet, there seems to be an area of opportunity where Liquid State Theory can greatly
benefit from all the current research in ML methods. Arguably, the first work to delve deep 
into the intersection between Liquid State Theory and ML theory was the work done by
Goodall and Lee.
In their work, a dataset is built from Molecular Dynamics computer simulations using several
interaction potentials. Then, a ML method was used to predict properties from a new,
unknown system, using most of the information available from the dataset.
The method worked great, but the precision was not the best in some cases. Granted, this
is just the first attempt to build a solution to the problem of closure relations
for the theoretical framework of the Ornstein-Zernike equation~\cite{hansenTheorySimpleLiquids2013}.
It was then extended, in the same work, to solve an inverse design problem of liquids, 
which indeed solved the problem but was lacking in accuracy, and was not tested in
more challenging problems, which would be interesting to see the generalization of
such methods.
After all, the attempt is a novel way to look at things, and most of all, a new way to
look at Liquid State Theory and its intersection with ML theory.
Exploring this area is the sole purpose of this thesis, and in particular, in the same
line of understanding how can ML methods be used in theoretical frameworks that model
simple liquids. It seems like a worthwhile task to test various approaches and look for a
way to streamline the methods used so far in Liquid State Theory. This is the basis for
this work, to explore the use of ML methods in the theoretical formalism of the
Ornstein-Zernike equation, a rigorous framework that provides most of the structural and thermodynamical information from
a simple liquid system. We wish to understand if some of the most common ML methods
can be used, how they can be used, if they can even provide physically-relevant
solutions or not.

\section{Thesis outline}
To achieve this goal, this thesis is organized as follows. In chapter 2, 
the theory of simple liquids will be outlined, focusing primarily on the Ornstein-Zernike
formalism, as well as the theory of integral equations.
A description of the reference system\textemdash the hard sphere fluid\textemdash 
will be discussed. The thermodynamical properties of simple liquids will also be described 
in full. Computer simulation methods will also be discussed as they are the standard tool 
for current research when a baseline result is needed. Computer simulation methods can 
provide similar results to those obtained from experimental data, which is why they are 
used as a reference.
Then, in chapter 3, an overview of ML theory is presented.
In particular, those methods used in the current thesis, namely neural networks and
optimization methods. These methods have their mathematical frameworks, which will be
outlined as well, without going into much mathematical rigor.
The most common learning tasks such as supervised and unsupervised learning will be
touched only slightly, in a manner that is consistent with the current presentation
of the topics.
After the theoretical background, a discussion is carried out on the first proposal of using 
ML methods in Liquid State Theory in chapter 4.
The proposal is to use a neural network as a 
parametrization for the closure relation within the Ornstein-Zernike formalism.
It is found that the proposal works, but if the neural
network is not given too much information or description about the system, the neural
network training dynamics will reduce to a common closure relation, which is not
thermodynamically self-consistent.
In chapter 5, an exploration of including physical information to the problem gives
a more direct solution. Instead of using a neural network to parametrize the closure
relation, a known approximation is used, and let an evolutionary algorithm find the
best parameters for it. When using an approximation that is already shown to give
accurate results, the ML method is better suited to work correctly, but not without
its drawbacks, which are discussed in detail.
The thesis closes in chapter 6, with important conclusions and future ideas
to improve the proposals presented here. A few other proposal are outlined, although
not with results, or rigorous theoretical frameworks. These can be thought as
suggestions that might somehow become new ways to use ML methods within Liquid
State Theory.