\chapter{Liquid State Theory}
\label{Cap2}

In this chapter, a brief description of Liquid State Theory is carried out. In particular, 
the focus of the chapter is to state what a liquid is, its thermodynamical properties and
how equilibrium statistical mechanics is used to understand them. Then, a description of 
The hard-sphere fluid is mentioned, which is the fundamental system studied in this work.
Then, the theory of integral equations is presented, and from this the Ornstein-Zernike 
formalism is derived. The chapter is closed with a brief description of computer simulation
methods, and in particular, of the Monte Carlo computer simulation method. Monte Carlo 
simulations are numerical schemes that make it possible to study liquids based on 
first-principles, attempting to approximate the underlying probability distribution of
the system, and with this, to obtain all the physical properties of the fluid.

\section{Equilibrium Statistical Mechanics}
\label{sec:eq-statmech}

Consider a system of $N$ spherical particles in three dimensions
where each particle is characterized by its position $\vecr$ and momentum $\vecp$.
The \emph{Hamiltonian} of the system is given by
\begin{equation}
    \mathcal{H} \left( \vecr, \vecp \right) = 
    K \left( \vecp \right) + U \left( \vecr \right)
    \label{eq:hamiltonian}
\end{equation}
with $K$ the kinetic energy and $U$ the potential energy of the system.
All together, the 6$N$ variables define a \emph{phase point} in a 6$N$-dimensional
\emph{phase space}. The state point of the system is then described by a
\emph{phase space vector} $\Gamma \left( \vecr, \vecp \right)$, however, considering that all
6$N$ particles move according to Newton's equations of motion, $\Gamma$ is
a function of time, or $\Gamma(t)$. Using this phase space vector, \emph{time averages}
can be obtained for a given observable $A$ by means of the following expression
\begin{equation}
    \left< A \right> = \lim_{t \to \infty} \frac{1}{t} 
    \int_{0}^{t} A \: \Gamma(t') \: dt'
    \label{eq:time-average}
\end{equation}
If instead the complete set of state points, also known as the \emph{ensemble} of state
poitns, is considered then the average $\left< A \right>$ can then be rewritten in
terms of this ensemble. First, we point out the following for the state point vectors.
This ensemble of state points is distributed in phase space according to a probability 
distribution that is specified by the \emph{thermodynamic ensemble.} Then, if the time 
evolution of $\Gamma(t)$ is such that all states are visited eventually, irrespective of 
its initial conditions, the system satisfies the weak ergodic
theorem~\cite{kittelElementaryStatisticalPhysics2004},
and we can now rewrite the time average in \autoref{eq:time-average} with
an \emph{ensamble average} that reads
\begin{equation}
    \left< A \right> = \sum_{\Gamma} A(\Gamma) \: \rho_{ens} (\Gamma)
    \label{eq:ensemble-average}
\end{equation}
where the sum is for all state point vectors $\Gamma$ and $\rho_{ens} (\Gamma)$
is the probability density function for the ensemble. This probability function is
a weight function for the averaging procedure and should be normalized,
\begin{equation}
    \sum_{\Gamma} \rho_{ens} (\Gamma) = 1 \, .
    \label{eq:normalized}
\end{equation}
With this information, it is now time to introduce the \emph{canonical ensemble}, which
is the main ensemble used throughout this work.

\subsection{Canonical ensemble}
The canonical ensemble is established as a system of $N$ particles in a fixed volume $V$
at temperature $T$ that can exchange energy with a heat bath. This ensemble has a
probability density function associated with it,
\begin{equation}
    \rho_{NVT} = \frac{e^{-\beta \mathcal{H}(\Gamma)}}{\sum_{\Gamma} e^{-\beta \mathcal{H}(\Gamma)}}
    \label{eq:canonical-density}
\end{equation}
where $\beta=1/k_{B} T$, with $k_{B}$ Boltzmann's constant. In the classical limit of
continuous distribution functions the denominator from \autoref{eq:canonical-density}
transform into
\begin{equation}
    Q(N,V,T) = \frac{1}{N! \, h^{3N}} \int d \vecp^{N} \, d \vecr^{N} \,
    e^{- \beta \mathcal{H} \left( \vecr^N, \vecp^N \right)}
    \label{eq:canonical-partition}
\end{equation}
which is known as the \emph{canonical partition function} or sum~\cite{huangStatisticalMechanics1987}.
Here, $h$ is an arbitrary but predetermined constant with the units of energy $\times$
time. As a side note, in the original formulation by Gibbs, the value of $h$ was set
to $h=1$~\cite{gibbsElementaryPrinciplesStatistical2014}, however, since the advent
of quantum mechanics, it is now taken to be Planck's constant~\cite{tolmanPrinciplesStatisticalMechanics1979}
in order to show a correspondence between the classical and quantum formulations.
Indeed, the integrals from \autoref{eq:canonical-partition} can be separated, and the
integral over the momentum coordinates can be carried out analitically giving,
\begin{equation}
    Q(N,V,T) = \frac{1}{N! \, \Lambda^{3N}} \int d \vecr^{N} \,
    e^{- \beta U \left( \vecr^N \right)}
    \label{eq:canonical-partition-position}
\end{equation}
with $\Lambda=\sqrt{h^2 / 2 \pi m k_{B} T}$ the thermal wavelength, also known as the
\emph{de Broglie} wavelength. This again shows a correspondence between classical
and quantum formulations. The remaining integral over the positions is called the
\emph{configuration integral},
\begin{equation}
    Z(N,V,T) = \int d \vecr^{N} \, e^{- \beta U \left( \vecr^N \right)} \, .
    \label{eq:configuration-int}
\end{equation}
Finally, we arrive at the canonical ensemble density function in the continuum limit
which is
\begin{equation}
    \rho_{NVT} = \frac{e^{- \beta U(\vecr^{N})}}{Z(N,V,T)} \, .
    \label{eq:canonical-limit}
\end{equation}
Using this probability density function, the ensemble average for an observable $A$ is
computed as
\begin{equation}
    \left< A \right> = \frac{\int e^{- \beta U(\vecr^{N})} \, A(\vecr^{N}) \, d \vecr^{N}}{Z(N,V,T)} \, .
    \label{eq:average-canonical}
\end{equation}

\section{Distribution functions}

Still, the probability density function in \autoref{eq:canonical-limit} provides far more
information from the system than necessary for the calculation of thermodynamical functions
and structure properties. Instead, a focus on a small set of particles $n \ll N$ is 
preferred, in which case a \emph{reduced probability density} is defined as
\begin{equation}
    \rho^{(n)}_{N} (\vecr^{n}) \coloneqq \frac{N!}{(N-n)!} \, \frac{1}{Z(N,V,T)}
    \int e^{- \beta U(\vecr^{N})} \, d \vecr^{(N-n)} \, ,
    \label{eq:reduced-canonical}
\end{equation}
where $\rho^{(n)}_{N} (\vecr^{n})$ is also known as the equilibrium 
$n$\emph{-particle density}.
The quantity $\rho^{(n)}_{N} (\vecr^{n}) \, d \vecr^{n}$ defines the probability of finding
$n$ particles of the system with coordinates in a volume element $d \vecr^{n}$ from the
phase space, regardless of the positions and momenta of the remaining particles.
As a result of this contraction of the probability density function, it is now possible to
provide a complete description of the \emph{structure} of a fluid, while the knowledge
of low-order particle distribution functions is sufficient to calculate thermodynamic
quantities~\cite{mcquarrieStatisticalMechanics2000}.

From the definition of the $n$-particle density in \autoref{eq:reduced-canonical}, the
normalization condition is
\begin{equation}
    \int \rho^{(n)}_{N} (\vecr^{n}) \, d \vecr^{n} = \frac{N!}{(N-n)!} \, ,
    \label{eq:normalization-reduced}
\end{equation}
and in particular, the \emph{single-particle} density is
\begin{equation}
    \int \rho^{(1)}_{N} (\vecr) \, d \vecr = N ,
    \label{eq:single-particle-density}
\end{equation}
For a homogeneous fluid, \autoref{eq:single-particle-density} is then simplified
to the following expression
\begin{equation}
    \rho^{(1)}_{N} = N / V \equiv \rho
    \label{eq:homogeneous-density}
\end{equation}
where $\rho$ is the \emph{particle number density}.

The $n$-particle distribution function $g^{(n)}_{N} (\vecr^{(n)})$ is defined in terms
of the particle densities by
\begin{equation}
    g^{(n)}_{N} (\vecr^{(n)}) \coloneqq \frac{\rho^{(n)}_{N} (\vecr_{1}, \dots , \vecr_{n})}
    {\prod_{i=1}^{n} \rho^{(1)}_{N} (\vecr_{i})} \, ,
    \label{eq:gr-def}
\end{equation}
which again for a homogeneous fluid it reduces to
\begin{equation}
    \rho^{n} g^{(n)}_{N} (\vecr^{(n)}) = \rho^{(n)}_{N} (\vecr^{n}) \, .
    \label{eq:homogenous-gr}
\end{equation}
Particle distribution functions measure the extent to which the structure of a fluid 
deviates from complete randomness~\cite{hansenTheorySimpleLiquids2013}.
If the system is also isotropic, which it shall be the consideration henceforth,
the pair distribution function $g^{(2)}_{N} (\vecr_{1}, \vecr_{2})$ is a function only
of the separation $r = r_{12} = \lvert \vecr_{2} - \vecr_{1} \rvert$;
this function is the \emph{radial distribution function}, and it is referred
to as \rdf for the rest of this work.
This radial distribution function is of crucial importance in Liquid State Theory for
several reasons. First, the radial distribution function can be obtained experimentally
for liquids using X-ray diffraction, digital videomicroscopy, and light scattering
experiments~\cite{mcquarrieStatisticalMechanics2000}.
Second, thermodynamic properties of liquids can be determined using integrals that
contain the radial distribution function. Lastly, the radial distribution function can
be easily computed in computer simulations~\cite{allenComputerSimulationLiquids2017}, 
which is the standard way to obtain these quantities.

\section{Thermodynamic properties}

It was mentioned before that thermodynamics properties can be defined in terms of
distribution functions, and in particular, in terms of the radial distribution
functions~\cite{hansenTheorySimpleLiquids2013}.
It is now time to discuss such expression, and the relation between thermodynamical
quantities and the radial distribution function.

The \emph{total energy} $E$ of a system of $N$ particles can be defined in terms of the
radial distribution function as
\begin{equation}
    E = E_{ideal} + E_{excess} = \frac{3 N k_{B} T}{2} +
    \frac{N}{2} \int_{0}^{\infty} \rho \, u(r) \, g(r) \, 4 \pi r^2 \, dr
    \, ,
    \label{eq:total-energy-rdf}
\end{equation}
where $E_{ideal}$ is the ideal gas contribution, whose result comes directly from
the energy equipartition theorem~\cite{mcquarrieStatisticalMechanics2000};
$E_{excess}$ is an interaction or excess contribution that can be understood
as the interaction energy between a central particle for all the $N$ particles,
and all the neighbors located in a spherical shell of radius $r$ and thickness
$dr$. The total number of neighbors is given by $4 \pi r^2 \rho g(r) \, dr$.
The integration from $0$ to $\infty$ gives all the interaction energy, and
the factor of $1/2$ accounts for double counting of particle pairs.
The function $u(r)$ is the \emph{pairwise interaction potential} which comes
from the fact that the total potential energy $U(\vecr^{N})$ can be expressed
in terms of the individual particle interactions as follows
\begin{equation}
    U(\vecr^{N}) = \sum_{i} u_{1} (\vecr_{i}^{N}) +
    \sum_{i} \sum_{j>i} u_{2} (\vecr_{i}^{N}, \vecr_{j}^{N}) +
    \dots
    \label{eq:pairwise-energy}
\end{equation}
These kind of interactions are the most commonly studied, given that most
systems can be modeled after such interaction potentials, such as the
hard-sphere, Lennard-Jones, Yukawa~\cite{huangStatisticalMechanics1987}
and several others. The hard-sphere interaction potential will be discussed
in a later section.
In the current work, the two-particle interaction potential
$u_{2} (\vecr_{i}^{N}, \vecr_{j}^{N})$ is referred to simply as $u(r)$.

The \emph{pressure equation} is a relation between the thermodynamic pressure
$P$ and the radial distribution function, defined as
\begin{equation}
    P=P_{ideal}+P_{excess}= \frac{\rho}{\beta} - \frac{2 \pi \beta \rho}{3}
    \int_{0}^{\infty} u'(r) \, g(r) \, r^3 \, dr \, ,
    \label{eq:pressure-equation}
\end{equation}
where $P_{ideal}$ is the contribution due the ideal gas, and $P_{excess}$,
the excess pressure can be derived using classical mechanics through the
virial equation~\cite{hansenTheorySimpleLiquids2013}.

Another important quantity that must be mentioned is the
\emph{isothermal compressibility}. Its thermodynamic definition is
\begin{equation}
    \chi_{T} = - \, \frac{1}{V} { \left( \frac{\partial V}{\partial P} \right) }_{T} =
    \frac{1}{\rho} { \left( \frac{\partial \rho}{\partial P} \right) }_{T}
    \label{eq:isothermal-chi}
\end{equation}
and is a physical measure of the relative change in volume due to a change
in pressure or stress.
It can also be computed using the radial distribution function using the following
expression~\cite{hansenTheorySimpleLiquids2013}
\begin{equation}
    \frac{\chi_{T} \, \beta}{\rho} = 1 + \rho \int d \vecr \, \left[ g(r) - 1 \right]
    \, .
    \label{eq:chi-rdf}
\end{equation}
We call this the \emph{compressibility equation}, and it is a standard way of computing
the isothermal compressibility. There is an alternative definition based on integral 
equation formalism, which will be discussed in a later section.

\section{Intermolecular potentials and models}

In order to model the behavior of materials, models are chosen according to specific
physical properties. All of these models are proposals for the intermolecular potentials,
and provide a particular functional form for the potential energy $U$ of the system.
Out of all the possible interaction potentials, there is one that stands out for its
simplicity and surprising ability to generalize to complex systems. This model is
called the \emph{hard sphere model}, and shall be the topic of discussion for this
section.

The \emph{hard sphere model} is a simple pairwise interaction potential, defined as
\begin{equation}
    u_{hs} = 
    \begin{cases}
        \infty \, , &r \leq \sigma \\
        0 \, , &r > \sigma \, .
    \end{cases}
    \label{eq:hard-sphere}
\end{equation}
This interaction is a model, an approximation of the intrinsic behavior of particles.
It attempts to model the excluded volume interaction of particles, similar to what
billiards balls in three dimensions. These particles seem to hit each other only to
be separated at contact with each other. Hence, this model is by definition a
\emph{repulsive model}.
Apart from being a reference model system for the liquid
state~\cite{hansenTheorySimpleLiquids2013}, hard spheres are extremely useful in
colloid science, as mentioned in the introduction section. This is due to the fact
that is has been demonstrated that the hard sphere interaction gives rise to a
fluid-crystal phase transition around a volume fraction of 50\% of hard 
spheres~\cite{hooverMeltingTransitionCommunal1968a,gastSimpleOrderingComplex1998,roblesNoteEquationState2014a}.
This phase transition was the subject of an extensive discussion in the early
1950's and was for the first time discovered in computer
simulations~\cite{alderPhaseTransitionHard1957a}.
The relationship between this simple model and colloid science was discovered
experimentally by Pusey and van Megen~\cite{puseyPhaseBehaviourConcentrated1986},
in dense colloidal suspensions of sterically stabilized particles in a solvent.

In this work a similar potential is used, though defined differently. Looking closely
at \autoref{eq:hard-sphere}, it is straightforward to see that the potential is a
discontinuous function. In mathematical proofs and statistical mechanics frameworks,
this poses no problem. It might be hard to manipulate, mathematically speaking,
but it is possible to do so. Yet, the problem lies with the use of computer simulations.
The technical issues arise mostly in Molecular and Brownian Dynamics computer
simulations~\cite{allenComputerSimulationLiquids2017}, so a different formulation
must be employed. Computer simulations of liquids are presented in the next section.
In 2018, Báez \emph{et al}~\cite{baezUsingSecondVirial2018} used the Extended Law
of Corresponding States~\cite{valadez-perezReversibleAggregationColloidal2018}
to map the second virial coefficient of the hard sphere model to a continuous
model. By doing so, most of the dynamic and thermodynamic properties of the hard sphere
model are "passed along" to the continuous function, which in turn is suited for all
kinds of computer simulation algorithms. This new continuous function is used
throughout this work, and it is defined as follows
\begin{equation}
    u_{CP} = 
    \begin{cases}
        A \, \epsilon \left[ {\left(\frac{\sigma}{r}\right)}^{\lambda} -
        {\left(\frac{\sigma}{r}\right)}^{\lambda - 1} \right] + \epsilon \, , 
        &r < \sigma \, B \, , \\
        \hspace{1.5cm} 0 \, , &r \geq \sigma \, B \, ,
    \end{cases}
    \label{eq:cont-hs}
\end{equation}
with
\begin{equation}
    A = \lambda {\left(\frac{\lambda}{\lambda -1}\right)}^{\lambda - 1} \, ,
    \quad
    B = \left(\frac{\lambda}{\lambda -1}\right) \, .
    \label{eq:ab-params}
\end{equation}
The value of $\lambda$ is fixed to $\lambda=50$, following the finding in of
Báez \emph{et al}~\cite{baezUsingSecondVirial2018}; and the value of $\epsilon$ is
not used, instead, the reduced potential is employed, i.e., $u^{*}=u_{CP} / \epsilon$.

\section{Computer simulations of liquids}

Liquid State Theory is always supplemented with results from computer simulations
because these methods come from first-principles Physics and yield accurate estimations
of the behavior of the physical systems. Also, given the availability of computational
resources, these methods have become a standard tool for research.
There has been an extensive amount of research in these computer simulation methods,
following the pioneering work of Alder and Wainwright~\cite{alderPhaseTransitionHard1957a}.
There are two main computer simulations methods used in Liquid State Theory and
Soft Matter research: \emph{Molecular Dynamics} and \emph{Monte Carlo methods}~\cite{allenComputerSimulationLiquids2017,frenkelUnderstandingMolecularSimulation2001}.
These methods serve different purposes, and because of that, a brief description of each
one will be outlined here. However, the main focus of this work is the use of
the Monte Carlo simulation technique. \emph{Brownian dynamics} methods are
equally as important to Soft Matter research, but they are beyond the scope of this work.
The book by Jan Dhont~\cite{dhontIntroductionDynamicsColloids1996} provides a rigorous
explanation of the Brownian dynamics framework.

\subsection{Molecular dynamics}
Molecular Dynamics simulations use the basic laws of Physics to understand the behavior
of liquids. In particular, Newton's laws are used to evolve a system of particles through
time and space. For a system of $N$ particles, each with mass $m_i$, $i=1,2,\dots,N$,
the equations of motion are
\begin{equation}
    \mathbf{F}_i (t) = m_i \frac{d^2 \vecr_i}{dt^2}
    \label{eq:newton}
\end{equation}
with
\begin{equation}
    \mathbf{F}_i (t) = \sum_{j} - \nabla_{\vecr_i} U(\lvert \vecr_i - \vecr_j \rvert)
    \, ,
    \label{eq:newton-potential}
\end{equation}
the force on particle $i$ with position $r_i$ exerted by particle $j$ with position
$\vecr_j$, and $U$ the potential energy, which in the context of liquids this is the
interaction potential between the particles.

To integrate these equations of motion, different integration schemes have been developed.
At first, Alder and Wainwright~\cite{alderPhaseTransitionHard1957a} used the simple
Euler method to integrate these equations. It was later understood that such scheme
does not yield stable results, i.e., the integration scheme is \emph{unstable}.
Integration methods must conserve important physical quantities, such as energy,
momentum and phase space properties~\cite{razafindralandyReviewGeometricIntegrators2018}.
The Verlet method was later introduced in 1967 by Loup Verlet~\cite{verletComputerExperimentsClassical1967a},
in order to alleviate the drawbacks of the Euler method. These methods are crucial to obtain
correct results from molecular dynamics computer simulations.

The main appeal of molecular dynamics computer simulations is the fact that
\emph{dynamical quantities} can be computed without effort. Physical observables such
as the \emph{mean square displacement} and the \emph{dynamic structure factor}~\cite{dhontIntroductionDynamicsColloids1996,hansenTheorySimpleLiquids2013}
are of great importance to the understanding of transport properties in liquids and
more complex systems in Soft Matter.

\subsection{Monte Carlo methods}
Monte Carlo methods make use of randomness to explore a possible solution to a
given problem. The basic idea is to exploit specific but random possibilities
to provide a solution. Monte Carlo methods have successfully been applied to
three main problems: optimization, numerical integration, and probability density
function estimation~\cite{kroeseWhyMonteCarlo2014}. It might seem odd that Statistical
Physics is not mentioned here, but the reason for that is that it actually belong to
the wider range of probability density function estimation techniques. It is the
purpose of this section to explore Monte Carlo methods and their application to
the simulation of liquids, mainly because this is the method used in this
work, and because in this work there is no dynamical physical quantity involved,
only statical ones, i.e., the radial distribution function.

The basic idea of Monte Carlo methods in the computer simulation of liquids is to
generate a sequence of configurations for a given system. The idea to arrive at a
probability distribution function close to the ensemble probability distribution
function which was chosen at the start of the simulation. For instance, if a system
is under the $NVT$ ensemble, the sequence of Monte Carlo step are expected to converge
to the distribution function in \autoref{eq:canonical-limit}. Having arrived at this
estimation, observables can the be computed, as in \autoref{eq:average-canonical}.

In general, Monte Carlo methods are completely random. However, liquids cannot be
subject to complete randomness due to their intermolecular interactions. In the case
of the hard sphere fluid, the denominator of \autoref{eq:canonical-limit} would be zero
at least 50\% of the time, which would incur in numerical instabilities and observables
would not get measured properly. So, a \emph{bias} must be introduced in order to
\emph{guide} the sequence of configurations to a path which could avoid unnecessary
configurations. This bias is called \emph{importance sampling}, and the most common rule,
as well as the one used here, is the Metropolis rule~\cite{landauGuideMonteCarlo2021}.
Nevertheless, when a bias is introduced, there is one important rule that must
\emph{always} be obeyed: the condition of \emph{detailed balance}. Detailed balance
is a strong condition stating that if there is a transition from a state $o$ to a
state $n$, this should be balanced with the number of accepted trial moves that go
from state $n$ to state $o$. This can also be summarized in mathematical form,
\begin{equation}
    p(o) \, T(o \Longrightarrow n) = p(n) \, T(n \Longrightarrow o) \, ,
    \label{eq:detailed-balance}
\end{equation}
where $p(\alpha)$ is the probability to be at state $\alpha$, and 
$T(\alpha \Longrightarrow \beta)$ denotes the probability to go from a state 
$\alpha$ to a state $\beta$. It might look like a simple rule, but this condition
is what guarantees the ergodicity of the computer
simulation~\cite{landauGuideMonteCarlo2021}, i.e., that the phase space is explored
correctly and with physical significance. If this condition is not met, the results
do not represent the physical properties of the system, and the observables are not
obtained from the correct thermodynamic ensemble.

In the $NVT$ ensemble, the Metropolis rule that satisfies detailed balance is
\begin{equation}
    \frac{p(n)}{p(o)} = \frac{A(o \Longrightarrow n)}{A(n \Longrightarrow o)} =
    \exp{ \left\{ -\beta \left[U(n) - U(o)\right] \right\} }
    \label{eq:nvt-balance}
\end{equation}
with
\begin{equation}
    A(o \Longrightarrow n) = \min{ \left(1, \exp{ \left\{ -\beta \left[U(n) - U(o)\right] \right\} } \right)} 
    \, .
    \label{eq:nvt-acceptance}
\end{equation}
With this acceptance rule, it is guaranteed that the configurations that have the larger
Boltzmann factor, i.e. $\exp{ \left\{ -\beta \left[U(n) - U(o)\right] \right\} }$, 
will be visited more frequently rather than the configurations with a smaller
Boltzmann factor, which will be avoided.
In practice, the core of the method which is the Metropolis rule, is implemented as follows
\newpage
\begin{itemize}
    \item Select a particle $i$ at random.
    \item Calculate the energy $U_{i}(o)$ of particle $i$.
    \item Move particle $i$ randomly.
    \item Calculate the new energy $U_{i}(n)$ of particle $i$.
    \item Accept or reject the move according to \autoref{eq:nvt-balance} and \autoref{eq:nvt-acceptance}.
\end{itemize}
Monte Carlo methods are efficient when stratic quantities, such as the radial distribution
function, are computed. These methods can efficiently explore a large portion of the phase
space due to their stochastic nature. Although the phase space might be large, it is often
the case that a simple exploration of one, or two replicas of the system, are enough
to obtain good estimations for a given observable. In this work, Monte Carlo computer 
simulations were used for all the observables computed throughout.

\subsection{On the computation of the radial distribution function}
The procedure to obtain the radial distribution function from computer simulations is as
follows. When a Monte Carlo step has been achieved, a particle $i$ is chosen at random.
Then, all the particles that are a distance $\delta r$ from the particle $i$ are counted
as a neighbor particle. After that, all the particle that are now at a distance
$2 \delta r$ from the particle $i$ are counted. This is done after all the particles
have been accounted for. If the interaction potential between the particles is truncated
to a cutoff radius $r_c$, then this is the last value of the distance used to count
for neighboring particles. When every particle has been counter, a histogram is built,
which corresponds to a discrete distribution function. Now, this distribution function is
not normalized, and the procedure to do so is as follows. A normalization constant is 
obtained by dividing the total number of particles at position $r$ with the product
of the total number of particles in the simulation, multiplied by the total number of
configurations visited throughout the simulation, multiplied by the total volume of the
spherical shell that lies between $r$ and $r + \delta r$.
In mathematical form, this can be summarized as follows
\begin{equation}
    g(r)=\frac{1}{\rho} \left< \frac{1}{N} \sum_{i=1}^{N} \sum_{i \neq j}^{N} 
    \frac{1}{2 \pi r} \delta (r - \vecr_{ij}) \right>
    \label{eq:rdf-simulation}
\end{equation}
with $\vecr_{ij}=\vecr_{i} - \vecr{j}$.

\section{The Ornstein-Zernike Integral Equation}
Up until this point, the presentation on Liquid State Theory has been driven by the
possibility of calculating a given observable for a liquid. The method to compute such
observables is to obtain a main quantity, the radial distribution function, and
make use of any of the thermodynamic equations that relate \rdf with the needed
thermodynamic observable. There has been a discussion of how the \rdf can be obtained
through computer simulations.