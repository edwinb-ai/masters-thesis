%%--------- Comandos especiales
\newcommand{\vecr}{\mathbf{r}}
%%
\chapter{Solución a la ecuación de Ornstein-Zernike mediante redes neuronales} % Main chapter title

\label{Cap3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

Las \emph{redes neuronales} fungen como aproximadores universales~\cite{hornikMultilayerFeedforwardNetworks1989, hornikApproximationCapabilitiesMultilayer1991, cybenkoApproximationSuperpositionsSigmoidal1989},
y como tal pueden ser utilizadas para aproximar cualquier función continua para un determinado tipo de arquitectura.
En particular, se espera que una red neuronal pueda servir como paramatrización de la
función puente en la condición de cerradura de la ecuación de OZ, y así evitar la decisión
arbitraria de aproximaciones escogiendo una función puente en particular.

En este capítulo se detalla la metodología creada y los fundamentos matemáticos bajo los
cuales se puede hacer uso de redes neuronales para resolver la ecuación de OZ.
Se comparan estos resultados con los obtenidos con simulación por computadora.
En el apéndice se describe la solución general a la ecuación de OZ, mientras que
en este capítulo se detalla su modificación para emplear redes neuronales.

\section{Parametrización de la función puente}

La ecuación de Ornstein-Zernike está dada por

\begin{subequations}
    \begin{align*}
         & c(\vecr) = h(\vecr) +
        n \int_{V}
        c(\vecr^{\prime})
        h(\lvert \vecr - \vecr^{\prime} \rvert)
        d\vecr^{\prime} \label{eq:oz1} \\
         & c(\vecr)
        = \exp{\left[
                -  \beta u(\vecr)
                +  \gamma(\vecr)
                + B(\vecr)
                \right]} -
        \gamma(\vecr)
        - 1
    \end{align*}
\end{subequations}

con la notación ya conocida para esta ecuación.

Sea $N_{\theta}(\vecr)$ una red neuronal con pesos $\theta$. Se propone que
$N_{\theta}(\vecr)$ reemplace a la función puente $B(\vecr)$ en la ecuación
anterior tal que ahora se tiene la siguiente expresión para la cerradura

\begin{equation}
    c(\vecr) = \exp{\left[
            -  \beta u(\vecr)
            +  \gamma(\vecr)
            + N_{\theta}(\vecr)
            \right]} -
    \gamma(\vecr)
    - 1 ,
    \label{eq:parametrizacion}
\end{equation}

y la ecuación de OZ queda de la misma forma.
Se pretende trabajar a partir de esta hipótesis donde la función puente
pueda tomar cualquier valor en particular que permita la solución de la
ecuación de OZ.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Esquema de entrenamiento}
Ahora que se tiene la parametrización, se debe desarrollar una forma para ajustar los
pesos de la red neuronal $N_{\theta}(\vecr),$ y que al mismo tiempo permita la solución
de la ecuación de OZ.

\subsection{Función de costo}
Para crear un esquema de entrenamiento se requiere primero de una \textbf{función de costo}
que sea utilizada para generar información del ajuste necesario de los pesos $\theta$.
Tomando en cuenta que el esquema iterativo de Piccard crea una sucesión de funciones
estimadas $\{\gamma_1(\vecr), \gamma_2(\vecr), \dots, \gamma_n(\vecr)\}$, se propone que la función de costo
sea

\begin{equation}
    J(\theta) = \left[\gamma_{n}(\vecr; \theta) - \gamma_{n-1}(\vecr; \theta) \right]^2
    \label{eq:costo}
\end{equation}

donde $\gamma_{n}(\vecr; \theta)$ es la enésima función estimada de $\gamma(\vecr)$
mediante el método iterativo. La notación $\gamma(\vecr; \theta)$ indica que depende
implícitamente de los pesos de la red neuronal, como se puede ver en el ecuación~\eqref{eq:parametrizacion}.
Esto significa que si los pesos de $N_{\theta}(\vecr)$ cambian, entonces la función $\gamma$ debe cambiar.
Sin embargo, esto no significa que $\gamma$ depende \emph{directamente} de los pesos.

\subsection{Problema de optimización}
Usando la función de costo~\eqref{eq:costo} ahora se puede plantear un problema de optimización
que permita el ajuste de los pesos de $N_{\theta}(\vecr).$

El problema de optimización en este caso es \emph{sin restricciones} y está
definido de la siguiente forma

\begin{equation}
    \begin{aligned}
    & \underset{\theta}{\text{min}}
    & & J(\theta)
    \end{aligned}
    \label{eq:optimizacion}
\end{equation}

Esto significa que buscamos los parámetros óptimos $\theta^*$ que minimicen la diferencia
entre las iteraciones calculadas de las funciones $\gamma(\vecr; \theta).$
Este problema de optimización se puede resolver de forma iterativa, al mismo tiempo que
se resuelve la ecuación de OZ.

\subsection{Actualización de pesos}
El método iterativo empleado está basado en la técnica de \emph{descenso de gradiente}
para actualizar los pesos de la red neuronal.
Esto significa que la regla de actualización es

\begin{equation}
    \theta_{n+1} = \theta_n - \nabla_{\theta} J(\theta) .
    \label{eq:gradiente}
\end{equation}

En este cálculo se necesita la información del gradiente de la función de costo respecto
a los pesos. El cálculo detallado del gradiente se describe en el apéndice.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Resultados}

