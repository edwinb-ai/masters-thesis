\chapter{Computational Intelligence and Machine Learning}
\label{Cap3}

In this chapter, the fundamentals of Computational Intelligence and Machine Learning 
are developed. Particularly, the focus of the chapter is to present the main tools 
used in this thesis, namely \emph{neural networks} and \emph{evolutionary algorithms}. To 
reach a general understanding of these tools, a brief description of learning mechanisms
and numerical optimization is carried out.

\section{Computational Intelligence}
The first thing to address is the meaning and scope of \emph{Computational 
Intelligence} (CI). With recent advancements in fundamental research in this area
and its sub-fields, there seems to be a blurry definition of what exactly is CI, and there 
is no concrete one until now. For this reason, in this work the definition of CI is an 
umbrella term for several other applications. However, these applications are related to 
each other for the same reason that CI exists: to provide a computational solution to a 
problem using as inspiration the paradigms of nature-inspired intelligence. For instance, 
following the handbook by Kacprzyk and 
Pedrycz~\cite{kacprzykSpringerHandbookComputational2015},
the definition of CI is to be a collection of nature-inspired computational methods that
provide solutions to problems where \emph{hard computing} is inefficient or it not even
suited to provide a solution to a given problem. Here, there is an important distinction
that should be carried throughout the remaining of the work: that there are problems for
which traditional tools are insufficient for the traditional problems. In this work, this
is the philosophy used to provide solutions: that the traditional methods might seem hard 
and unfitting to provide solutions to the problems presented, and therefore new ways of 
approaching these solutions should be used.

In general, CI methods do not provide exact and accurate results, but this is expected.
It does not mean that CI is providing the ultimate, best solution to a problem. Rather, it 
is providing an approximate solution that can be used later with more robust algorithms and
methods. CI is not meant to be used as the sole method to solve a problem, but instead to
help find \emph{some} solution to a difficult problem. That is why CI is considered an 
umbrella term, because it comprises diverse fields that can be used to find such an 
approximate solution. Indeed, in this work, two main field of CI are used:
\emph{neural networks}, which is a part of ML, a sub-field of CI; and
\emph{evolutionary algorithms}, which are stochastic optimization methods inspired by the 
evolution mechanisms found in nature.

\section{Machine Learning}
In modern times, data is constantly being created and used to model the reality around us.
However, traditional methods (hard computing) have not been enough to handle the large
data sets created. For this reason, new ways of dealing with this information are needed. 
Most importantly, the need for automated discovery and \emph{pattern recognition} within the
data. Following Murphy~\cite{murphyMachineLearningProbabilistic2012}, ML 
is defined as the set of techniques that can automatically detect patterns in data, and use 
these patterns to create new predictions, or to perform other kinds of decision making.
In other words, \emph{the data creates the ML algorithm}, not the other way around.
It is with data that ML methods work, however, these methods are essentially
\emph{function approximation} methods, which shall be discussed in a later section.
For now, a brief overview of the different kinds of ML tasks and problems will be presented,
focusing primarily on \emph{supervised learning}. Nonetheless, there exist other types
of learning, such as \emph{unsupervised learning}~\cite{goodfellowDeepLearning2016,hastieElementsStatisticalLearning2009} and \emph{reinforcement learning}~\cite{suttonReinforcementLearningSecond2018,kaelblingReinforcementLearningSurvey1996},
which are out of scope of this work, but remain an important part of modern ML theory.

\section{Supervised Learning}
The most common kind of ML methods is that of \emph{predictive} learning. For a set 
\(\mathcal{D}={ \{(\mathbf{x}_{i}, y_i)\} }_{i=1}^{N}$ with inputs $\mathbf{x}\)
and outputs $y$, the goal of \emph{supervised learning} is to learn a map between
inputs and outputs. Here, $\mathcal{D}$ is the so-called \emph{training set} and $N$
is the number of \emph{training samples}.

In most common cases, the inputs $\mathbf{x}$ are $D$-dimensional vectors that
represent information about something. For instance, the height and weight of a
person, or the evolution of the stock market throughout the years. 
These vectors are colloquially referred to as \emph{features} or \emph{attributes}.
The general form of $\mathbf{x}$ is not defined, it can be anything from an image, a time 
series, sentences from a text, graphs, molecules, and so on.

In a similar fashion, the \emph{response} variables $y$ can be, in general, anything. 
However, there is a clear distinction given the forms that these variables can take. For 
instance, if the variable $y$ has \emph{categorical} values, the supervised learning task 
is considered a \emph{classification} problem. Categorical values come from a finite set of 
possible values, \(y_{i} \in \{1, \dots, C\}\), and might represent any type of discrete or 
nominal value. For example, it might represent the colors in a clothing line, the gender 
between people, and so on. On the other hand, if the values of $y$ are real-valued, such 
that \(y_{i} \in \mathbb{R}\), then the learning task is dubbed a \emph{regression} problem.

\subsection{Classification}
In this section, the problem of classification is looked at with more detail. Although 
classification is not used at all in this thesis, it is helpful to discuss it as it makes
it easier to understand the importance of supervised learning. It also creates a basis on 
which the problem of regression can then be generalized from.

In the problem of classification, the computer algorithm is given a data set \(\mathcal
{D}={ \{(\mathbf{x}_{i}, y_i)\} }_{i=1}^{N}\), and is asked to specify to which category 
does the input belong to. Here, $\mathbf{x}_i$ is an $n$-dimensional feature vector. As 
mentioned before, the idea is for the algorithm to learn a map, or more precisely, a \emph
{function} such that $f \colon \mathbb{R}^n \mapsto \{1, \dots, k\}$, with $k$ the total 
number of categories, or \emph{classes}, to which the input can be assigned to. More 
specifically, when $y=f(\mathbf{x})$, the model assigns an input described by the 
$n$-dimensional feature vector $\mathbf{x}$ to a particular categorical value of $y$.

One of the most common uses of classification is object recognition. The goal of the object 
recognition problem is to decode a particular image with a specific object in it, and label 
it accordingly. An extremely popular data set for this kind of task is the MNIST 
handwritten digits data set~\cite{lecunGradientbasedLearningApplied1998a}, and its most 
common variation, the Fashion-MNIST~\cite{xiaoFashionMNISTNovelImage2017a}. 
These data sets are 
comprised of several images and categories, and the goal is to \emph{classify} each of the 
several categories. In the case of the handwritten digits, the goal is to specify which 
digit is represented in the image; in the case of the Fashion-MNIST data set, the goal is 
to specify the type of clothing. These data sets have become the standard benchmarks 
in the ML community for a long time. They are used primarily to test whether a ML 
algorithm is working properly, and if it is \emph{accurate} enough. The word 
\emph{accuracy} means something quite specific in ML theory, and will be discussed in a 
later section.

\subsection{Regression}
In \emph{regression}, the goal is for the computer algorithm to learn a map, or 
equivalently a function $f \colon \mathbb{R}^n \mapsto \mathbb{R}$, that predicts a 
numerical or real-valued output. The resemblance to the classification task is quite 
obvious: categories or classes can now be represented as a continuous value within the 
reals and there is no restriction for the number of classes. In a sense, this is the most 
general form of classification problem. So then, why make a distinction between the two 
problems? Most of the time, regression tasks are created to \emph{predict}, \emph{forecast}
, or even \emph{generate} outputs for a given data set $\mathcal{D}$.

One of the major applications of regression is \emph{time series forecast}. This can 
be found mostly in economical and financial contexts~\cite{bontempiMachineLearningStrategies2013,sezerFinancialTimeSeries2020}, but there are also 
applications in bioengineering and medical situations~\cite{mccoyAssessmentTimeSeriesMachine2018}. The goal here is to obtain a good approximation of 
the function $f$ in order to \emph{extrapolate} its 
domain to obtain new, and unseen, results. It is expected that ML methods, with their 
ability to find undiscovered 
patterns, can effectively predict and forecast outputs that are not in the data set 
$\mathcal{D}$. This is an extremely hard task, but one that has been finding a lot of 
applications and many groups have done extensive research on the matter.

Regression is an important task, and in \autoref{Cap4} it is the primary learning mechanism 
used to solve a particular problem in Liquid State Theory. The problem of regression, as 
stated before, is to learn a \emph{good} approximation of the function $f$. Again, the 
measure of \emph{good} is not clear enough, as was the case with \emph{accuracy}, and both 
these concepts shall be discussed next.

\subsection{Performance Measure}
ML algorithms must be assessed on their abilities to perform a certain task $T$, either a 
classification or regression problem in the current discussion. 
It is common practice to extract a subset of the 
training data set $\mathcal{D}$ as the \emph{testing set}, $\mathcal{T}$, to evaluate the 
algorithm in the task $T$. The measure depends on the task $T$, as well as the type of data 
used.

In classification tasks, the \emph{accuracy} is one of the most general performance 
measures. The accuracy is simply defined as the proportion of data points in $\mathcal{T}$ 
for which the model produces the correct output. This measure is quite strict in the sense 
that if there are examples that are \emph{mislabeled}, this is carried on to the ML model.

Another common performance measure for classification tasks is the receiver operating 
characteristic, also known as the ROC~\cite{hastieElementsStatisticalLearning2009}. More 
specifically, the area under the ROC curve. This measure of performance is created by 
plotting the \emph{true positive rate} against the \emph{false positive rate}. This measure 
is used because these rates are more permissive, since they stem from the well established 
type I and II errors from statistical theory~\cite{riceMathematicalStatisticsData2006}. 
Depending on the data set, other measures can 
be used, such as the F1 score, the Jaccard index, Akaike information criterion, and 
others~\cite{murphyMachineLearningProbabilistic2012}.

Regression tasks are different when measuring performance since these methods attempt to 
approximate continuous, real-valued functions. So in a sense, one can simply use \emph
{metrics} in the mathematical analysis sense of the word. For instance, the use of the 
$L^2$ norm, also known as the so-called Euclidean norm, defined as
\begin{equation}
    { \left\lVert y - \hat{y} \right\rVert }_{2} = \sqrt{ \sum_{i=1}^{N} { \left(y_i - \hat{y}_i \right) }^2 }
    \; ,
    \label{eq:l2norm}
\end{equation}
is extremely common in regression tasks. Here, the training examples $y$ are measured 
against the output value from the ML model, $\hat{y}$. Another extremely common, and 
profoundly useful metric, is the $L^1$ norm,
\begin{equation}
    { \left\lVert y - \hat{y} \right\rVert }_{1} = \sum_{i=1}^{N} \left\lvert y_i - \hat{y}_i \right\rvert
    \; .
    \label{eq:l1norm}
\end{equation}
The $L^1$ has the amazing property that it can create \emph{sparse} representations of the 
learned function $f$. This is the basis of the LASSO method~\cite{hastieElementsStatisticalLearning2009}, a very useful and common ML method to do 
\emph{variable selection}, the discrimination of variables that are useful or not to the 
prediction of the model; and \emph{regularization}, which is adding information in order to 
solve a problem that might seem hard or impossible to solve, also known as an
\emph{ill-posed} problem~\cite{goodfellowDeepLearning2016}.
When generalizing these metrics to the full data set, they take on different names. For 
instance, the squared $L^2$ norm takes the name \emph{mean squared error}, defined as
\begin{equation}
    MSE \left( y_i, \hat{y}_i \right) = \frac{1}{N} \sum_{i=1}^{N} { \left(y_i - \hat{y}_i \right) }^2
    \; ,
    \label{eq:mse} 
\end{equation}
where $N$ is the total number of examples used to compute the MSE.

Similarly, the $L^1$ norm can be generalized to the \emph{mean absolute error}, defined as
\begin{equation}
    MAE \left( y_i, \hat{y}_i \right) = \frac{1}{N} \sum_{i=1}^{N} \left\lvert y_i - \hat{y}_i \right\rvert
    \; .
    \label{eq:mae}
\end{equation}
These generalizations might seem trivial, but they allow for more general formulations, as 
they are in fact \emph{estimators} that measure the average of the outputs. When measuring 
the performance of regression models, the goal is to \emph{minimize} these errors, with 
zero being the optimal value.

\subsection{Approximation Theory}
Before closing this brief overview on ML theory, a note on approximation theory is in 
order. The link to ML has to do with the striking resemblance between both classification 
and regression tasks. Both problems seem to be, in a sense, the same exact problem which is to learn a \emph{map} or \emph{function} that relates an input with an output. However, this problem is not 
new, and in fact, it has been extensively studied before, so much so that is has created a 
branch within mathematics, called \emph{approximation theory}.

The primary goal of approximation theory is simple: to understand how functions can be \emph{best} approximated with even simpler functions~\cite{trefethenApproximationTheoryApproximation2013}. This sounds analogous to the learning tasks, however, there was no mention of simpler functions in the previous discussion. This is because to see this more clearly, there has to be a detailed explanation of each of the models used. For instance, \emph{Gaussian Processes}~\cite{rasmussenGaussianProcessesMachine2006} can approximate an arbitrary function with Gaussian functions, or more precisely, with normal probability distributions. In any case, there is not enough space in this work to talk about all the possible methods and how these are formulated, even if they are formulated based on simpler functions or not.

However, the discussion can be simplified by noting the following. If looked at closely,
\autoref{eq:l2norm} and \autoref{eq:l1norm} look quite similar. Indeed, there is a generalization of this norm, called the $L^p$ norm,
\begin{equation}
    { \left\lVert y - \hat{y} \right\rVert }_{p} = { \left( \sum_{i=1}^{N} { \left(y_i - \hat{y}_i \right) }^{p} \right) }^{1/p}
    \; ,
    \label{eq:lp-norm}
\end{equation}
which generalizes the Euclidean norm to a more general norm, defined now in function spaces 
called $L^p$ spaces~\cite{rudinPrinciplesMathematicalAnalysis2013}. These spaces are, 
roughly speaking, a generalization of vector spaces where the basis that span the linear 
spaces is comprised of functions instead of vectors. There is, however, a particular form of the $L^p$ norm called the \emph{supreme norm}, or equivalently, the \emph{infinity norm} defined as
\begin{equation}
    { \left\lVert y - \hat{y} \right\rVert }_{\infty} = 
    \underset{i}{\max}{\left\lvert y_i - \hat{y}_{i} \right\rvert}
    \; ,
    \label{eq:linf-norm}
\end{equation}
also known as \(L^{\infty}\), and it is a function space that contains all the essentially bounded measurable functionss~\cite{taoIntroductionMeasureTheory2011}.

Why is $L^{\infty}$ so important in this context? The answer is one simple, but outstandingly powerful theorem: the Stone-Weierstrass theorem~\cite{stoneApplicationsTheoryBoolean1937, stoneGeneralizedWeierstrassApproximation1948}. In 
1885, Karl Weierstrass proved that any function can be approximated by a polynomial of a 
given degree. This result is so powerful that it created the field of approximation theory. 
And it is so compelling, that it might arguably be the fundamental theorem of ML theory. 
This is the reason for the \(L^{\infty}\), and its relation to the learning tasks should 
become clear once the main theorem is stated.
\begin{theorem}[\textbf{Weierstrass approximation theorem}]
    If \(f\) is a continuous real-valued function on \([a, b]\), and if any \(\epsilon > 0\) is given, then there exists a polynomial \(P\) on \([a, b]\) such that
    \[
        { \left\lVert f(x) - P(x) \right\rVert }_{\infty} < \epsilon
        \]
        for all \(x \in [a, b]\) .
    \label{approx}
\end{theorem}
The proof is left for a specialized text on the subject, for instance the book by Richard 
Beals~\cite{bealsAnalysisIntroduction2004}. Nonetheless, it is important to see the striking resemblance between the kinds of learning discussed so far, and \autoref{approx}.
On one hand, classification and regression both deal with finding the best mapping between 
inputs and outputs. On the other hand, \autoref{approx} states that every function can be 
approximated by another, simpler function. So, in a sense, these problems are related. One 
seeks to find the best mapping, while the other guarantees that there is such a mapping. 
\autoref{approx} helps visualize that the main problem of ML theory is to approximate the 
underlying function between inputs and outputs in a data set. However, the question of how to find such polynomial \(P\) is still open.

With the presentation of the Weierstrass approximation theorem it is now time to move on to 
the case of neural networks, which are in fact a way to build a polynomial \(P\) with the 
help of data. If looked at closely, the theorem only states that there is such polynomial, 
but does not specify how it might be constructed or formulated. There are roughly two 
ways: with pure mathematics or with the help of data. To use mathematics is to turn to the help of approximation theory; in this case 
orthogonal polynomials are used, such as Tchebyshev polynomials, to build such a polynomial 
\(P\). Of course, many other approximation methods can be used~\cite{trefethenApproximationTheoryApproximation2013}.
On the other hand, turn to 
statistics and data, and seek the help of ML theory, such that the data is used to construct
such a polynomial. Before moving on, it should be pointed out that this is not a unique or 
novel way of seeing the problem of learning. Indeed, a more general formulation is based on 
probability theory, which uses much more rigorous arguments to link the relation between ML 
theory and approximation theory. 
The book by Murphy~\cite{murphyMachineLearningProbabilistic2012} is a great reference for 
that.

\section{Neural Networks}
In this section, neural networks (NN), their architectures, training and mathematical formulations are discussed. Although, this is just a short overview of the full theory of NN. In recent years, NN have been the most used, researched and applied method in modern ML. Up to this day, there are unmeasurable number of research articles and books devoted to NN. So this space is obviously quite small to fit all the information about them. It is for this reason that this section will focus primarily on how NN are trained and how they learn a mapping given a data set.
For a more thorough understanding of NN, these references~\cite{mehligMachineLearningNeural2021,goodfellowDeepLearning2016,hastieElementsStatisticalLearning2009,bernerModernMathematicsDeep2021} should suffice.

\subsection{Motivation}
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{figuras/capitulo-3/perceptron}
    \caption{A representation of the original McCulloch-Pitts perceptron.}
    \label{fig:perceptron}
\end{figure}

The basic idea of NN is to mimic the way the brain works, and more specifically, the way 
neurons interact with each other. In the modern form of NN, this interaction is highly 
idealized in the sense that the actual human brain does not follow the same form of 
interaction, but it is somewhat a rough approximation. In 1947, Pitts and McCulloch~\cite{pittsHowWeKnow1947} devised the \emph{perceptron}, an idealized model of a neuron. This perceptron can be observed in \autoref{fig:perceptron}. Here, the middle circle is the perceptron or \emph{unit}, and it accepts three inputs \(x_1, x_2, x_3\), or equivalently \(\mathbf{x}=(x_1, x_2, x_3)\) with \(\mathbf{x}\) a \(3\)-dimensional input vector. The output of the McCulloch-Pitts perceptron is a single binary output, it can either return a \(0\) or a \(1\). Then, in 1958, Rosenblatt~\cite{rosenblattPerceptronProbabilisticModel1958} extended this idea and introduced a way to compute the output of the perceptron. He introduced what he called \emph{weights}, and gave a weight to each of the three inputs, \(w_1, w_2, w_3\). Later, he proposed to compute the output as a weighted sum of each of the three inputs, such that the output would have the following form,
\begin{equation}
    \text{output} = \begin{cases}
        0 \quad \text{if} \quad \sum_{i} w_i x_i \leq \varepsilon \\
        1 \quad \text{if} \quad \sum_{i} w_i x_i > \varepsilon
    \end{cases}
    \; ,
    \label{eq:output-perceptron}
\end{equation}
where \(\varepsilon\) is a \emph{threshold} value. This model is simple, yet effective. If 
one input would be more important than the other, the weight could be adjusted to reflect 
this. As simple as the model is, it is a good approximation of how decision-making models 
can be constructed efficiently. Furthermore, a more complex decision-making system can be built if several perceptrons are connected with each other. After all, the inputs and outputs of a single perceptron do not have limitations, and they can well be defined to be the inputs and outputs of other perceptrons as well. Instead of using a simple perceptron, a more complex structure can be built upon using several of them. If \emph{layers} of perceptrons are stacked between each other, a more complete model of decision-making is created. This is the case of the \emph{multilayer perceptron}, or MLP, shown in \autoref{fig:multi-perceptron}.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{figuras/capitulo-3/multi-perceptron}
    \caption{A representation of several perceptrons connected with each other.}
    \label{fig:multi-perceptron}
\end{figure}

Despite this, the model has serious drawbacks. For one, 
there is no clear understanding of the threshold value and what it is. On the other hand, 
having the perceptron output just two values might make it inaccesible in the most general 
case. After all, the idea of NN is to learn a mapping, a function that in general is 
real-valued. Finally, how are the weights adjusted? Are these values picked randomly? Does 
the user choose them? If in fact NN are learning models, then there is no real reason why 
the user would likely pick the weights for each problem they need to solve. All of this 
questions will be addressed in a later section, when training is discussed in more detail.
For now, architectures and activations functions will be presented next. These topics will
provide answers as to how the output of perceptrons can be modified to return other values,
rather than just a \(0\) or \(1\).

\subsection{Architectures}
\subsection{Universal Approximation Theorem}
\subsection{Training}

\section{Evolutionary Computation}
\subsection{Overview of Optimization}
\subsection{Black-box Optimization}
\subsection{Natural Evolution Strategies}