%%--------- Comandos especiales
\newcommand{\vecr}{\mathbf{r}}
\newcommand{\veck}{\mathbf{k}}
\newcommand{\nnet}{N_{\theta}(\mathbf{r})}
%%
\chapter{Neural networks as an approximation for the bridge function} % Main chapter title

\label{Cap3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

Neural networks can be used as \emph{universal approximators}~\cite{hornikMultilayerFeedforwardNetworks1989, hornikApproximationCapabilitiesMultilayer1991, cybenkoApproximationSuperpositionsSigmoidal1989},
in other words, they can take the form of any continuous function for some specific
types of architectures.
In particular, it is hypothesized that a neural network might be useful as a bridge function
parametrization in the closure expression for the Ornstein-Zernike equation. If this is true,
then choosing a particular approximation can be avoided for a given interaction potential, 
and leave the choice of the bridge function to the neural network itself, while
simultaneously solving the Ornstein-Zernike equation.

In this chapter, we show in detail the methodology created to achieve such a task, and
the mathematical structure with which a neural network can be used to solve the
Ornstein-Zernike equation.
These results are compared to those obtained from computer simulations to assess the
quality of the solution.
In the appendix, the detailed algorithm used to solve the Ornstein-Zernike equation
is presented, along with a detailed computation of the gradients used for the
training scheme. Here, we shall focus only on the main results and the algorithm structure
in general.

\section{Parametrization of the bridge function}

The Ornstein-Zernike equation is given by the following expression

\begin{subequations}
    \begin{align*}
         & c(\vecr) = h(\vecr) +
        n \int_{V}
        c(\vecr^{\prime})
        h(\lvert \vecr - \vecr^{\prime} \rvert)
        d\vecr^{\prime} \label{eq:oz1} \\
         & c(\vecr)
        = \exp{\left[
                -  \beta u(\vecr)
                +  \gamma(\vecr)
                + B(\vecr)
                \right]} -
        \gamma(\vecr)
        - 1
    \end{align*}
\end{subequations}

with the already known notation for each quantity (Ref a marco teórico).

Let $\nnet$ be a neural network with weights $\theta$. The main hypothesis
of this chapter is that $\nnet$ can replace the bridge function $B(\vecr)$
in the previous equation, which will yield the following expression for
the closure relation

\begin{equation}
    c(\vecr) = \exp{\left[
            -  \beta u(\vecr)
            +  \gamma(\vecr)
            + \nnet
            \right]} -
    \gamma(\vecr)
    - 1 .
    \label{eq:parametrizacion}
\end{equation}

With this new expression, the main problem to solve is to find the weights
of $\nnet$ that can successfully solve the Ornstein-Zernike equation
for a given interaction potential, $\beta u(\vecr)$.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Training scheme}
Now that a parametrization is defined, a way to fit the weights of the neural network must
be devised. This new numerical scheme must also be able to solve the OZ equation, while
simultaneously finding the appropiate weights for $\nnet$.

\subsection{Cost function}
It was mentioned previously that the main problem to solve is to find the weights of
$\nnet$ that can successfully solve the Ornstein-Zernike equation
for a given interaction potential.
To solve such problem, a \textbf{cost function} must be defined, and be used as part of
a \emph{minimization} problem.

To define such a function, we consider the successive approximations obtained from the
iterative Piccard scheme to solve the OZ equation, $\{\gamma_1(\vecr), \gamma_2(\vecr), \dots, \gamma_n(\vecr)\}$.
From this, we expect to have found a solution when each approximation
is \emph{close enough} to the previous one. This can be translated into the following
cost function

\begin{equation}
    J(\theta) = \left[\gamma_{n}(\vecr; \theta) - \gamma_{n-1}(\vecr; \theta) \right]^2
    \label{eq:costo}
\end{equation}

where $\gamma_{n}(\vecr; \theta)$ is the $n-$th approximation of the indirect
correlation function, $\gamma(\vecr)$.
The notation $\gamma(\vecr; \theta)$ indicates that the function depends implicitly
on the weights of the neural network, as seen in equation~\eqref{eq:parametrizacion}.
This means that, if the weights of $\nnet$ change, we should expect a change in the output
from the $\gamma$ function. Nevertheless, this does not mean that the indirect
correlation function itself depends explicitly, nor directly, on the weights of
$\nnet$.

In other words, the expression~\eqref{eq:costo} is requiring for the last two approximations
to be as equal as possible. This will enforce a change on the weights every time both
approximations deviate between them.

\subsection{Optimization problem}
With a cost function at hand, an optimization problem can be defined such that the
weights of $\nnet$ will be adjusted properly.

This optimization problem is in fact an \emph{unconstrained optimization problem},
and it is defined simply as

\begin{equation}
    \begin{aligned}
         & \underset{\theta}{\text{min}}
         & & J(\theta)
    \end{aligned}
    .
    \label{eq:optimizacion}
\end{equation}

This formulation is just a search for the best values for the weights that minimize
the squared difference between successive approximations.
This optimization problem can be solved iteratively, along with the solution of the
OZ equation, which is also an iterative process.

\subsection{Weight updates}
The iterative method employed to adjust the weights of $\nnet$ is based on the
\emph{gradient descent} method~\cite{nocedalNumericalOptimization2006}.
The most general update rule for a method based on gradient descent reads

\begin{equation}
    \theta_{n+1} = \theta_n - \eta \nabla_{\theta} J(\theta) .
    \label{eq:gradiente}
\end{equation}

where $\eta$ is known as the \emph{learning rate}, and it is a hyperparameter
that controls the step size at each iteration while moving toward the minimum
of a cost function. Tthis value needs to be \emph{tuned} accordingly, so
that the method converges properly.

% TODO: Cambiar el nombre del apéndice
Regardless of the particular expression for the weight updates, every method
based on the gradient descent method \emph{requires} the gradient information from
the cost function with respect to the weights, $\nabla_{\theta} J(\theta)$.
In this particular case, the detailed computation of the gradient is described in
the appendix (Ref a apéndice).
Once this information is obtained, all that is left is to build an algorithm that
can correctly use this training scheme and solve the OZ equation.

\subsection{Solving the Ornstein-Zernike equation with neural networks}
Having described all the necessary elements needed, a general layout for the solution
of the Ornstein-Zernike using neural networks is now presented.

Thus, we have the following step to solve the OZ using the parametrization~\eqref{eq:parametrizacion}:

\begin{enumerate}
    \item Given a particular interaction potential $\beta u(\vecr)$, equation~\eqref{eq:parametrizacion} is used to obtain the value of the direct correlation function $c(\vecr; \theta)$, which now depends implicitly on the weights of $\nnet$. In this step, an initial value for $\gamma_{n}(\vecr)$ is needed, which is initialized based on the five-point Ng methodology. (Ref a apéndice)
    \item The newly found function $c(\vecr; \theta)$ is transformed to a reciprocal space by means of the Fourier transform yielding the new function $\hat{c}(\veck; \theta)$.
    \item Then, the full OZ equation(Ref a ec) is Fourier transformed. Using the information from the previous step, a new estimation of the indirect correlation function is obtained, $\hat{\gamma}_{n+1}(\veck; \theta)$.
    \item The Fourier transform is applied once again to return all the functions to real space. With this operation, a new estimation $\gamma_{n+1}(\vecr; \theta)$ is computed from the transformed function, $\hat{\gamma}_{n+1}(\veck; \theta)$.
    \item Both estimations, $\gamma_{n}$ and $\gamma_{n+1}$, are used to evaluate the cost function~\eqref{eq:costo}, and the computation of the gradient $\nabla_{\theta} J(\theta)$ is performed.
    \item The weights $\theta$ are updated with the gradient descent rule~\eqref{eq:gradiente}, and the process is repeated from step 1. In the next iteration, the initial value for the indirect correlation function will be $\gamma_{n+1}$, and a new estimation $\gamma_{n+2}$ will be obtained. This process is repeated until convergence.
\end{enumerate}

\subsection{Convergence criterion}
The procedure describe in the previous section is repeated indefinetely until convergence
is achieved. This convergence criterion is defined as follows

\begin{equation}
    {\lvert \gamma_{n+1} - \gamma_{n} \rvert}^2 \leq \epsilon
    \label{eq:tolerancia}
\end{equation}

where $\lvert \cdot \rvert$ indicates the absolute value, and $\epsilon \in \numlist{0; 1}$.
In particular, the numerical tolerance in all the experiments has a value of $\epsilon = \num{1e-5}$.
This means that the weights are adjusted until the successive estimations of the $\gamma$
functions are equal between them, up to the defined tolerance $\epsilon$.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Implementación}
En esta sección se detallan los aspectos importantes de la implementación del método
descrito en la sección anterior, como lo son la arquitectura de la red neuronal, el método
de optimización, y la elección de las funciones de activación, así como los parámetros
utilizados para resolver la ecuación de OZ.

\subsection{Elección de optimizador}
La regla general de actualización de pesos~\eqref{eq:gradiente} fue implementada para 
resolver el problema de optimización, sin embargo, problemas numéricos impedían la
convergencia del entrenamiento.

Para resolver este problema se optó por utilizar el método \emph{Adam}~\cite{kingmaAdamMethodStochastic2017},
el cual es un método de optimización adecuado para entrenamiento de redes neuronales,
sobre todo cuando el gradiente puede ser \emph{disperso}, i.e. que muchas de sus entradas
sean cero.
El método \emph{Adam} emplea diversas reglas para ajustar la dirección de descenso del
gradiente, así como los hiperparámetros asociados al método. En particular, este método
utiliza dos hiperparámetros, $\beta_1$, que controla el promedio móvil
del gradiente calculado; y $\beta_2$, que controla el cuadrado del gradiente. Ambas
cantidades son necesarias para la convergencia óptima del algoritmo.

Las ecuaciones que definen al método de optimización son las siguientes

\begin{align}
    m &= \beta_1 m - (1 - \beta_1) \nabla_{\theta} J(\theta) \nonumber \\
    s &= \beta_2 s + (1 - \beta_2) \nabla_{\theta} J(\theta) \odot \nabla_{\theta} J(\theta) \nonumber \\
    \hat{m} &= \frac{m}{1 - \beta_1^t} \nonumber \\
    \hat{s} &= \frac{s}{1 - \beta_2^t} \nonumber \\
    \theta &= \theta + \eta \hat{m} \oslash \sqrt{\hat{s} + \varepsilon}
    \label{eq:adam}
\end{align}

donde $\odot$ significa multiplicación entrada por entrada, o producto de Hadamard;
$\oslash$ significa división entrada por entrada, o división de Hadamard;
y $\varepsilon$ es un número de tolerancia que impide la división por cero.

En los resultados presentados en esta sección, se emplearon los hiperparámetros con sus
valores estándar, $\beta_1=\num{0.9}$ y $\beta_2=\num{0.999}$.Es importante notar que
este método de optimización utiliza sus propios mecanismos para controlar los gradientes,
así como estos hiperparámetros. El valor de la \emph{razón de aprendizaje}, $\eta$
en la ecuación~\eqref{eq:gradiente}, utilizado en todos los experimentos fue de
$\eta=\num{1e-4}$.

\subsection{Arquitectura de la red neuronal}

\begin{figure}[t]
    \includegraphics[width=\textwidth]{figuras/capitulo-3/neural-network.pdf}
    \vspace{-1.5cm}
    \caption[Esquema genérico de una red neuronal.]{Esquema genérico de una red neuronal multicapa completamente conectada. Es importante notar la forma de la red, donde se tienen dos capas escondidas. Los círculos representan \emph{unidades} o \emph{nodos}, que están siendo evaluados por una función de activación. Los nodos que se ven separados de las capas (aquellos que están en la parte superior del esquema) son los \emph{sesgos} de las capas penúltima y última, respectivamente. La arquitectura usada en los resultados de este capítulo es diferente y más grande, pero tiene la misma estructura.}
    \label{fig:nn-esquema}
\end{figure}

La arquitectura de la red neuronal empleada en todos los experimentos es aquella que se
muestra en el esquema de la figura~\ref{fig:nn-esquema}, excepto por el número de unidades
en las capas ocultas.
Es decir, la arquitectura es de \emph{cuatro capas}, una capa de \emph{entrada} con una
unidad, dos capas ocultas con 1024 unidades cada una, y por último una capa de \emph{salida}
con una sola unidad.

La función de activación que se empleó fue la función \emph{ReLU}~\cite{glorotDeepSparseRectifier2011},
la cual está definida con la siguiente expresión

\begin{equation*}
    \text{ReLU}(x) = \text{max}{(0, x)} .
\end{equation*}

Esta función de activación se aplica a todas las capas a excepción de la capa de entrada.
Se escogió esta función de activación debido a que todas las más comunes ($\tanh, \text{softmax}$, etc.),
provocaban inconsistencias numéricas.

\subsection{Parámetros físicos}

% TODO: Poner la referencia a la ecuación del potencial
Para resolver la ecuación de OZ se empleó un radio de corte $r_c=7\sigma$, donde $\sigma$
es el diámetro de las partículas y tiene por valor $\sigma=1$.
El potencial empleado fue el de pseudo-esferas duras (Ref), tanto para la solución de la ecuación
de OZ como para los resultados obtenidos de simulación por computadora.

Se exploraron siete densidades diferentes en un rango de
$\phi \in [\num{0.15}, \num{0.45}]$, con $\Delta \phi = \num{0.05}$. Por cada valor
de la densidad se utilizó una partición de la densidad de 70 puntos para lograr la
convergencia del método iterativo cuando se resolvió la ecuación de OZ. En el caso de las
simulaciones por computadora, no se necesita esta partición.

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
\section{Resultados}

\begin{figure}[p]
    \begin{tabular}{cc}
        \includegraphics[width=0.47\textwidth]{figuras/capitulo-3/p=0.15.pdf} & 
        \includegraphics[width=0.47\textwidth]{figuras/capitulo-3/p=0.25.pdf} \\
        (a) & (b) \\[6pt]
        \includegraphics[width=0.47\textwidth]{figuras/capitulo-3/p=0.35.pdf} &   \includegraphics[width=0.47\textwidth]{figuras/capitulo-3/p=0.45.pdf} \\
        (c) & (d) \\[6pt]
    \end{tabular}
    \captionsetup{singlelinecheck=off}
    \caption[Funciones de distribución radial con redes neuronales.]{Funciones de distribución radial obtenidas con simulación por computadora y mediante el uso de la metodología descrita en este capítulo, usando redes neuronales. Se presentan cuatro densidades: 
    \begin{enumerate*}[label=(\alph*),itemjoin={,\enspace}]
        \item $\phi=\num{0.15}$
        \item $\phi=\num{0.25}$
        \item $\phi=\num{0.35}$
        \item $\phi=\num{0.45}$
    \end{enumerate*}
    . En cada una, se muestran las comparaciones con tres diferentes aproximaciones para la función puente: \emph{mV}, Verlet modificada; \emph{PY}, Percus-Yevick; y \emph{HNC}, Hypernetted Chain.
    }
    \label{fig:estructuras-neuronales}
\end{figure}

Con todos los elementos descritos en las secciones anteriores, los resultados obtenidos en 
esta sección se muestran en la figura~\ref{fig:estructuras-neuronales}, que corresponden
a las funciones de distribución radial, $g(r^*)$, de las densidades del líquido estudiadas.
Estas funciones describen la estructura del líquido, y son el tema central de estudio
de la metodología descrita en este capítulo.

\subsection{Densidades bajas}
Primero se analizarán y discutirán las densidades bajas $\phi=\numlist{0.15; 0.25}$,
las cuales corresponden a las gráficas (a) y (b) en la figura~\ref{fig:estructuras-neuronales}.
Los resultados que se observan es que, a densidades bajas, las aproximaciones HNC y
redes neuronales son más precisas que Percus-Yevick y Verlet modificada, sin embargo
ninguna de estas cuatro aproximaciones logra obtener resultados comparables a aquellos
de la simulación por computadora. En especial, es importante observar que
la aproximación de redes neuronales es un poco más precisa que la HNC, aunque sigue 
sobreestimando el pico principal que se encuentra en $r^* = \sigma$.
Por otro lado, las aproximaciones de Percus-Yevick y Verlet modificada subestiman el pico
principal para ambas densidades.

Tambíen es importante observar la forma funcional de $g(r^*)$. Para el caso de HNC y redes 
neuronales, parece ser que la forma es muy semejante entre ambas aproximaciones, e incluso 
pudiera ser la misma. Esto implica que, de alguna forma, los pesos de la red neuronal se 
ajustaron lo suficiente tal que la aproximación resulta ser HNC, o algo semejante.
Dicho de otra forma, los pesos son muy cercanos a cero tal que al ser
evaluada la red neuronal, la aproximación es muy semajante a HNC.
Otro factor importante a observar es que esta forma también es ligeramente diferente a
la encontrada con las simulaciones por computadora, pues las aproximaciones de
Percus-Yevick y Verlet modificada son más precisas en este aspecto.

\subsection{Densidades altas}
Ahora se discutirán los resultados para las densidades altas $\phi=\numlist{0.35; 0.45}$,
las cuales corresponden a las gráficas (c) y (d) en la figura~\ref{fig:estructuras-neuronales}.
De la misma forma que para las densidades bajas, las aproximaciones HNC y redes neuronales
para las densidades altas no son muy precisas. Ahora, y como era de esperarse, las 
aproximaciones Percus-Yevick y Verlet modificada son las más precisas, siendo Verlet
modificada la más precisa de todas. Esto se puede observar en la figura~\ref{fig:estructuras-neuronales}
al ver que el pico principal está muy bien estimado por la aproximación Verlet modificada,
mientras que las aproximaciones HNC y redes neuronales sobreestiman por mucho este valor.

Por otro lado, y semejante al caso para densidades bajas, la forma funcional de $g(r^*)$
es diferente entre la obtenida con simulación por computadora de aquella utilizando la
aproximación de redes neuronales. De hecho, al igual que antes, el resultado es muy
semejante al obtenido con la aproximación HNC. Esto es algo importante de observar, y
sustenta la hipótesis de que la aproximación de red neuronal se reduce a la aproximación
HNC.
Esto implica que la red neuronal en realidad está aproximando una función puente
$B(\vecr) \approx 0$.
Si se observan las aproximaciones de Percus-Yevick y Verlet modificada, volvemos a observar
que la aproximación de Verlet modificada es la más precisa, pues la forma funcional de
$g(r^*)$ es muy semejante a la obtenida con simulación por computadora.