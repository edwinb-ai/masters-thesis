\chapter{Computational Intelligence and Machine Learning}
\label{Cap3}

In this chapter, the fundamentals of Computational Intelligence and Machine Learning 
are developed. Particularly, the focus of the chapter is to present the main tools 
used in this thesis, namely \emph{neural networks} and \emph{evolutionary algorithms}. To 
reach a general understanding of these tools, a brief description of learning mechanisms
and numerical optimization is carried out.

\section{Computational Intelligence}
The first thing to address is the meaning and scope\emph{Computational 
Intelligence} (CI). With increasingly advancements in fundamental research in this area
and its sub-fields, there seems to be a blurry definition of what exactly is CI, and there 
is no concrete one until now. For this reason, in this work the definition of CI is an 
umbrella term for several other applications. However, these applications are related to 
each other for the same reason that CI exists: to provide a computational solution to a 
problem using as inspiration the paradigms of nature-inspired intelligence. For instance, 
following the handbook by Kacprzyk and 
Pedrycz~\cite{kacprzykSpringerHandbookComputational2015},
the definition of CI is to be a collection of nature-inspired computational methods that
provide solutions to problems where \emph{hard computing} is inefficient or it not even
suited to provide a solution to a given problem. Here, there is an important distinction
that should be carried throughout the remaining of the work: that there are problems for
which traditional tools are insufficient for the traditional problems. In this work, this
is the philosophy used to provide solutions: that the traditional forms of solutions might
seem hard and unfitting to provide solutions to the problems presented, and therefore
new way of approaching these solutions should be used.

In most cases, CI methods do not provide exact and accurate results, but this is expected.
It does not mean that CI is providing the ultimate, best solution to a problem. Rather, it 
is providing an approximate solution that can be used later with more robust algorithms and
methods. CI is not meant to be used as the sole method to solve a problem, but instead to
help find a simple solution to a difficult problem. That is why CI is considered an 
umbrella term, because it comprises diverse fields that can be used to find such an 
approximate solution. Indeed, in this work, two main field of CI are used:
\emph{neural networks}, which is a part of ML, a sub-field of CI; and
\emph{evolutionary algorithms}, which are stochastic optimization methods inspired by the 
evolution mechanisms found in nature.

\section{Machine Learning}
In modern times, data is constantly being created and used to model the reality around us.
However, traditional methods (hard computing) have not been enough to handle the large
data sets created. So new ways to dealing with this information are needed. Most 
importantly, the need for automated discovery and \emph{pattern recognition} within the
data. Follwing Murphy~\cite{murphyMachineLearningProbabilistic2012}, ML is defined
as the set of techniques that can automatically detect patterns in data, and use these
patterns to create new predictions, or to perform other kinds of decision making.
In other words, \emph{the data creates the ML algorithm}, not the other way around.
It is with data that ML methods work, however, these methods are essentially
\emph{function approximation} methods, which shall be discussed in a later section.
For now, a brief overview of the different kinds of ML tasks and problems will be presented,
focusing primarily on \emph{supervised learning}. Nonetheless, there exist other types
of learning, such as \emph{unsupervised learning}~\cite{goodfellowDeepLearning2016,hastieElementsStatisticalLearning2009} and \emph{reinforcement learning}~\cite{suttonReinforcementLearningSecond2018,kaelblingReinforcementLearningSurvey1996},
which are out of scope of this work, but remain an important part of modern ML theory.

\section{Supervised Learning}
The most common kind of ML methods is that of \emph{predictive} learning. For a set of
points $\mathcal{D}={ \{(\mathbf{x}_{i}, y_i)\} }_{i=1}^{N}$ with inputs $\mathbf{x}$
and outputs $y$, the goal of \emph{supervised learning} is to learn a map between
inputs and outputs. Here, $\mathcal{D}$ is the so-called \emph{training set} and $N$
is the number of \emph{training samples}.

In the most common cases, the inputs $\mathbf{x}$ are a set of $D$-dimensional vectors that
represent information about a given applications. For instance, the height and weight of a
person. These vectors are coloquially referred to as \emph{features} or \emph{attributes}.
The general form of $\mathbf{x}$ is not defined, it can be anything from an image, a time 
series, sentences from a text, graphs, molecules, and so on.

In a similar fashion, the \emph{response} variables $y$ can be in general anything. However,
there is a clear distinction given the forms that these variables can take. For instance,
if the variable $y$ are \emph{categorical}, the supervised learning task is considered
a \emph{classification} problem. Categorical values come from a finite set of possible 
values, $y_{i} \in \{1, \dots, C\}$, and might represent any type of discrete or nominal
value. For example, it might represent the colors in a clothing line, the gender between
people, and so on. On the other hand, if the values of $y$ are real-valued, such that 
$y_{i} \in \mathbb{R}$, then the learning task is dubbed a \emph{regression} problem.

\subsection{Classification}
In this section, the problem of classification is looked at with more detail, although 
classification is not really used in this thesis, it helps understand the importance of 
supervised learning, as well as a form of generalization to the problem of regression, 
which is the topic of the next section.

In the problem of classification, the computer algorithm is tasked with a data set $\mathcal
{D}={ \{(\mathbf{x}_{i}, y_i)\} }_{i=1}^{N}$, and is asked to specify to which category 
does the input belong to. Here, $\mathbf{x}_i$ is an $n$-dimensional feature vector. As 
mentioned before, the idea is for the algorithm to learn a map, or more precisely, a \emph
{function} such that $f \colon \mathbb{R}^n \mapsto \{1, \dots, k\}$, with $k$ the total 
number of categories, or \emph{classes}, to which the input can be assigned to. More 
specifically, when $y=f(\mathbf{x})$, the model assigns an input described by the 
$n$-dimensional feature vector $\mathbf{x}$ to a particular categorical value of $y$.

One of the most common uses of classification is object recognition. The goal of the object 
recognition problem is to decode a particular image with a specific object in it, and label 
it accordingly. An extremely popular and used data set for this kind of task is the MNIST 
handwritten digits data set~\cite{lecunGradientbasedLearningApplied1998a}, with variations 
such as the Fashion-MNIST~\cite{xiaoFashionMNISTNovelImage2017a}. These data sets are 
comprised of several images and categories, and the goal is to \emph{classify} each of the 
several categories. In the case of the handwritten digits, the goal is to specify which 
digits is represented in the image; in the case of the Fashion-MNIST data set, the goal is 
to specify the type of clothing. These data sets have become the standard benchmark data 
sets in the ML community for a long time; they are used primarily to test whether a ML 
algorithm is working properly, and if it is \emph{accurate} enough. The word 
\emph{accuracy} means something quite specific in ML theory, and will be discussed in a 
later section.

\subsection{Regression}
In \emph{regression}, the goal is for the computer algorithm to learn a map, or 
equivalently a function $f \colon \mathbb{R}^n \mapsto \mathbb{R}$, that predicts a 
numerical or real-valued output. The resemblance to the classification task is quite 
obvious: categories or classes can now be represented as a continuous value within the 
reals and there is no restriction for the number of classes. In a sense, this is the most 
general form of classification problem. So then, why make a distinction between the two 
problems? Most of the time, regression tasks are created to \emph{predict}, \emph{forecast}
, or even \emph{generate} outputs for a given data set $\mathcal{D}$.

One of the major applications of regression is to \emph{time series forecasting}. This can 
be found mostly in economical and financial contexts~\cite{bontempiMachineLearningStrategies2013,sezerFinancialTimeSeries2020}, but there are also 
applications in bioengineering and medical situations~\cite{mccoyAssessmentTimeSeriesMachine2018}. The idea here is that, more than regression, the 
goal is obtain a good approximation of the function $f$ in order to \emph{extrapolate} its 
domain to new results. It is expected that ML methods, with their ability to find unseen 
patterns, can effectively predict and forecast outputs that are not in the data set 
$\mathcal{D}$. This is an extremely hard task, but one that has been finding a lot of 
applications and many groups have done extensive research on the matter.

Regression is an important task, and in \autoref{Cap4} it is the primary learning mechanism 
used to solve a particular problem in Liquid State Theory. The problem of regression, as 
stated before, is to learn a \emph{good} approximation of the function $f$. Yet, the 
measure of \emph{good} is not clear enough, and shall be discussed in the next section.

\subsection{Performance Measure}
ML algorithms must be assessed on their abilities to perform a certain task $T$, either a 
classification or regression problem. It is common practice to extract a subset of the 
training data set $\mathcal{D}$ as the \emph{testing set}, $\mathcal{T}$, to evaluate the 
algorithm in the task $T$. The measure depends on the task $T$, as well as the type of data 
used.

In classification tasks, the \emph{accuracy} is one of the most general performance 
measures. The accuracy is simply defined as the proportion of data points in $\mathcal{T}$ 
for which the model produces the correct output. This measure is quite strict in the sense 
that if there are examples that are \emph{mislabeled}, this is carried on to the ML model.
Another common performance measure for classification tasks is the receiver operating 
characteristic, also known as the ROC~\cite{hastieElementsStatisticalLearning2009}. More 
specifically, the area under the ROC curve. This measure of performance is created by 
plotting the \emph{true positive rate} against the \emph{false positive rate}. This measure 
is used because these rates are more permissive, since they stem from the well established 
type I and II errors from statistical theory~\cite{riceMathematicalStatisticsData2006}. 
Depending on the data set, other measures can 
be used, such as the F1 score, the Jaccard index, Akaike information criterion, and 
others~\cite{murphyMachineLearningProbabilistic2012}.

Regression tasks are different when measuring performance since these methods attempt to 
approximate continuous, real-valued functions. So in a sense, one can simply use \emph
{metrics} in the mathematical analysis sense of the word. For instance, the use of the 
$L^2$ norm, also known as the so-called Euclidean norm, defined as
\begin{equation}
    { \left\lVert y - \hat{y} \right\rVert }_{2} = \sqrt{ \sum_{i=1}^{N} { \left(y_i - \hat{y}_i \right) }^2 }
    \; ,
    \label{eq:l2norm}
\end{equation}
is extremely common in regression tasks. Here, the training examples $y$ are measured 
against the output value from the ML model, $\hat{y}$. Another extremely common, and 
profoundly useful metric, is the $L^1$ norm,
\begin{equation}
    { \left\lVert y - \hat{y} \right\rVert }_{1} = \sum_{i=1}^{N} \left\lvert y_i - \hat{y}_i \right\rvert
    \; .
    \label{eq:l1norm}
\end{equation}
The $L^1$ has the amazing property that it can create \emph{sparse} representations of the 
learned function $f$. This is the basis of the LASSO method~\cite{hastieElementsStatisticalLearning2009}, a very useful and common ML method to do 
\emph{variable selection}, the discrimination of variables that are useful or not to the 
prediction of the model; and \emph{regularization}, which is adding information in order to 
solve a problem that might seem hard or impossible to solve, also known as an
\emph{ill-posed} problem~\cite{goodfellowDeepLearning2016}.
When generalizing these metrics to the full data set, they take in different names. For 
instance, the squared $L^2$ norm takes the name \emph{mean squared error}, defined as
\begin{equation}
    MSE \left( y_i, \hat{y}_i \right) = \frac{1}{N} \sum_{i=1}^{N} { \left(y_i - \hat{y}_i \right) }^2
    \; ,
    \label{eq:mse} 
\end{equation}
where $N$ is the total number of examples used to compute the MSE. Similarly, the $L^1$ 
norm can be generalized to the \emph{mean absolute error}, defined as
\begin{equation}
    MAE \left( y_i, \hat{y}_i \right) = \frac{1}{N} \sum_{i=1}^{N} \left\lvert y_i - \hat{y}_i \right\rvert
    \; .
    \label{eq:mae}
\end{equation}
These generalizations might seem trivial, but they allow for more general formulations, as 
they are in fact \emph{estimators} that measure the average of the outputs. When measure 
the performance of regression models, the goal is to \emph{minimize} these errors, with 
zero being the optimal value.

\subsection{Approximation Theory}

\section{Neural Networks}
\subsection{Definition}
\subsection{Universal Approximation Theorem}
\subsection{Training}

\section{Evolutionary Computation}
\subsection{Overview of Optimization}
\subsection{Black-box Optimization}
\subsection{Natural Evolution Strategies}