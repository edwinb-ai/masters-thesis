% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{anyt/global//global/global}
  \entry{arulkumaranAlphaStarEvolutionaryComputation2019}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=AK}{%
         family={Arulkumaran},
         familyi={A\bibinitperiod},
         given={Kai},
         giveni={K\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Cully},
         familyi={C\bibinitperiod},
         given={Antoine},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TJ}{%
         family={Togelius},
         familyi={T\bibinitperiod},
         given={Julian},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \keyw{co-evolution,lamarckian evolution,quality diversity}
    \strng{namehash}{AKCATJ1}
    \strng{fullhash}{AKCATJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{ACT19}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    In January 2019, DeepMind revealed AlphaStar to the world---the first
  artificial intelligence (AI) system to beat a professional player at the game
  of StarCraft II---representing a milestone in the progress of AI. AlphaStar
  draws on many areas of AI research, including deep learning, reinforcement
  learning, game theory, and evolutionary computation (EC). In this paper we
  analyze AlphaStar primarily through the lens of EC, presenting a new look at
  the system and relating it to many concepts in the field. We highlight some
  of its most interesting aspects---the use of Lamarckian evolution,
  competitive co-evolution, and quality diversity. In doing so, we hope to
  provide a bridge between the wider EC community and one of the most
  significant AI systems developed in recent times.%
    }
    \field{booktitle}{Proceedings of the Genetic and Evolutionary Computation
  Conference Companion}
    \verb{doi}
    \verb 10.1145/3319619.3321894
    \endverb
    \field{isbn}{978-1-4503-6748-6}
    \field{pages}{314\bibrangedash 315}
    \field{series}{GECCO '19}
    \field{shorttitle}{AlphaStar}
    \field{title}{AlphaStar: An Evolutionary Computation Perspective}
    \list{location}{1}{%
      {New York, NY, USA}%
    }
    \field{month}{07}
    \field{year}{2019}
  \endentry

  \entry{amariWhyNaturalGradient1998}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=AS}{%
         family={Amari},
         familyi={A\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=DS}{%
         family={Douglas},
         familyi={D\bibinitperiod},
         given={S.C.},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Adaptive filters,Cities and towns,Convergence,Cost
  function,Geometry,Information systems,Iterative methods,Newton
  method,Optimization methods,Parameter estimation}
    \strng{namehash}{ASDS1}
    \strng{fullhash}{ASDS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{AD98}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Gradient adaptation is a useful technique for adjusting a set of parameters
  to minimize a cost function. While often easy to implement, the convergence
  speed of gradient adaptation can be slow when the slope of the cost function
  varies widely for small changes in the parameters. In this paper, we outline
  an alternative technique, termed natural gradient adaptation, that overcomes
  the poor convergence properties of gradient adaptation in many cases. The
  natural gradient is based on differential geometry and employs knowledge of
  the Riemannian structure of the parameter space to adjust the gradient search
  direction. Unlike Newton's method, natural gradient adaptation does not
  assume a locally-quadratic cost function. Moreover, for maximum likelihood
  estimation tasks, natural gradient adaptation is asymptotically
  Fisher-efficient. A simple example illustrates the desirable properties of
  natural gradient adaptation.%
    }
    \field{booktitle}{Proceedings of the 1998 IEEE International Conference on
  Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}
    \verb{doi}
    \verb 10.1109/ICASSP.1998.675489
    \endverb
    \field{issn}{1520-6149}
    \field{pages}{1213\bibrangedash 1216 vol.2}
    \field{title}{Why Natural Gradient?}
    \field{volume}{2}
    \field{month}{05}
    \field{year}{1998}
  \endentry

  \entry{adamNoFreeLunch2019}{incollection}{}
    \name{author}{4}{}{%
      {{hash=ASP}{%
         family={Adam},
         familyi={A\bibinitperiod},
         given={Stavros\bibnamedelima P.},
         giveni={S\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=ASAN}{%
         family={Alexandropoulos},
         familyi={A\bibinitperiod},
         given={Stamatios-Aggelos\bibnamedelima N.},
         giveni={S\bibinithyphendelim A\bibinitperiod\bibinitdelim
  N\bibinitperiod},
      }}%
      {{hash=PPM}{%
         family={Pardalos},
         familyi={P\bibinitperiod},
         given={Panos\bibnamedelima M.},
         giveni={P\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=VMN}{%
         family={Vrahatis},
         familyi={V\bibinitperiod},
         given={Michael\bibnamedelima N.},
         giveni={M\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=DIC}{%
         family={Demetriou},
         familyi={D\bibinitperiod},
         given={Ioannis\bibnamedelima C.},
         giveni={I\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=PPM}{%
         family={Pardalos},
         familyi={P\bibinitperiod},
         given={Panos\bibnamedelima M.},
         giveni={P\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \strng{namehash}{ASP+1}
    \strng{fullhash}{ASPASANPPMVMN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Ada+19}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    The ``No Free Lunch'' theorem states that, averaged over all optimization
  problems, without re-sampling, all optimization algorithms perform equally
  well. Optimization, search, and supervised learning are the areas that have
  benefited more from this important theoretical concept. Formulation of the
  initial No Free Lunch theorem, very soon, gave rise to a number of research
  works which resulted in a suite of theorems that define an entire research
  field with significant results in other scientific areas where successfully
  exploring a search space is an essential and critical task. The objective of
  this paper is to go through the main research efforts that contributed to
  this research field, reveal the main issues, and disclose those points that
  are helpful in understanding the hypotheses, the restrictions, or even the
  inability of applying No Free Lunch theorems.%
    }
    \field{booktitle}{Approximation and Optimization : Algorithms, Complexity
  and Applications}
    \verb{doi}
    \verb 10.1007/978-3-030-12767-1_5
    \endverb
    \field{isbn}{978-3-030-12767-1}
    \field{pages}{57\bibrangedash 82}
    \field{series}{Springer Optimization and Its Applications}
    \field{shorttitle}{No Free Lunch Theorem}
    \field{title}{No Free Lunch Theorem: A Review}
    \field{langid}{english}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2019}
  \endentry

  \entry{a.goodallDatadrivenApproximationsBridge2021}{article}{}
    \name{author}{2}{}{%
      {{hash=AGRE}{%
         family={A.\bibnamedelima Goodall},
         familyi={A\bibinitperiod\bibinitdelim G\bibinitperiod},
         given={Rhys\bibnamedelima E.},
         giveni={R\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=ALA}{%
         family={A.\bibnamedelima Lee},
         familyi={A\bibinitperiod\bibinitdelim L\bibinitperiod},
         given={Alpha},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Royal Society of Chemistry}%
    }
    \strng{namehash}{AGREALA1}
    \strng{fullhash}{AGREALA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{AGAL21}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \verb{doi}
    \verb 10.1039/D1SM00402F
    \endverb
    \field{number}{21}
    \field{pages}{5393\bibrangedash 5400}
    \field{title}{Data-Driven Approximations to the Bridge Function Yield
  Improved Closures for the Ornstein\textendash Zernike Equation}
    \field{volume}{17}
    \field{langid}{english}
    \field{journaltitle}{Soft Matter}
    \field{year}{2021}
  \endentry

  \entry{agostinelliLearningActivationFunctions2015}{article}{}
    \name{author}{4}{}{%
      {{hash=AF}{%
         family={Agostinelli},
         familyi={A\bibinitperiod},
         given={Forest},
         giveni={F\bibinitperiod},
      }}%
      {{hash=HM}{%
         family={Hoffman},
         familyi={H\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SP}{%
         family={Sadowski},
         familyi={S\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=BP}{%
         family={Baldi},
         familyi={B\bibinitperiod},
         given={Pierre},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer
  Science - Machine Learning,Computer Science - Neural and Evolutionary
  Computing,Statistics - Machine Learning}
    \strng{namehash}{AF+1}
    \strng{fullhash}{AFHMSPBP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ago+15}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Artificial neural networks typically have a fixed, non-linear activation
  function at each neuron. We have designed a novel form of piecewise linear
  activation function that is learned independently for each neuron using
  gradient descent. With this adaptive activation function, we are able to
  improve upon deep neural network architectures composed of static rectified
  linear units, achieving state-of-the-art performance on CIFAR-10 (7.51\%),
  CIFAR-100 (30.83\%), and a benchmark from high-energy physics involving Higgs
  boson decay modes.%
    }
    \verb{eprint}
    \verb 1412.6830
    \endverb
    \field{title}{Learning Activation Functions to Improve Deep Neural
  Networks}
    \field{journaltitle}{arXiv:1412.6830 [cs, stat]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs, stat}
    \field{month}{04}
    \field{year}{2015}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{amariNaturalGradientWorks1998}{article}{}
    \name{author}{1}{}{%
      {{hash=ASi}{%
         family={Amari},
         familyi={A\bibinitperiod},
         given={Shun-ichi},
         giveni={S\bibinithyphendelim i\bibinitperiod},
      }}%
    }
    \strng{namehash}{ASi1}
    \strng{fullhash}{ASi1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ama98}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    When a parameter space has a certain underlying structure, the ordinary
  gradient of a function does not represent its steepest direction, but the
  natural gradient does. Information geometry is used for calculating the
  natural gradients in the parameter space of perceptrons, the space of
  matrices (for blind source separation), and the space of linear dynamical
  systems (for blind source deconvolution). The dynamical behavior of natural
  gradient online learning is analyzed and is proved to be Fisher efficient,
  implying that it has asymptotically the same performance as the optimal batch
  estimation of parameters. This suggests that the plateau phenomenon, which
  appears in the backpropagation learning algorithm of multilayer perceptrons,
  might disappear or might not be so serious when the natural gradient is used.
  An adaptive method of updating the learning rate is proposed and analyzed.%
    }
    \verb{doi}
    \verb 10.1162/089976698300017746
    \endverb
    \field{issn}{0899-7667}
    \field{number}{2}
    \field{pages}{251\bibrangedash 276}
    \field{title}{Natural Gradient Works Efficiently in Learning}
    \field{volume}{10}
    \field{journaltitle}{Neural Computation}
    \field{month}{02}
    \field{year}{1998}
  \endentry

  \entry{abdollahiVNetEndtoEndFully2020}{article}{}
    \name{author}{3}{}{%
      {{hash=AA}{%
         family={Abdollahi},
         familyi={A\bibinitperiod},
         given={Abolfazl},
         giveni={A\bibinitperiod},
      }}%
      {{hash=PB}{%
         family={Pradhan},
         familyi={P\bibinitperiod},
         given={Biswajeet},
         giveni={B\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Alamri},
         familyi={A\bibinitperiod},
         given={Abdullah},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{CEDL,Convolutional neural networks,Data mining,Feature
  extraction,Image segmentation,remote sensing,Remote sensing,road
  extraction,Roads,Training,VNet network}
    \strng{namehash}{AAPBAA1}
    \strng{fullhash}{AAPBAA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{APA20}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    One of the most important tasks in the advanced transportation systems is
  road extraction. Extracting road region from high-resolution remote sensing
  imagery is challenging due to complicated background such as buildings, trees
  shadows, pedestrians and vehicles and rural road networks that have
  heterogeneous forms with low interclass and high intraclass differences.
  Recently, deep learning-based techniques have presented a notable enhancement
  in the image segmentation results, however, most of them still cannot
  preserve boundary information and obtain high-resolution road segmentation
  map when processing the remote sensing imagery. In the present study, we
  introduce a new deep learning-based convolutional network called VNet model
  to produce a high-resolution road segmentation map. Moreover, a new dual loss
  function called cross-entropy-dice-loss (CEDL) is defined that synthesize
  cross-entropy (CE) and dice loss (DL) and consider both local information
  (CE) and global information (DL) to decrease the class imbalance influence
  and improve the road extraction results. The proposed VNet+CEDL model is
  implemented on two various road datasets called Massachusetts and Ottawa
  datasets. The suggested VNet+CEDL approach achieved an average F1 accuracy of
  90.64\% for Massachusetts dataset and 92.41\% for Ottawa dataset. When
  compared to other state-of-the-art deep learning-based frameworks like FCN,
  Segnet and Unet, the proposed approach could improve the results to 1.09\%,
  2.45\% and 0.39\%, for Massachusetts dataset and 7.21\%, 1.86\% and 2.68\%,
  for Ottawa dataset. Also, we compared the proposed method with the
  state-of-the-art road extraction techniques, and the results proved that the
  proposed technique outperformed other deep learning-based techniques in road
  extraction.%
    }
    \verb{doi}
    \verb 10.1109/ACCESS.2020.3026658
    \endverb
    \field{issn}{2169-3536}
    \field{pages}{179424\bibrangedash 179436}
    \field{shorttitle}{VNet}
    \field{title}{VNet: An End-to-End Fully Convolutional Neural Network for
  Road Extraction From High-Resolution Remote Sensing Data}
    \field{volume}{8}
    \field{journaltitle}{IEEE Access}
    \field{year}{2020}
  \endentry

  \entry{APSAPSMarcha}{inproceedings}{}
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Aps}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{booktitle}{Bulletin of the American Physical Society}
    \field{shorttitle}{APS -APS March Meeting 2020 - Event - Inverse Design of
  Soft Materials}
    \field{title}{APS -APS March Meeting 2020 - Event - Inverse Design of Soft
  Materials: Crystals, Quasi Crystals, Liquid Crystals}
    \field{volume}{Volume 65, Number 1}
  \endentry

  \entry{allenComputerSimulationLiquids2017}{book}{}
    \name{author}{2}{}{%
      {{hash=AMP}{%
         family={Allen},
         familyi={A\bibinitperiod},
         given={Michael\bibnamedelima P.},
         giveni={M\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=TDJ}{%
         family={Tildesley},
         familyi={T\bibinitperiod},
         given={Dominic\bibnamedelima J.},
         giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Oxford University Press}%
    }
    \keyw{Computers / Computer Simulation,Science / Mechanics / Fluids,Science
  / Physics / General}
    \strng{namehash}{AMPTDJ1}
    \strng{fullhash}{AMPTDJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{AT17}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    This book provides a practical guide to molecular dynamics and Monte Carlo
  simulation techniques used in the modelling of simple and complex liquids.
  Computer simulation is an essential tool in studying the chemistry and
  physics of condensed matter, complementing and reinforcing both experiment
  and theory. Simulations provide detailed information about structure and
  dynamics, essential to understand the many fluid systems that play a key role
  in our daily lives: polymers, gels, colloidal suspensions, liquid crystals,
  biological membranes, and glasses. The second edition of this pioneering book
  aims to explain how simulation programs work, how to use them, and how to
  interpret the results, with examples of the latest research in this rapidly
  evolving field. Accompanying programs in Fortran and Python provide
  practical, hands-on, illustrations of the ideas in the text.%
    }
    \field{isbn}{978-0-19-252470-6}
    \field{title}{Computer Simulation of Liquids}
    \field{langid}{english}
    \field{month}{08}
    \field{year}{2017}
  \endentry

  \entry{alderPhaseTransitionHard1957a}{article}{}
    \name{author}{2}{}{%
      {{hash=ABJ}{%
         family={Alder},
         familyi={A\bibinitperiod},
         given={B.\bibnamedelima J.},
         giveni={B\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=WTE}{%
         family={Wainwright},
         familyi={W\bibinitperiod},
         given={T.\bibnamedelima E.},
         giveni={T\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{ABJWTE1}
    \strng{fullhash}{ABJWTE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{AW57}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \verb{doi}
    \verb 10.1063/1.1743957
    \endverb
    \field{issn}{0021-9606}
    \field{number}{5}
    \field{pages}{1208\bibrangedash 1209}
    \field{title}{Phase Transition for a Hard Sphere System}
    \field{volume}{27}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{11}
    \field{year}{1957}
  \endentry

  \entry{baezUsingSecondVirial2018}{article}{}
    \name{author}{6}{}{%
      {{hash=BCA}{%
         family={B{\'a}ez},
         familyi={B\bibinitperiod},
         given={C{\'e}sar\bibnamedelima Alejandro},
         giveni={C\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=TA}{%
         family={{Torres-Carbajal}},
         familyi={T\bibinitperiod},
         given={Alexis},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={{Casta{\~n}eda-Priego}},
         familyi={C\bibinitperiod},
         given={Ram{\'o}n},
         giveni={R\bibinitperiod},
      }}%
      {{hash=VA}{%
         family={{Villada-Balbuena}},
         familyi={V\bibinitperiod},
         given={Alejandro},
         giveni={A\bibinitperiod},
      }}%
      {{hash=MJM}{%
         family={{M{\'e}ndez-Alcaraz}},
         familyi={M\bibinitperiod},
         given={Jos{\'e}\bibnamedelima Miguel},
         giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=HS}{%
         family={{Herrera-Velarde}},
         familyi={H\bibinitperiod},
         given={Salvador},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{BCA+1}
    \strng{fullhash}{BCATACRVAMJMHS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{B{\'a}e+18}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \verb{doi}
    \verb 10.1063/1.5049568
    \endverb
    \field{issn}{0021-9606}
    \field{number}{16}
    \field{pages}{164907}
    \field{title}{Using the Second Virial Coefficient as Physical Criterion to
  Map the Hard-Sphere Potential onto a Continuous Potential}
    \field{volume}{149}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{10}
    \field{year}{2018}
  \endentry

  \entry{baydinAutomaticDifferentiationMachine2018}{article}{}
    \name{author}{4}{}{%
      {{hash=BAG}{%
         family={Baydin},
         familyi={B\bibinitperiod},
         given={Atilim\bibnamedelima Gunes},
         giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=PBA}{%
         family={Pearlmutter},
         familyi={P\bibinitperiod},
         given={Barak\bibnamedelima A.},
         giveni={B\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=RAA}{%
         family={Radul},
         familyi={R\bibinitperiod},
         given={Alexey\bibnamedelima Andreyevich},
         giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=SJM}{%
         family={Siskind},
         familyi={S\bibinitperiod},
         given={Jeffrey\bibnamedelima Mark},
         giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \strng{namehash}{BAG+1}
    \strng{fullhash}{BAGPBARAASJM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Bay+18}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Derivatives, mostly in the form of gradients and Hessians, are ubiquitous
  in machine learning. Automatic differentiation (AD), also called algorithmic
  differentiation or simply \^aautodiff\^a, is a family of techniques
  similar to but more general than backpropagation for efficiently and
  accurately evaluating derivatives of numeric functions expressed as computer
  programs. AD is a small but established field with applications in areas
  including computational fluid dynamics, atmospheric sciences, and engineering
  design optimization. Until very recently, the fields of machine learning and
  AD have largely been unaware of each other and, in some cases, have
  independently discovered each other's results. Despite its relevance,
  general-purpose AD has been missing from the machine learning toolbox, a
  situation slowly changing with its ongoing adoption under the names
  \^adynamic computational graphs\^a and \^adifferentiable
  programming\^a. We survey the intersection of AD and machine learning,
  cover applications where AD has direct relevance, and address the main
  implementation techniques. By precisely defining the main differentiation
  techniques and their interrelationships, we aim to bring clarity to the usage
  of the terms \^aautodiff\^a, \^aautomatic differentiation\^a,
  and \^asymbolic differentiation\^a as these are encountered more and
  more in machine learning settings.%
    }
    \field{issn}{1533-7928}
    \field{number}{153}
    \field{pages}{1\bibrangedash 43}
    \field{shorttitle}{Automatic Differentiation in Machine Learning}
    \field{title}{Automatic Differentiation in Machine Learning: A Survey}
    \field{volume}{18}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2018}
  \endentry

  \entry{bontempiMachineLearningStrategies2013}{incollection}{}
    \name{author}{3}{}{%
      {{hash=BG}{%
         family={Bontempi},
         familyi={B\bibinitperiod},
         given={Gianluca},
         giveni={G\bibinitperiod},
      }}%
      {{hash=BTS}{%
         family={Ben\bibnamedelima Taieb},
         familyi={B\bibinitperiod\bibinitdelim T\bibinitperiod},
         given={Souhaib},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LBYA}{%
         family={Le\bibnamedelima Borgne},
         familyi={L\bibinitperiod\bibinitdelim B\bibinitperiod},
         given={Yann-A{\"e}l},
         giveni={Y\bibinithyphendelim A\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=AMA}{%
         family={Aufaure},
         familyi={A\bibinitperiod},
         given={Marie-Aude},
         giveni={M\bibinithyphendelim A\bibinitperiod},
      }}%
      {{hash=ZE}{%
         family={Zim{\'a}nyi},
         familyi={Z\bibinitperiod},
         given={Esteban},
         giveni={E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \keyw{lazy learning,local learning,machine learning,MIMO,Time series
  forecasting}
    \strng{namehash}{BGBTSLBYA1}
    \strng{fullhash}{BGBTSLBYA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BBTLB13}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    The increasing availability of large amounts of historical data and the
  need of performing accurate forecasting of future behavior in several
  scientific and applied domains demands the definition of robust and efficient
  techniques able to infer from observations the stochastic dependency between
  past and future. The forecasting domain has been influenced, from the 1960s
  on, by linear statistical methods such as ARIMA models. More recently,
  machine learning models have drawn attention and have established themselves
  as serious contenders to classical statistical models in the forecasting
  community. This chapter presents an overview of machine learning techniques
  in time series forecasting by focusing on three aspects: the formalization of
  one-step forecasting problems as supervised learning tasks, the discussion of
  local learning techniques as an effective tool for dealing with temporal data
  and the role of the forecasting strategy when we move from one-step to
  multiple-step forecasting.%
    }
    \field{booktitle}{Business Intelligence: Second European Summer School,
  EBISS 2012, Brussels, Belgium, July 15-21, 2012, Tutorial Lectures}
    \verb{doi}
    \verb 10.1007/978-3-642-36318-4_3
    \endverb
    \field{isbn}{978-3-642-36318-4}
    \field{pages}{62\bibrangedash 77}
    \field{series}{Lecture Notes in Business Information Processing}
    \field{title}{Machine Learning Strategies for Time Series Forecasting}
    \field{langid}{english}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{2013}
  \endentry

  \entry{boattiniUnsupervisedLearningLocal2019a}{article}{}
    \name{author}{3}{}{%
      {{hash=BE}{%
         family={Boattini},
         familyi={B\bibinitperiod},
         given={Emanuele},
         giveni={E\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
      {{hash=FL}{%
         family={Filion},
         familyi={F\bibinitperiod},
         given={Laura},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{BEDMFL1}
    \strng{fullhash}{BEDMFL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BDF19}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \verb{doi}
    \verb 10.1063/1.5118867
    \endverb
    \field{issn}{0021-9606}
    \field{number}{15}
    \field{pages}{154901}
    \field{title}{Unsupervised Learning for Local Structure Detection in
  Colloidal Systems}
    \field{volume}{151}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{10}
    \field{year}{2019}
  \endentry

  \entry{bealsAnalysisIntroduction2004}{book}{}
    \name{author}{1}{}{%
      {{hash=BR}{%
         family={Beals},
         familyi={B\bibinitperiod},
         given={Richard},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cambridge University Press}%
    }
    \keyw{Mathematics / Complex Analysis,Mathematics / Functional
  Analysis,Mathematics / Logic,Mathematics / Mathematical Analysis}
    \strng{namehash}{BR1}
    \strng{fullhash}{BR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Bea04}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    This self-contained text, suitable for advanced undergraduates, provides an
  extensive introduction to mathematical analysis, from the fundamentals to
  more advanced material. It begins with the properties of the real numbers and
  continues with a rigorous treatment of sequences, series, metric spaces, and
  calculus in one variable. Further subjects include Lebesgue measure and
  integration on the line, Fourier analysis, and differential equations. In
  addition to this core material, the book includes a number of interesting
  applications of the subject matter to areas both within and outside the field
  of mathematics. The aim throughout is to strike a balance between being too
  austere or too sketchy, and being so detailed as to obscure the essential
  ideas. A large number of examples and 500 exercises allow the reader to test
  understanding, practise mathematical exposition and provide a window into
  further topics.%
    }
    \field{isbn}{978-0-521-60047-7}
    \field{shorttitle}{Analysis}
    \field{title}{Analysis: An Introduction}
    \field{langid}{english}
    \field{month}{09}
    \field{year}{2004}
  \endentry

  \entry{behlerPerspectiveMachineLearning2016a}{article}{}
    \name{author}{1}{}{%
      {{hash=BJ}{%
         family={Behler},
         familyi={B\bibinitperiod},
         given={J{\"o}rg},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{BJ1}
    \strng{fullhash}{BJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Beh16}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Nowadays, computer simulations have become a standard tool in essentially
  all fields of chemistry, condensed matter physics, and materials science. In
  order to keep up with state-of-the-art experiments and the ever growing
  complexity of the investigated problems, there is a constantly increasing
  need for simulations of more realistic, i.e., larger, model systems with
  improved accuracy. In many cases, the availability of sufficiently efficient
  interatomic potentials providing reliable energies and forces has become a
  serious bottleneck for performing these simulations. To address this problem,
  currently a paradigm change is taking place in the development of interatomic
  potentials. Since the early days of computer simulations simplified
  potentials have been derived using physical approximations whenever the
  direct application of electronic structure methods has been too demanding.
  Recent advances in machine learning (ML) now offer an alternative approach
  for the representation of potential-energy surfaces by fitting large data
  sets from electronic structure calculations. In this perspective, the central
  ideas underlying these ML potentials, solved problems and remaining
  challenges are reviewed along with a discussion of their current
  applicability and limitations.%
    }
    \verb{doi}
    \verb 10.1063/1.4966192
    \endverb
    \field{issn}{0021-9606}
    \field{number}{17}
    \field{pages}{170901}
    \field{shorttitle}{Perspective}
    \field{title}{Perspective: Machine Learning Potentials for Atomistic
  Simulations}
    \field{volume}{145}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{11}
    \field{year}{2016}
  \endentry

  \entry{bernerModernMathematicsDeep2021}{article}{}
    \name{author}{4}{}{%
      {{hash=BJ}{%
         family={Berner},
         familyi={B\bibinitperiod},
         given={Julius},
         giveni={J\bibinitperiod},
      }}%
      {{hash=GP}{%
         family={Grohs},
         familyi={G\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
      {{hash=KG}{%
         family={Kutyniok},
         familyi={K\bibinitperiod},
         given={Gitta},
         giveni={G\bibinitperiod},
      }}%
      {{hash=PP}{%
         family={Petersen},
         familyi={P\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \strng{namehash}{BJ+1}
    \strng{fullhash}{BJGPKGPP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ber+21}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    We describe the new field of mathematical analysis of deep learning. This
  field emerged around a list of research questions that were not answered
  within the classical framework of learning theory. These questions concern:
  the outstanding generalization power of overparametrized neural networks, the
  role of depth in deep architectures, the apparent absence of the curse of
  dimensionality, the surprisingly successful optimization performance despite
  the non-convexity of the problem, understanding what features are learned,
  why deep architectures perform exceptionally well in physical problems, and
  which fine aspects of an architecture affect the behavior of a learning task
  in which way. We present an overview of modern approaches that yield partial
  answers to these questions. For selected approaches, we describe the main
  ideas in more detail.%
    }
    \verb{eprint}
    \verb 2105.04026
    \endverb
    \field{title}{The Modern Mathematics of Deep Learning}
    \field{journaltitle}{arXiv:2105.04026 [cs, stat]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs, stat}
    \field{month}{05}
    \field{year}{2021}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{bianchiPredictingPatchyParticle2012}{article}{}
    \name{author}{5}{}{%
      {{hash=BE}{%
         family={Bianchi},
         familyi={B\bibinitperiod},
         given={Emanuela},
         giveni={E\bibinitperiod},
      }}%
      {{hash=DG}{%
         family={Doppelbauer},
         familyi={D\bibinitperiod},
         given={G{\"u}nther},
         giveni={G\bibinitperiod},
      }}%
      {{hash=FL}{%
         family={Filion},
         familyi={F\bibinitperiod},
         given={Laura},
         giveni={L\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
      {{hash=KG}{%
         family={Kahl},
         familyi={K\bibinitperiod},
         given={Gerhard},
         giveni={G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{BE+1}
    \strng{fullhash}{BEDGFLDMKG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Bia+12}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \verb{doi}
    \verb 10.1063/1.4722477
    \endverb
    \field{issn}{0021-9606}
    \field{number}{21}
    \field{pages}{214102}
    \field{shorttitle}{Predicting Patchy Particle Crystals}
    \field{title}{Predicting Patchy Particle Crystals: Variable Box Shape
  Simulations and Evolutionary Algorithms}
    \field{volume}{136}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{06}
    \field{year}{2012}
  \endentry

  \entry{biswasTanhSoftDynamicTrainable2021}{article}{}
    \name{author}{4}{}{%
      {{hash=BK}{%
         family={Biswas},
         familyi={B\bibinitperiod},
         given={Koushik},
         giveni={K\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={Kumar},
         familyi={K\bibinitperiod},
         given={Sandeep},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BS}{%
         family={Banerjee},
         familyi={B\bibinitperiod},
         given={Shilpak},
         giveni={S\bibinitperiod},
      }}%
      {{hash=PAK}{%
         family={Pandey},
         familyi={P\bibinitperiod},
         given={Ashish\bibnamedelima Kumar},
         giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
    }
    \keyw{activation function,Biological neural networks,Deep
  learning,Licenses,neural network,Neurons,Object detection,Task
  analysis,Training}
    \strng{namehash}{BK+1}
    \strng{fullhash}{BKKSBSPAK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Bis+21}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Deep learning, at its core, contains functions that are the composition of
  a linear transformation with a nonlinear function known as the activation
  function. In the past few years, there is an increasing interest in the
  construction of novel activation functions resulting in better learning. In
  this work, we propose three novel activation functions with learnable
  parameters, namely TanhSoft-1, TanhSoft-2, and TanhSoft-3, which are shown to
  outperform several well-known activation functions. For instance, replacing
  ReLU with TanhSoft-1, TanhSoft-2, and Tanhsot-3 improves top-1 classification
  accuracy by 6.06\%, 5.75\%, and 5.38\% respectively on VGG-16(with
  batch-normalization), by 3.02\%, 3.25\% and 2.93\% respectively on
  PreActResNet-34 in CIFAR-100 dataset, by 1.76\%, 1.93\%, and 1.82\%
  respectively on WideResNet 28-10 in Tiny ImageNet dataset. TanhSoft-1,
  TanhSoft-2, and Tanhsot-3 outperformed ReLU on mean average precision (mAP)
  by 0.7\%, 0.8\%, and 0.6\% respectively in object detection problem on SSD
  300 model in Pascal VOC dataset.%
    }
    \verb{doi}
    \verb 10.1109/ACCESS.2021.3105355
    \endverb
    \field{issn}{2169-3536}
    \field{pages}{120613\bibrangedash 120623}
    \field{title}{TanhSoft\textemdash Dynamic Trainable Activation Functions
  for Faster Learning and Better Performance}
    \field{volume}{9}
    \field{journaltitle}{IEEE Access}
    \field{year}{2021}
  \endentry

  \entry{boattiniModelingManybodyInteractions2020}{article}{}
    \name{author}{5}{}{%
      {{hash=BE}{%
         family={Boattini},
         familyi={B\bibinitperiod},
         given={Emanuele},
         giveni={E\bibinitperiod},
      }}%
      {{hash=BN}{%
         family={Bezem},
         familyi={B\bibinitperiod},
         given={Nina},
         giveni={N\bibinitperiod},
      }}%
      {{hash=PSN}{%
         family={Punnathanam},
         familyi={P\bibinitperiod},
         given={Sudeep\bibnamedelima N.},
         giveni={S\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=SF}{%
         family={Smallenburg},
         familyi={S\bibinitperiod},
         given={Frank},
         giveni={F\bibinitperiod},
      }}%
      {{hash=FL}{%
         family={Filion},
         familyi={F\bibinitperiod},
         given={Laura},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{BE+2}
    \strng{fullhash}{BEBNPSNSFFL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Boa+20}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \verb{doi}
    \verb 10.1063/5.0015606
    \endverb
    \field{issn}{0021-9606}
    \field{number}{6}
    \field{pages}{064902}
    \field{title}{Modeling of Many-Body Interactions between Elastic Spheres
  through Symmetry Functions}
    \field{volume}{153}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{08}
    \field{year}{2020}
  \endentry

  \entry{behlerGeneralizedNeuralNetworkRepresentation2007a}{article}{}
    \name{author}{2}{}{%
      {{hash=BJ}{%
         family={Behler},
         familyi={B\bibinitperiod},
         given={J{\"o}rg},
         giveni={J\bibinitperiod},
      }}%
      {{hash=PM}{%
         family={Parrinello},
         familyi={P\bibinitperiod},
         given={Michele},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{BJPM1}
    \strng{fullhash}{BJPM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BP07}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    The accurate description of chemical processes often requires the use of
  computationally demanding methods like density-functional theory (DFT),
  making long simulations of large systems unfeasible. In this Letter we
  introduce a new kind of neural-network representation of DFT potential-energy
  surfaces, which provides the energy and forces as a function of all atomic
  positions in systems of arbitrary size and is several orders of magnitude
  faster than DFT. The high accuracy of the method is demonstrated for bulk
  silicon and compared with empirical potentials and DFT. The method is general
  and can be applied to all types of periodic and nonperiodic systems.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevLett.98.146401
    \endverb
    \field{number}{14}
    \field{pages}{146401}
    \field{title}{Generalized Neural-Network Representation of High-Dimensional
  Potential-Energy Surfaces}
    \field{volume}{98}
    \field{journaltitle}{Physical Review Letters}
    \field{month}{04}
    \field{year}{2007}
  \endentry

  \entry{bedollaMachineLearningCondensed2020}{article}{}
    \name{author}{3}{}{%
      {{hash=BE}{%
         family={Bedolla},
         familyi={B\bibinitperiod},
         given={Edwin},
         giveni={E\bibinitperiod},
      }}%
      {{hash=PLC}{%
         family={Padierna},
         familyi={P\bibinitperiod},
         given={Luis\bibnamedelima Carlos},
         giveni={L\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={{Casta{\~n}eda-Priego}},
         familyi={C\bibinitperiod},
         given={Ram{\'o}n},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IOP Publishing}%
    }
    \strng{namehash}{BEPLCCR1}
    \strng{fullhash}{BEPLCCR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BPC20}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Condensed matter physics (CMP) seeks to understand the microscopic
  interactions of matter at the quantum and atomistic levels, and describes how
  these interactions result in both mesoscopic and macroscopic properties. CMP
  overlaps with many other important branches of science, such as chemistry,
  materials science, statistical physics, and high-performance computing. With
  the advancements in modern machine learning (ML) technology, a keen interest
  in applying these algorithms to further CMP research has created a compelling
  new area of research at the intersection of both fields. In this review, we
  aim to explore the main areas within CMP, which have successfully applied ML
  techniques to further research, such as the description and use of ML schemes
  for potential energy surfaces, the characterization of topological phases of
  matter in lattice systems, the prediction of phase transitions in off-lattice
  and atomistic simulations, the interpretation of ML theories with
  physics-inspired frameworks and the enhancement of simulation methods with ML
  algorithms. We also discuss in detail the main challenges and drawbacks of
  using ML methods on CMP problems, as well as some perspectives for future
  developments.%
    }
    \verb{doi}
    \verb 10.1088/1361-648X/abb895
    \endverb
    \field{issn}{0953-8984}
    \field{number}{5}
    \field{pages}{053001}
    \field{title}{Machine Learning for Condensed Matter Physics}
    \field{volume}{33}
    \field{langid}{english}
    \field{journaltitle}{Journal of Physics: Condensed Matter}
    \field{month}{11}
    \field{year}{2020}
  \endentry

  \entry{boattiniAveragingLocalStructure2021}{article}{}
    \name{author}{3}{}{%
      {{hash=BE}{%
         family={Boattini},
         familyi={B\bibinitperiod},
         given={Emanuele},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SF}{%
         family={Smallenburg},
         familyi={S\bibinitperiod},
         given={Frank},
         giveni={F\bibinitperiod},
      }}%
      {{hash=FL}{%
         family={Filion},
         familyi={F\bibinitperiod},
         given={Laura},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{BESFFL1}
    \strng{fullhash}{BESFFL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BSF21}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Predicting the local dynamics of supercooled liquids based purely on local
  structure is a key challenge in our quest for understanding glassy materials.
  Recent years have seen an explosion of methods for making such a prediction,
  often via the application of increasingly complex machine learning
  techniques. The best predictions so far have involved so-called Graph Neural
  Networks (GNNs) whose accuracy comes at a cost of models that involve on the
  order of 105 fit parameters. In this Letter, we propose that the key
  structural ingredient to the GNN method is its ability to consider not only
  the local structure around a central particle, but also averaged structural
  features centered around nearby particles. We demonstrate that this insight
  can be exploited to design a significantly more efficient model that provides
  essentially the same predictive power at a fraction of the computational
  complexity (approximately 1000 fit parameters), and demonstrate its success
  by fitting the dynamic propensity of Kob-Andersen and binary hard-sphere
  mixtures. We then use this to make predictions regarding the importance of
  radial and angular descriptors in the dynamics of both models.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevLett.127.088007
    \endverb
    \field{number}{8}
    \field{pages}{088007}
    \field{title}{Averaging Local Structure to Predict the Dynamic Propensity
  in Supercooled Liquids}
    \field{volume}{127}
    \field{journaltitle}{Physical Review Letters}
    \field{month}{08}
    \field{year}{2021}
  \endentry

  \entry{byrdTrustRegionAlgorithm1987}{article}{}
    \name{author}{3}{}{%
      {{hash=BRH}{%
         family={Byrd},
         familyi={B\bibinitperiod},
         given={Richard\bibnamedelima H.},
         giveni={R\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=SRB}{%
         family={Schnabel},
         familyi={S\bibinitperiod},
         given={Robert\bibnamedelima B.},
         giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=SGA}{%
         family={Shultz},
         familyi={S\bibinitperiod},
         given={Gerald\bibnamedelima A.},
         giveni={G\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{2}{%
      {Society for Industrial}%
      {Applied Mathematics}%
    }
    \keyw{49D37,65K05,constrained optimization,trust region}
    \strng{namehash}{BRHSRBSGA1}
    \strng{fullhash}{BRHSRBSGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{BSS87}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    We present a trust region-based method for the general nonlinearly equality
  constrained optimization problem. The method works by iteratively minimizing
  a quadratic model of the Lagrangian subject to a possibly relaxed
  linearization of the problem constraints and a trust region constraint. The
  model minimization may be done approximately with a dogleg-type approach. We
  show that this method is globally convergent even if singular or indefinite
  Hessian approximations are made. A second order correction step that brings
  the iterates closer to the feasible set is described. If sufficiently precise
  Hessian information is used, this correction step allows us to prove that the
  method is also locally quadratically convergent, and that the limit satisfies
  the second order necessary conditions for constrained optimization. An
  example is given to show that, without this correction, a situation similar
  to the Maratos effect may occur where the iteration is unable to move away
  from a saddle point.%
    }
    \verb{doi}
    \verb 10.1137/0724076
    \endverb
    \field{issn}{0036-1429}
    \field{number}{5}
    \field{pages}{1152\bibrangedash 1170}
    \field{title}{A Trust Region Algorithm for Nonlinearly Constrained
  Optimization}
    \field{volume}{24}
    \field{journaltitle}{SIAM Journal on Numerical Analysis}
    \field{month}{10}
    \field{year}{1987}
  \endentry

  \entry{carleoMachineLearningPhysical2019a}{article}{}
    \name{author}{8}{}{%
      {{hash=CG}{%
         family={Carleo},
         familyi={C\bibinitperiod},
         given={Giuseppe},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CI}{%
         family={Cirac},
         familyi={C\bibinitperiod},
         given={Ignacio},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CK}{%
         family={Cranmer},
         familyi={C\bibinitperiod},
         given={Kyle},
         giveni={K\bibinitperiod},
      }}%
      {{hash=DL}{%
         family={Daudet},
         familyi={D\bibinitperiod},
         given={Laurent},
         giveni={L\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Schuld},
         familyi={S\bibinitperiod},
         given={Maria},
         giveni={M\bibinitperiod},
      }}%
      {{hash=TN}{%
         family={Tishby},
         familyi={T\bibinitperiod},
         given={Naftali},
         giveni={N\bibinitperiod},
      }}%
      {{hash=VL}{%
         family={{Vogt-Maranto}},
         familyi={V\bibinitperiod},
         given={Leslie},
         giveni={L\bibinitperiod},
      }}%
      {{hash=ZL}{%
         family={Zdeborov{\'a}},
         familyi={Z\bibinitperiod},
         given={Lenka},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{CG+1}
    \strng{fullhash}{CGCICKDLSMTNVLZL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Car+19}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Machine learning (ML) encompasses a broad range of algorithms and modeling
  tools used for a vast array of data processing tasks, which has entered most
  scientific disciplines in recent years. This article reviews in a selective
  way the recent research on the interface between machine learning and the
  physical sciences. This includes conceptual developments in ML motivated by
  physical insights, applications of machine learning techniques to several
  domains in physics, and cross fertilization between the two fields. After
  giving a basic notion of machine learning methods and principles, examples
  are described of how statistical physics is used to understand methods in ML.
  This review then describes applications of ML methods in particle physics and
  cosmology, quantum many-body physics, quantum computing, and chemical and
  material physics. Research and development into novel computing architectures
  aimed at accelerating ML are also highlighted. Each of the sections describe
  recent successes as well as domain-specific methodology and challenges.%
    }
    \verb{doi}
    \verb 10.1103/RevModPhys.91.045002
    \endverb
    \field{number}{4}
    \field{pages}{045002}
    \field{title}{Machine Learning and the Physical Sciences}
    \field{volume}{91}
    \field{journaltitle}{Reviews of Modern Physics}
    \field{month}{12}
    \field{year}{2019}
  \endentry

  \entry{chenUniversalApproximationNonlinear1995}{article}{}
    \name{author}{2}{}{%
      {{hash=CT}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Tianping},
         giveni={T\bibinitperiod},
      }}%
      {{hash=CH}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Hong},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{Computer networks,H infinity control,Integral
  equations,Kernel,Mathematics,Neural networks,Nonlinear dynamical
  systems,Polynomials,Sufficient conditions,Sun}
    \strng{namehash}{CTCH1}
    \strng{fullhash}{CTCH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{CC95}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    The purpose of this paper is to investigate neural network capability
  systematically. The main results are: 1) every Tauber-Wiener function is
  qualified as an activation function in the hidden layer of a three-layered
  neural network; 2) for a continuous function in S'(R/sup 1/) to be a
  Tauber-Wiener function, the necessary and sufficient condition is that it is
  not a polynomial; 3) the capability of approximating nonlinear functionals
  defined on some compact set of a Banach space and nonlinear operators has
  been shown; and 4) the possibility by neural computation to approximate the
  output as a whole (not at a fixed point) of a dynamical system, thus
  identifying the system.{$<>$}%
    }
    \verb{doi}
    \verb 10.1109/72.392253
    \endverb
    \field{issn}{1941-0093}
    \field{number}{4}
    \field{pages}{911\bibrangedash 917}
    \field{title}{Universal Approximation to Nonlinear Operators by Neural
  Networks with Arbitrary Activation Functions and Its Application to Dynamical
  Systems}
    \field{volume}{6}
    \field{journaltitle}{IEEE Transactions on Neural Networks}
    \field{month}{07}
    \field{year}{1995}
  \endentry

  \entry{carrasquillaMachineLearningPhases2017a}{article}{}
    \name{author}{2}{}{%
      {{hash=CJ}{%
         family={Carrasquilla},
         familyi={C\bibinitperiod},
         given={Juan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MRG}{%
         family={Melko},
         familyi={M\bibinitperiod},
         given={Roger\bibnamedelima G.},
         giveni={R\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{CJMRG1}
    \strng{fullhash}{CJMRG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{CM17}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    The success of machine learning techniques in handling big data sets proves
  ideal for classifying condensed-matter phases and phase transitions. The
  technique is even amenable to detecting non-trivial states lacking in
  conventional order.%
    }
    \verb{doi}
    \verb 10.1038/nphys4035
    \endverb
    \field{issn}{1745-2481}
    \field{number}{5}
    \field{pages}{431\bibrangedash 434}
    \field{title}{Machine Learning Phases of Matter}
    \field{volume}{13}
    \field{langid}{english}
    \field{journaltitle}{Nature Physics}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Research Subject\_term: Phase transitions and critical phenomena;Statistical
  physics Subject\_term\_id:
  phase-transitions-and-critical-phenomena;statistical-physics%
    }
    \field{month}{05}
    \field{year}{2017}
  \endentry

  \entry{cohenLearningCurvesOverparametrized2021}{article}{}
    \name{author}{3}{}{%
      {{hash=CO}{%
         family={Cohen},
         familyi={C\bibinitperiod},
         given={Omry},
         giveni={O\bibinitperiod},
      }}%
      {{hash=MO}{%
         family={Malka},
         familyi={M\bibinitperiod},
         given={Or},
         giveni={O\bibinitperiod},
      }}%
      {{hash=RZ}{%
         family={Ringel},
         familyi={R\bibinitperiod},
         given={Zohar},
         giveni={Z\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{COMORZ1}
    \strng{fullhash}{COMORZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{CMR21}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    In the past decade, deep neural networks (DNNs) came to the fore as the
  leading machine-learning algorithms for a variety of tasks. Their rise was
  founded on market needs and engineering craftsmanship, the latter based more
  on trial and error than on theory. While still far behind the application
  forefront, the theoretical study of DNNs has recently made important
  advancements in analyzing the highly overparametrized regime where some exact
  results have been obtained. Leveraging these ideas and adopting a more
  physicslike approach, here we construct a versatile field theory formalism
  for supervised deep learning, involving renormalization group, Feynman
  diagrams, and replicas. In particular, we show that our approach leads to
  highly accurate predictions of learning curves of truly deep DNNs trained on
  polynomial regression problems. It also explains in a concrete manner why
  DNNs generalize well despite being highly overparametrized, this due to an
  entropic bias to simple functions which, for the case of fully connected DNNs
  with data sampled on the hypersphere, are low-order polynomials in the input
  vector. Being a complex interacting system of artificial neurons, we believe
  that such tools and methodologies borrowed from condensed matter physics
  would prove essential for obtaining an accurate quantitative understanding of
  deep learning.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevResearch.3.023034
    \endverb
    \field{number}{2}
    \field{pages}{023034}
    \field{shorttitle}{Learning Curves for Overparametrized Deep Neural
  Networks}
    \field{title}{Learning Curves for Overparametrized Deep Neural Networks: A
  Field Theory Perspective}
    \field{volume}{3}
    \field{journaltitle}{Physical Review Research}
    \field{month}{04}
    \field{year}{2021}
  \endentry

  \entry{cybenkoApproximationSuperpositionsSigmoidal1989}{article}{}
    \name{author}{1}{}{%
      {{hash=CG}{%
         family={Cybenko},
         familyi={C\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
    }
    \strng{namehash}{CG1}
    \strng{fullhash}{CG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Cyb89}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    In this paper we demonstrate that finite linear combinations of
  compositions of a fixed, univariate function and a set of affine functionals
  can uniformly approximate any continuous function ofn real variables with
  support in the unit hypercube; only mild conditions are imposed on the
  univariate function. Our results settle an open question about
  representability in the class of single hidden layer neural networks. In
  particular, we show that arbitrary decision regions can be arbitrarily well
  approximated by continuous feedforward neural networks with only a single
  internal, hidden layer and any continuous sigmoidal nonlinearity. The paper
  discusses approximation properties of other possible types of nonlinearities
  that might be implemented by artificial neural networks.%
    }
    \verb{doi}
    \verb 10.1007/BF02551274
    \endverb
    \field{issn}{1435-568X}
    \field{number}{4}
    \field{pages}{303\bibrangedash 314}
    \field{title}{Approximation by Superpositions of a Sigmoidal Function}
    \field{volume}{2}
    \field{langid}{english}
    \field{journaltitle}{Mathematics of Control, Signals and Systems}
    \field{month}{12}
    \field{year}{1989}
  \endentry

  \entry{dunjkoMachineLearningArtificial2018}{article}{}
    \name{author}{2}{}{%
      {{hash=DV}{%
         family={Dunjko},
         familyi={D\bibinitperiod},
         given={Vedran},
         giveni={V\bibinitperiod},
      }}%
      {{hash=BHJ}{%
         family={Briegel},
         familyi={B\bibinitperiod},
         given={Hans\bibnamedelima J.},
         giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IOP Publishing}%
    }
    \strng{namehash}{DVBHJ1}
    \strng{fullhash}{DVBHJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{DB18}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    Quantum information technologies, on the one hand, and intelligent learning
  systems, on the other, are both emergent technologies that are likely to have
  a transformative impact on our society in the future. The respective
  underlying fields of basic research\textemdash quantum information versus
  machine learning (ML) and artificial intelligence (AI)\textemdash have their
  own specific questions and challenges, which have hitherto been investigated
  largely independently. However, in a growing body of recent work, researchers
  have been probing the question of the extent to which these fields can indeed
  learn and benefit from each other. Quantum ML explores the interaction
  between quantum computing and ML, investigating how results and techniques
  from one field can be used to solve the problems of the other. Recently we
  have witnessed significant breakthroughs in both directions of influence. For
  instance, quantum computing is finding a vital application in providing
  speed-ups for ML problems, critical in our `big data' world. Conversely, ML
  already permeates many cutting-edge technologies and may become instrumental
  in advanced quantum technologies. Aside from quantum speed-up in data
  analysis, or classical ML optimization used in quantum experiments, quantum
  enhancements have also been (theoretically) demonstrated for interactive
  learning tasks, highlighting the potential of quantum-enhanced learning
  agents. Finally, works exploring the use of AI for the very design of quantum
  experiments and for performing parts of genuine research autonomously, have
  reported their first successes. Beyond the topics of mutual
  enhancement\textemdash exploring what ML/AI can do for quantum physics and
  vice versa\textemdash researchers have also broached the fundamental issue of
  quantum generalizations of learning and AI concepts. This deals with
  questions of the very meaning of learning and intelligence in a world that is
  fully described by quantum mechanics. In this review, we describe the main
  ideas, recent developments and progress in a broad spectrum of research
  investigating ML and AI in the quantum domain.%
    }
    \verb{doi}
    \verb 10.1088/1361-6633/aab406
    \endverb
    \field{issn}{0034-4885}
    \field{number}{7}
    \field{pages}{074001}
    \field{shorttitle}{Machine Learning \& Artificial Intelligence in the
  Quantum Domain}
    \field{title}{Machine Learning \& Artificial Intelligence in the Quantum
  Domain: A Review of Recent Progress}
    \field{volume}{81}
    \field{langid}{english}
    \field{journaltitle}{Reports on Progress in Physics}
    \field{month}{06}
    \field{year}{2018}
  \endentry

  \entry{degennesSoftMatter1992}{article}{}
    \name{author}{1}{}{%
      {{hash=dPG}{%
         family={{de Gennes}},
         familyi={d\bibinitperiod},
         given={P.\bibnamedelima G.},
         giveni={P\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{dPG1}
    \strng{fullhash}{dPG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{{de }92}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    DOI:https://doi.org/10.1103/RevModPhys.64.645%
    }
    \verb{doi}
    \verb 10.1103/RevModPhys.64.645
    \endverb
    \field{number}{3}
    \field{pages}{645\bibrangedash 648}
    \field{title}{Soft Matter}
    \field{volume}{64}
    \field{journaltitle}{Reviews of Modern Physics}
    \field{month}{07}
    \field{year}{1992}
  \endentry

  \entry{dorigoAntColonySystem1997}{article}{}
    \name{author}{2}{}{%
      {{hash=DM}{%
         family={Dorigo},
         familyi={D\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GL}{%
         family={Gambardella},
         familyi={G\bibinitperiod},
         given={L.M.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{Ant colony optimization,Computational modeling,Distributed
  algorithms,Evolutionary computation,Feedback,Global communication,Legged
  locomotion,Simulated annealing,Traveling salesman problems}
    \strng{namehash}{DMGL1}
    \strng{fullhash}{DMGL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{DG97}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    This paper introduces the ant colony system (ACS), a distributed algorithm
  that is applied to the traveling salesman problem (TSP). In the ACS, a set of
  cooperating agents called ants cooperate to find good solutions to TSPs. Ants
  cooperate using an indirect form of communication mediated by a pheromone
  they deposit on the edges of the TSP graph while building solutions. We study
  the ACS by running experiments to understand its operation. The results show
  that the ACS outperforms other nature-inspired algorithms such as simulated
  annealing and evolutionary computation, and we conclude comparing ACS-3-opt,
  a version of the ACS augmented with a local search procedure, to some of the
  best performing algorithms for symmetric and asymmetric TSPs.%
    }
    \verb{doi}
    \verb 10.1109/4235.585892
    \endverb
    \field{issn}{1941-0026}
    \field{number}{1}
    \field{pages}{53\bibrangedash 66}
    \field{shorttitle}{Ant Colony System}
    \field{title}{Ant Colony System: A Cooperative Learning Approach to the
  Traveling Salesman Problem}
    \field{volume}{1}
    \field{journaltitle}{IEEE Transactions on Evolutionary Computation}
    \field{month}{04}
    \field{year}{1997}
  \endentry

  \entry{dhontIntroductionDynamicsColloids1996}{book}{}
    \name{author}{1}{}{%
      {{hash=DJKG}{%
         family={Dhont},
         familyi={D\bibinitperiod},
         given={J.\bibnamedelima K.\bibnamedelima G.},
         giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim
  G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier}%
    }
    \keyw{Science / Chemistry / Computational \& Molecular Modeling,Science /
  Chemistry / Physical \& Theoretical,Science / Mechanics / Fluids,Technology
  \& Engineering / Chemical \& Biochemical}
    \strng{namehash}{DJKG1}
    \strng{fullhash}{DJKG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Dho96}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    One of the few textbooks in the field, this volume deals with several
  aspects of the dynamics of colloids. A self-contained treatise, it fills the
  gap between research literature and existing books for graduate students and
  researchers. For readers with a background in chemistry, the first chapter
  contains a section on frequently used mathematical techniques, as well as
  statistical mechanics.Some of the topics covered include:\textbullet{}
  diffusion of free particles on the basis of the Langevin equation\textbullet
  the separation of time, length and angular scales;\textbullet{} the
  fundamental Fokker-Planck and Smoluchowski equations derived for interacting
  particles\textbullet{} friction of spheres and rods, and hydrodynamic
  interaction of spheres (including three body interactions)\textbullet{}
  diffusion, sedimentation, critical phenomena and phase separation
  kinetics\textbullet{} experimental light scattering results.For universities
  and research departments in industry this textbook makes vital reading.%
    }
    \field{isbn}{978-0-08-053507-4}
    \field{title}{An Introduction to Dynamics of Colloids}
    \field{langid}{english}
    \field{month}{05}
    \field{year}{1996}
  \endentry

  \entry{dijkstraPredictiveModellingMachine2021a}{article}{}
    \name{author}{2}{}{%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LE}{%
         family={Luijten},
         familyi={L\bibinitperiod},
         given={Erik},
         giveni={E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{DMLE1}
    \strng{fullhash}{DMLE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{DL21}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    An overwhelming diversity of colloidal building blocks with distinct sizes,
  materials and tunable interaction potentials are now available for colloidal
  self-assembly. The application space for materials composed of these building
  blocks is vast. To make progress in the rational design of new self-assembled
  materials, it is desirable to guide the experimental synthesis efforts by
  computational modelling. Here, we discuss computer simulation methods and
  strategies used for the design of soft materials created through bottom-up
  self-assembly of colloids and nanoparticles. We describe simulation
  techniques for investigating the self-assembly behaviour of colloidal
  suspensions, including crystal structure prediction methods, phase diagram
  calculations and enhanced sampling techniques, as well as their limitations.
  We also discuss the recent surge of interest in machine learning and
  reverse-engineering methods. Although their implementation in the colloidal
  realm is still in its infancy, we anticipate that these data-science tools
  offer new paradigms in understanding, predicting and (inverse) design of
  novel colloidal materials.%
    }
    \verb{doi}
    \verb 10.1038/s41563-021-01014-2
    \endverb
    \field{issn}{1476-4660}
    \field{number}{6}
    \field{pages}{762\bibrangedash 773}
    \field{title}{From Predictive Modelling to Machine Learning and Reverse
  Engineering of Colloidal Self-Assembly}
    \field{volume}{20}
    \field{langid}{english}
    \field{journaltitle}{Nature Materials}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Reviews Subject\_term: Materials science;Nanoscale materials;Statistical
  physics, thermodynamics and nonlinear dynamics Subject\_term\_id:
  materials-science;nanoscale-materials;statistical-physics-thermodynamics-and-nonlinear-dynamics%
    }
    \field{month}{06}
    \field{year}{2021}
  \endentry

  \entry{dasDifferentialEvolutionSurvey2011}{article}{}
    \name{author}{2}{}{%
      {{hash=DS}{%
         family={Das},
         familyi={D\bibinitperiod},
         given={Swagatam},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SPN}{%
         family={Suganthan},
         familyi={S\bibinitperiod},
         given={Ponnuthurai\bibnamedelima Nagaratnam},
         giveni={P\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
    }
    \keyw{Chromium,Derivative-free optimization,differential evolution
  (DE),direct search,Evolution (biology),evolutionary algorithms (EAs),genetic
  algorithms (GAs),Heuristic
  algorithms,metaheuristics,Minimization,Optimization,Particle swarm
  optimization,particle swarm optimization (PSO)}
    \strng{namehash}{DSSPN1}
    \strng{fullhash}{DSSPN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{DS11}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    Differential evolution (DE) is arguably one of the most powerful stochastic
  real-parameter optimization algorithms in current use. DE operates through
  similar computational steps as employed by a standard evolutionary algorithm
  (EA). However, unlike traditional EAs, the DE-variants perturb the
  current-generation population members with the scaled differences of randomly
  selected and distinct population members. Therefore, no separate probability
  distribution has to be used for generating the offspring. Since its inception
  in 1995, DE has drawn the attention of many researchers all over the world
  resulting in a lot of variants of the basic algorithm with improved
  performance. This paper presents a detailed review of the basic concepts of
  DE and a survey of its major variants, its application to multiobjective,
  constrained, large scale, and uncertain optimization problems, and the
  theoretical studies conducted on DE so far. Also, it provides an overview of
  the significant engineering applications that have benefited from the
  powerful nature of DE.%
    }
    \verb{doi}
    \verb 10.1109/TEVC.2010.2059031
    \endverb
    \field{issn}{1941-0026}
    \field{number}{1}
    \field{pages}{4\bibrangedash 31}
    \field{shorttitle}{Differential Evolution}
    \field{title}{Differential Evolution: A Survey of the State-of-the-Art}
    \field{volume}{15}
    \field{journaltitle}{IEEE Transactions on Evolutionary Computation}
    \field{month}{02}
    \field{year}{2011}
  \endentry

  \entry{dijkstraPhaseDiagramHighly1999}{article}{}
    \name{author}{3}{}{%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
      {{hash=vR}{%
         family={{van Roij}},
         familyi={v\bibinitperiod},
         given={Ren{\'e}},
         giveni={R\bibinitperiod},
      }}%
      {{hash=ER}{%
         family={Evans},
         familyi={E\bibinitperiod},
         given={Robert},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{DMvRER1}
    \strng{fullhash}{DMvRER1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{DvE99}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    We study the phase behavior and structure of highly asymmetric binary
  hard-sphere mixtures. By first integrating out the degrees of freedom of the
  small spheres in the partition function we derive a formal expression for the
  effective Hamiltonian of the large spheres. Then using an explicit pairwise
  (depletion) potential approximation to this effective Hamiltonian in computer
  simulations, we determine fluid-solid coexistence for size ratios
  q=0.033,0.05,0.1,0.2, and 1.0. The resulting two-phase region becomes very
  broad in packing fractions of the large spheres as q becomes very small. We
  find a stable, isostructural solid-solid transition for q{$<\sptilde$}0.05
  and a fluid-fluid transition for q{$<\sptilde$}0.10. However, the latter
  remains metastable with respect to the fluid-solid transition for all size
  ratios we investigate. In the limit \textrightarrow q0 the phase diagram
  mimics that of the sticky-sphere system. As expected, the radial distribution
  function g(r) and the structure factor S(k) of the effective one-component
  system show no sharp signature of the onset of the freezing transition and we
  find that at most points on the fluid-solid boundary the value of S(k) at its
  first peak is much lower than the value given by the Hansen-Verlet freezing
  criterion. Direct simulations of the true binary mixture of hard spheres were
  performed for q{$>\sptilde$}0.05 in order to test the predictions from the
  effective Hamiltonian. For those packing fractions of the small spheres where
  direct simulations are possible, we find remarkably good agreement between
  the phase boundaries calculated from the two approaches\textemdash even up to
  the symmetric limit q=1 and for very high packings of the large spheres,
  where the solid-solid transition occurs. In both limits one might expect that
  an approximation which neglects higher-body terms should fail, but our
  results support the notion that the main features of the phase equilibria of
  asymmetric binary hard-sphere mixtures are accounted for by the effective
  pairwise depletion potential description. We also compare our results with
  those of other theoretical treatments and experiments on colloidal
  hard-sphere mixtures.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevE.59.5744
    \endverb
    \field{number}{5}
    \field{pages}{5744\bibrangedash 5771}
    \field{title}{Phase Diagram of Highly Asymmetric Binary Hard-Sphere
  Mixtures}
    \field{volume}{59}
    \field{journaltitle}{Physical Review E}
    \field{month}{05}
    \field{year}{1999}
  \endentry

  \entry{evansSimpleLiquidsColloids2019}{article}{}
    \name{author}{3}{}{%
      {{hash=ER}{%
         family={Evans},
         familyi={E\bibinitperiod},
         given={Robert},
         giveni={R\bibinitperiod},
      }}%
      {{hash=FD}{%
         family={Frenkel},
         familyi={F\bibinitperiod},
         given={Daan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{ERFDDM1}
    \strng{fullhash}{ERFDDM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{EFD19}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \verb{doi}
    \verb 10.1063/PT.3.4135
    \endverb
    \field{issn}{0031-9228}
    \field{number}{2}
    \field{pages}{38\bibrangedash 39}
    \field{title}{From Simple Liquids to Colloids and Soft Matter}
    \field{volume}{72}
    \field{journaltitle}{Physics Today}
    \field{month}{02}
    \field{year}{2019}
  \endentry

  \entry{edelmanHowManyZeros1995}{article}{}
    \name{author}{2}{}{%
      {{hash=EA}{%
         family={Edelman},
         familyi={E\bibinitperiod},
         given={Alan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KE}{%
         family={Kostlan},
         familyi={K\bibinitperiod},
         given={Eric},
         giveni={E\bibinitperiod},
      }}%
    }
    \strng{namehash}{EAKE1}
    \strng{fullhash}{EAKE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{EK95}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \field{abstract}{%
    We provide an elementary geometric derivation of the Kac integral formula
  for the expected number of real zeros of a random polynomial with independent
  standard normally distributed coefficients. We show that the expected number
  of real zeros is simply the length of the moment curve (1, t, ... , t")
  projected onto the surface of the unit sphere, divided by n . The probability
  density of the real zeros is proportional to how fast this curve is traced
  out. We then relax Kac's assumptions by considering a variety of random sums,
  series, and distributions, and we also illustrate such ideas as integral
  geometry and the Fubini-Study metric.%
    }
    \verb{doi}
    \verb 10.1090/S0273-0979-1995-00571-9
    \endverb
    \field{issn}{0273-0979}
    \field{number}{1}
    \field{pages}{1\bibrangedash 38}
    \field{title}{How Many Zeros of a Random Polynomial Are Real?}
    \field{volume}{32}
    \field{langid}{english}
    \field{journaltitle}{Bulletin of the American Mathematical Society}
    \field{month}{01}
    \field{year}{1995}
  \endentry

  \entry{forresterRecentAdvancesSurrogatebased2009}{article}{}
    \name{author}{2}{}{%
      {{hash=FAIJ}{%
         family={Forrester},
         familyi={F\bibinitperiod},
         given={Alexander I.\bibnamedelima J.},
         giveni={A\bibinitperiod\bibinitdelim I\bibinitperiod\bibinitdelim
  J\bibinitperiod},
      }}%
      {{hash=KAJ}{%
         family={Keane},
         familyi={K\bibinitperiod},
         given={Andy\bibnamedelima J.},
         giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \strng{namehash}{FAIJKAJ1}
    \strng{fullhash}{FAIJKAJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{FK09}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    The evaluation of aerospace designs is synonymous with the use of long
  running and computationally intensive simulations. This fuels the desire to
  harness the efficiency of surrogate-based methods in aerospace design
  optimization. Recent advances in surrogate-based design methodology bring the
  promise of efficient global optimization closer to reality. We review the
  present state of the art of constructing surrogate models and their use in
  optimization strategies. We make extensive use of pictorial examples and,
  since no method is truly universal, give guidance as to each method's
  strengths and weaknesses.%
    }
    \verb{doi}
    \verb 10.1016/j.paerosci.2008.11.001
    \endverb
    \field{issn}{0376-0421}
    \field{number}{1}
    \field{pages}{50\bibrangedash 79}
    \field{title}{Recent Advances in Surrogate-Based Optimization}
    \field{volume}{45}
    \field{langid}{english}
    \field{journaltitle}{Progress in Aerospace Sciences}
    \field{month}{01}
    \field{year}{2009}
  \endentry

  \entry{fengPerformanceAnalysisVarious2019}{article}{}
    \name{author}{2}{}{%
      {{hash=FJ}{%
         family={Feng},
         familyi={F\bibinitperiod},
         given={Jianli},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Lu},
         familyi={L\bibinitperiod},
         given={Shengnan},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IOP Publishing}%
    }
    \strng{namehash}{FJLS1}
    \strng{fullhash}{FJLS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{FL19}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    The development of Artificial Neural Networks (ANNs) has achieved a lot of
  fruitful results so far, and we know that activation function is one of the
  principal factors which will affect the performance of the networks. In this
  work, the role of many different types of activation functions, as well as
  their respective advantages and disadvantages and applicable fields are
  discussed, so people can choose the appropriate activation functions to get
  the superior performance of ANNs.%
    }
    \verb{doi}
    \verb 10.1088/1742-6596/1237/2/022030
    \endverb
    \field{issn}{1742-6596}
    \field{pages}{022030}
    \field{title}{Performance Analysis of Various Activation Functions in
  Artificial Neural Networks}
    \field{volume}{1237}
    \field{langid}{english}
    \field{month}{06}
    \field{year}{2019}
  \endentry

  \entry{fogelWhatEvolutionaryComputation2000}{article}{}
    \name{author}{1}{}{%
      {{hash=FD}{%
         family={Fogel},
         familyi={F\bibinitperiod},
         given={D.B.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{Accidents,Chaos,Computer science,Cost function,Evolution
  (biology),Evolutionary computation,Heuristic algorithms,Joining
  processes,Organisms,Sea measurements}
    \strng{namehash}{FD1}
    \strng{fullhash}{FD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Fog00}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Taking a page from Darwin's 'On the origin of the species', computer
  scientists have found ways to evolve solutions to complex problems.
  Harnessing the evolutionary process within a computer provides a means for
  addressing complex engineering problems-ones involving chaotic disturbances,
  randomness, and complex nonlinear dynamics-that traditional algorithms have
  been unable to conquer. Indeed, the field of evolutionary computation is one
  of the fastest growing areas of computer science and engineering for just
  this reason; it is addressing many problems that were previously beyond
  reach, such as rapid design of medicines, flexible solutions to supply-chain
  management problems, and rapid analysis of battlefield tactics for defense.
  Potentially, the field may fulfil the dream of artificial intelligence: a
  computer that can learn on its own and become an expert in any chosen area.%
    }
    \verb{doi}
    \verb 10.1109/6.819926
    \endverb
    \field{issn}{1939-9340}
    \field{number}{2}
    \field{pages}{26\bibrangedash 32}
    \field{title}{What Is Evolutionary Computation?}
    \field{volume}{37}
    \field{journaltitle}{IEEE Spectrum}
    \field{month}{02}
    \field{year}{2000}
  \endentry

  \entry{fontenla-romeroNewConvexObjective2010}{article}{}
    \name{author}{4}{}{%
      {{hash=FO}{%
         family={{Fontenla-Romero}},
         familyi={F\bibinitperiod},
         given={Oscar},
         giveni={O\bibinitperiod},
      }}%
      {{hash=GB}{%
         family={{Guijarro-Berdi{\~n}as}},
         familyi={G\bibinitperiod},
         given={Bertha},
         giveni={B\bibinitperiod},
      }}%
      {{hash=PB}{%
         family={{P{\'e}rez-S{\'a}nchez}},
         familyi={P\bibinitperiod},
         given={Beatriz},
         giveni={B\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={{Alonso-Betanzos}},
         familyi={A\bibinitperiod},
         given={Amparo},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{Convex optimization,Global optimum,Incremental learning,Least
  squares,Single-layer neural networks,Supervised learning method}
    \strng{namehash}{FO+1}
    \strng{fullhash}{FOGBPBAA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{{Fon}+10}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    This paper proposes a novel supervised learning method for single-layer
  feedforward neural networks. This approach uses an alternative objective
  function to that based on the MSE, which measures the errors before the
  neuron's nonlinear activation functions instead of after them. In this case,
  the solution can be easily obtained solving systems of linear equations,
  i.e., requiring much less computational power than the one associated with
  the regular methods. A theoretical study is included to proof the
  approximated equivalence between the global optimum of the objective function
  based on the regular MSE criterion and the one of the proposed alternative
  MSE function. Furthermore, it is shown that the presented method has the
  capability of allowing incremental and distributed learning. An exhaustive
  experimental study is also presented to verify the soundness and efficiency
  of the method. This study contains 10 classification and 16 regression
  problems. In addition, a comparison with other high performance learning
  algorithms shows that the proposed method exhibits, in average, the highest
  performance and low-demanding computational requirements.%
    }
    \verb{doi}
    \verb 10.1016/j.patcog.2009.11.024
    \endverb
    \field{issn}{0031-3203}
    \field{number}{5}
    \field{pages}{1984\bibrangedash 1992}
    \field{title}{A New Convex Objective Function for the Supervised Learning
  of Single-Layer Neural Networks}
    \field{volume}{43}
    \field{langid}{english}
    \field{journaltitle}{Pattern Recognition}
    \field{month}{05}
    \field{year}{2010}
  \endentry

  \entry{frenkelUnderstandingMolecularSimulation2001}{book}{}
    \name{author}{2}{}{%
      {{hash=FD}{%
         family={Frenkel},
         familyi={F\bibinitperiod},
         given={Daan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SB}{%
         family={Smit},
         familyi={S\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier}%
    }
    \keyw{Computers / Design; Graphics \& Media / Graphics Tools,Science /
  Physics / Atomic \& Molecular,Science / Physics / Mathematical \&
  Computational,Technology \& Engineering / Materials Science / General}
    \strng{namehash}{FDSB1}
    \strng{fullhash}{FDSB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{FS01}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Understanding Molecular Simulation: From Algorithms to Applications
  explains the physics behind the \&quot;recipes\&quot; of molecular simulation
  for materials science. Computer simulators are continuously confronted with
  questions concerning the choice of a particular technique for a given
  application. A wide variety of tools exist, so the choice of technique
  requires a good understanding of the basic principles. More importantly, such
  understanding may greatly improve the efficiency of a simulation program. The
  implementation of simulation methods is illustrated in pseudocodes and their
  practical use in the case studies used in the text. Since the first edition
  only five years ago, the simulation world has changed significantly --
  current techniques have matured and new ones have appeared. This new edition
  deals with these new developments; in particular, there are sections on:
  Transition path sampling and diffusive barrier crossing to simulaterare
  eventsDissipative particle dynamic as a course-grained simulation
  techniqueNovel schemes to compute the long-ranged forcesHamiltonian and
  non-Hamiltonian dynamics in the context constant-temperature and
  constant-pressure molecular dynamics simulationsMultiple-time step algorithms
  as an alternative for constraintsDefects in solidsThe pruned-enriched
  Rosenbluth sampling, recoil-growth, and concerted rotations for complex
  moleculesParallel tempering for glassy Hamiltonians Examples are included
  that highlight current applications and the codes of case studies are
  available on the World Wide Web. Several new examples have been added since
  the first edition to illustrate recent applications. Questions are included
  in this new edition. No prior knowledge of computer simulation is assumed.%
    }
    \field{isbn}{978-0-08-051998-2}
    \field{shorttitle}{Understanding Molecular Simulation}
    \field{title}{Understanding Molecular Simulation: From Algorithms to
  Applications}
    \field{langid}{english}
    \field{month}{10}
    \field{year}{2001}
  \endentry

  \entry{fukushimaProposalDistanceweightedExponential2011}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=FN}{%
         family={Fukushima},
         familyi={F\bibinitperiod},
         given={Nobusumi},
         giveni={N\bibinitperiod},
      }}%
      {{hash=NY}{%
         family={Nagata},
         familyi={N\bibinitperiod},
         given={Yuichi},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={Kobayashi},
         familyi={K\bibinitperiod},
         given={Sigenobu},
         giveni={S\bibinitperiod},
      }}%
      {{hash=OI}{%
         family={Ono},
         familyi={O\bibinitperiod},
         given={Isao},
         giveni={I\bibinitperiod},
      }}%
    }
    \keyw{Accuracy,Benchmark testing,Convergence,Covariance matrix,Gaussian
  distribution,Optimization,Search problems}
    \strng{namehash}{FN+1}
    \strng{fullhash}{FNNYKSOI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Fuk+11}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    This paper presents a new evolutionary algorithm for function optimization
  named the distance-weighted exponential natural evolution strategies
  (DX-NES). DX-NES remedies two problems of a conventional method, the
  exponential natural evolution strategies (xNES), that shows good performance
  when it does not need to move the distribution for sampling individuals down
  the slope to the optimal point. The first problem of xNES is that the search
  efficiency deteriorates while the distribution moves down the slope of an
  ill-scaled function because it degenerates before reaching the optimal point.
  The second problem is that the settings of learning rates are inappropriate
  because they do not taking account of some factors affecting the estimate
  accuracy of the natural gradient. We compared the performance of DX-NES with
  that of xNES and CMA-ES on typical benchmark functions and confirmed that
  DX-NES outperformed the xNES on all the benchmark functions and that DX-NES
  showed better performance than CMA-ES on the almost all functions except the
  k-tablet function.%
    }
    \field{booktitle}{2011 IEEE Congress of Evolutionary Computation (CEC)}
    \verb{doi}
    \verb 10.1109/CEC.2011.5949614
    \endverb
    \field{issn}{1941-0026}
    \field{pages}{164\bibrangedash 171}
    \field{title}{Proposal of Distance-Weighted Exponential Natural Evolution
  Strategies}
    \field{month}{06}
    \field{year}{2011}
  \endentry

  \entry{glorotUnderstandingDifficultyTraining2010}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=GX}{%
         family={Glorot},
         familyi={G\bibinitperiod},
         given={Xavier},
         giveni={X\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{2}{%
      {JMLR Workshop}%
      {Conference Proceedings}%
    }
    \strng{namehash}{GXBY1}
    \strng{fullhash}{GXBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{GB10}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Whereas before 2006 it appears that deep multi-layer neural networks were
  not successfully trained, since then several algorithms have been shown to
  successfully train them, with experimental resul...%
    }
    \field{booktitle}{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}
    \field{issn}{1938-7228}
    \field{pages}{249\bibrangedash 256}
    \field{title}{Understanding the Difficulty of Training Deep Feedforward
  Neural Networks}
    \field{langid}{english}
    \field{month}{03}
    \field{year}{2010}
  \endentry

  \entry{gelbartNewScienceComplex1996}{article}{}
    \name{author}{2}{}{%
      {{hash=GWM}{%
         family={Gelbart},
         familyi={G\bibinitperiod},
         given={William\bibnamedelima M.},
         giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={{Ben-Shaul}},
         familyi={B\bibinitperiod},
         given={Avinoam},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Chemical Society}%
    }
    \strng{namehash}{GWMBA1}
    \strng{fullhash}{GWMBA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{GB96}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    We present an overview of the modern study of complex fluids which, because
  of the overwhelming breadth and richness of this field, unavoidably neglects
  many interesting systems and research developments. In proposing a definition
  of the field, we discuss first the special role played by phenomenological
  theory and the limitations of molecular-level description. The remainder of
  the article is organized into sections which treat model colloids, micellized
  surfactant solutions, interfacial films and microemulsions, bilayers and
  membranes, and new materials. In each instance we try to provide a physical
  basis for the special nature of interactions and long-range ordering
  transitions in these novel colloidal and thin layer systems. At the heart of
  understanding these highly varied phenomena lie the curvature dependence of
  surface energies and the coupling between self-assembly on small length
  scales and phase changes at large ones.%
    }
    \verb{doi}
    \verb 10.1021/jp9606570
    \endverb
    \field{issn}{0022-3654}
    \field{number}{31}
    \field{pages}{13169\bibrangedash 13189}
    \field{title}{The ``New'' Science of ``Complex Fluids''}
    \field{volume}{100}
    \field{journaltitle}{The Journal of Physical Chemistry}
    \field{month}{01}
    \field{year}{1996}
  \endentry

  \entry{glorotDeepSparseRectifier2011}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=GX}{%
         family={Glorot},
         familyi={G\bibinitperiod},
         given={Xavier},
         giveni={X\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Bordes},
         familyi={B\bibinitperiod},
         given={Antoine},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{2}{%
      {JMLR Workshop}%
      {Conference Proceedings}%
    }
    \strng{namehash}{GXBABY1}
    \strng{fullhash}{GXBABY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{GBB11}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    While logistic sigmoid neurons are more biologically plausible than
  hyperbolic tangent neurons, the latter work better for training multi-layer
  neural networks. This paper shows that rectifying neu...%
    }
    \field{booktitle}{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}
    \field{issn}{1938-7228}
    \field{pages}{315\bibrangedash 323}
    \field{title}{Deep Sparse Rectifier Neural Networks}
    \field{langid}{english}
    \field{month}{06}
    \field{year}{2011}
  \endentry

  \entry{goodfellowDeepLearning2016}{book}{}
    \name{author}{3}{}{%
      {{hash=GI}{%
         family={Goodfellow},
         familyi={G\bibinitperiod},
         given={Ian},
         giveni={I\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Courville},
         familyi={C\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{Computers / Artificial Intelligence / General,Computers / Computer
  Science}
    \strng{namehash}{GIBYCA1}
    \strng{fullhash}{GIBYCA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{GBC16}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    An introduction to a broad range of topics in deep learning, covering
  mathematical and conceptual background, deep learning techniques used in
  industry, and research perspectives.``Written by three experts in the field,
  Deep Learning is the only comprehensive book on the subject.''\textemdash
  Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep
  learning is a form of machine learning that enables computers to learn from
  experience and understand the world in terms of a hierarchy of concepts.
  Because the computer gathers knowledge from experience, there is no need for
  a human computer operator to formally specify all the knowledge that the
  computer needs. The hierarchy of concepts allows the computer to learn
  complicated concepts by building them out of simpler ones; a graph of these
  hierarchies would be many layers deep. This book introduces a broad range of
  topics in deep learning. The text offers mathematical and conceptual
  background, covering relevant concepts in linear algebra, probability theory
  and information theory, numerical computation, and machine learning. It
  describes deep learning techniques used by practitioners in industry,
  including deep feedforward networks, regularization, optimization algorithms,
  convolutional networks, sequence modeling, and practical methodology; and it
  surveys such applications as natural language processing, speech recognition,
  computer vision, online recommendation systems, bioinformatics, and
  videogames. Finally, the book offers research perspectives, covering such
  theoretical topics as linear factor models, autoencoders, representation
  learning, structured probabilistic models, Monte Carlo methods, the partition
  function, approximate inference, and deep generative models. Deep Learning
  can be used by undergraduate or graduate students planning careers in either
  industry or research, and by software engineers who want to begin using deep
  learning in their products or platforms. A website offers supplementary
  material for both readers and instructors.%
    }
    \field{isbn}{978-0-262-33737-3}
    \field{title}{Deep Learning}
    \field{langid}{english}
    \field{month}{11}
    \field{year}{2016}
  \endentry

  \entry{gibbsElementaryPrinciplesStatistical2014}{book}{}
    \name{author}{1}{}{%
      {{hash=GJW}{%
         family={Gibbs},
         familyi={G\bibinitperiod},
         given={J.\bibnamedelima Willard},
         giveni={J\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Courier Corporation}%
    }
    \keyw{History / General,Science / Mechanics / General,Science / Physics /
  General}
    \strng{namehash}{GJW1}
    \strng{fullhash}{GJW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Gib14}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Written by J. Willard Gibbs, the most distinguished American mathematical
  physicist of the nineteenth century, this book was the first to bring
  together and arrange in logical order the works of Clausius, Maxwell,
  Boltzmann, and Gibbs himself. The lucid, advanced-level text remains a
  valuable collection of fundamental equations and principles. Topics include
  the general problem and the fundamental equation of statistical mechanics,
  the canonical distribution of the average energy values in a canonical
  ensemble of systems, and formulas for evaluating important functions of the
  energies of a system. Additional discussions cover maximum and minimal
  properties of distribution in phase, a valuable comparison of statistical
  mechanics with thermodynamics, and many other subjects.%
    }
    \field{isbn}{978-0-486-78995-8}
    \field{title}{Elementary Principles in Statistical Mechanics}
    \field{langid}{english}
    \field{month}{12}
    \field{year}{2014}
  \endentry

  \entry{goldsteinClassicalMechanics2002}{book}{}
    \name{author}{3}{}{%
      {{hash=GH}{%
         family={Goldstein},
         familyi={G\bibinitperiod},
         given={Herbert},
         giveni={H\bibinitperiod},
      }}%
      {{hash=PCP}{%
         family={Poole},
         familyi={P\bibinitperiod},
         given={Charles\bibnamedelima P.},
         giveni={C\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=SJL}{%
         family={Safko},
         familyi={S\bibinitperiod},
         given={John\bibnamedelima L.},
         giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Addison Wesley}%
    }
    \keyw{Science / Physics / General}
    \strng{namehash}{GHPCPSJL1}
    \strng{fullhash}{GHPCPSJL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{GPS02}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    For thirty years this has been the acknowledged standard in advanced
  classical mechanics courses. This text enables students to make connections
  between classical and modern physics. In this edition, Beams Medal winner
  Charles Poole and John Safko have updated the text to include the latest
  topics, applications, and notation, to reflect today's physics curriculum.
  They introduce students to the increasingly important role that
  nonlinearities play in contemporary applications of classical mechanics.
  Numerical exercises help students to develop skills in how to use computer
  techniques to solve problems in physics. Mathematical techniques are
  presented in detail so that the text remains fully accessible to students who
  have not had an intermediate course in classical mechanics.%
    }
    \field{isbn}{978-0-201-65702-9}
    \field{title}{Classical Mechanics}
    \field{langid}{english}
    \field{year}{2002}
  \endentry

  \entry{gastSimpleOrderingComplex1998}{article}{}
    \name{author}{2}{}{%
      {{hash=GAP}{%
         family={Gast},
         familyi={G\bibinitperiod},
         given={Alice\bibnamedelima P.},
         giveni={A\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=RWB}{%
         family={Russel},
         familyi={R\bibinitperiod},
         given={William\bibnamedelima B.},
         giveni={W\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{GAPRWB1}
    \strng{fullhash}{GAPRWB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{GR98}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \verb{doi}
    \verb 10.1063/1.882495
    \endverb
    \field{issn}{0031-9228}
    \field{number}{12}
    \field{pages}{24\bibrangedash 30}
    \field{title}{Simple Ordering in Complex Fluids}
    \field{volume}{51}
    \field{journaltitle}{Physics Today}
    \field{month}{12}
    \field{year}{1998}
  \endentry

  \entry{grzybowskiSelfassemblyCrystalsCells2009}{article}{}
    \name{author}{5}{}{%
      {{hash=GBA}{%
         family={Grzybowski},
         familyi={G\bibinitperiod},
         given={Bartosz\bibnamedelima A.},
         giveni={B\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=WCE}{%
         family={Wilmer},
         familyi={W\bibinitperiod},
         given={Christopher\bibnamedelima E.},
         giveni={C\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=KJ}{%
         family={Kim},
         familyi={K\bibinitperiod},
         given={Jiwon},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BKP}{%
         family={Browne},
         familyi={B\bibinitperiod},
         given={Kevin\bibnamedelima P.},
         giveni={K\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=BKJM}{%
         family={Bishop},
         familyi={B\bibinitperiod},
         given={Kyle J.\bibnamedelima M.},
         giveni={K\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {The Royal Society of Chemistry}%
    }
    \strng{namehash}{GBA+1}
    \strng{fullhash}{GBAWCEKJBKPBKJM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Grz+09}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Self-assembly (SA) is the process in which a system's components\textemdash
  be it molecules, polymers, colloids, or macroscopic particles\textemdash
  organize into ordered and/or functional structures without human
  intervention. The main challenge in SA research is the ability to ``program''
  the properties of the individual pieces such that they organize into a
  desired structure. Although a general strategy for doing so is still elusive,
  heuristic rules can be formulated that guide design of SA under various
  conditions and thermodynamic constraints. This Review examines SA in both the
  equilibrium and non-equilibrium/dynamic systems and discusses different SA
  modalities: energy driven, entropy-driven, templated, and field-directed.
  Non-equilibrium SA is discussed as a route to reconfigurable (``adaptive'')
  materials, and its connection to biological systems is emphasized.%
    }
    \verb{doi}
    \verb 10.1039/B819321P
    \endverb
    \field{issn}{1744-6848}
    \field{number}{6}
    \field{pages}{1110\bibrangedash 1128}
    \field{shorttitle}{Self-Assembly}
    \field{title}{Self-Assembly: From Crystals to Cells}
    \field{volume}{5}
    \field{langid}{english}
    \field{journaltitle}{Soft Matter}
    \field{month}{03}
    \field{year}{2009}
  \endentry

  \entry{hammingNumericalMethodsScientists2012}{book}{}
    \name{author}{1}{}{%
      {{hash=HR}{%
         family={Hamming},
         familyi={H\bibinitperiod},
         given={Richard},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Courier Corporation}%
    }
    \keyw{Mathematics / Mathematical Analysis}
    \strng{namehash}{HR1}
    \strng{fullhash}{HR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ham12}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Numerical analysis is a subject of extreme interest to mathematicians and
  computer scientists, who will welcome this first inexpensive paperback
  edition of a groundbreaking classic text on the subject. In an introductory
  chapter on numerical methods and their relevance to computing, well-known
  mathematician Richard Hamming (\&quot;the Hamming code,\&quot; \&quot;the
  Hamming distance,\&quot; and \&quot;Hamming window,\&quot; etc.), suggests
  that the purpose of computing is insight, not merely numbers. In that
  connection he outlines five main ideas that aim at producing meaningful
  numbers that will be read and used, but will also lead to greater
  understanding of how the choice of a particular formula or algorithm
  influences not only the computing but our understanding of the results
  obtained.The five main ideas involve (1) insuring that in computing there is
  an intimate connection between the source of the problem and the usability of
  the answers (2) avoiding isolated formulas and algorithms in favor of a
  systematic study of alternate ways of doing the problem (3) avoidance of
  roundoff (4) overcoming the problem of truncation error (5) insuring the
  stability of a feedback system.In this second edition, Professor Hamming
  (Naval Postgraduate School, Monterey, California) extensively rearranged,
  rewrote and enlarged the material. Moreover, this book is unique in its
  emphasis on the frequency approach and its use in the solution of problems.
  Contents include:I. Fundamentals and AlgorithmsII. Polynomial Approximation-
  Classical TheoryIll. Fourier Approximation- Modern TheoryIV. Exponential
  Approximation ... and moreHighly regarded by experts in the field, this is a
  book with unlimited applications for undergraduate and graduate students of
  mathematics, science and engineering. Professionals and researchers will find
  it a valuable reference they will turn to again and again.%
    }
    \field{isbn}{978-0-486-13482-6}
    \field{title}{Numerical Methods for Scientists and Engineers}
    \field{langid}{english}
    \field{month}{04}
    \field{year}{2012}
  \endentry

  \entry{hansenCMAEvolutionStrategy2006}{incollection}{}
    \name{author}{1}{}{%
      {{hash=HN}{%
         family={Hansen},
         familyi={H\bibinitperiod},
         given={Nikolaus},
         giveni={N\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=LJA}{%
         family={Lozano},
         familyi={L\bibinitperiod},
         given={Jose\bibnamedelima A.},
         giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Larra{\~n}aga},
         familyi={L\bibinitperiod},
         given={Pedro},
         giveni={P\bibinitperiod},
      }}%
      {{hash=II}{%
         family={Inza},
         familyi={I\bibinitperiod},
         given={I{\~n}aki},
         giveni={I\bibinitperiod},
      }}%
      {{hash=BE}{%
         family={Bengoetxea},
         familyi={B\bibinitperiod},
         given={Endika},
         giveni={E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \keyw{Covariance Matrix,Distribution Algorithm,Evolution Path,Search
  Point,Step Length}
    \strng{namehash}{HN1}
    \strng{fullhash}{HN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Han06}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    SummaryDerived from the concept of self-adaptation in evolution strategies,
  the CMA (Covariance Matrix Adaptation) adapts the covariance matrix of a
  multi-variate normal search distribution. The CMA was originally designed to
  perform well with small populations. In this review, the argument starts out
  with large population sizes, reflecting recent extensions of the CMA
  algorithm. Commonalities and differences to continuous Estimation of
  Distribution Algorithms are analyzed. The aspects of reliability of the
  estimation, overall step size control, and independence from the coordinate
  system (invariance) become particularly important in small populations sizes.
  Consequently, performing the adaptation task with small populations is more
  intricate.%
    }
    \field{booktitle}{Towards a New Evolutionary Computation: Advances in the
  Estimation of Distribution Algorithms}
    \verb{doi}
    \verb 10.1007/3-540-32494-1_4
    \endverb
    \field{isbn}{978-3-540-32494-2}
    \field{pages}{75\bibrangedash 102}
    \field{series}{Studies in Fuzziness and Soft Computing}
    \field{shorttitle}{The CMA Evolution Strategy}
    \field{title}{The CMA Evolution Strategy: A Comparing Review}
    \field{langid}{english}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{2006}
  \endentry

  \entry{hornMatrixAnalysis2012}{book}{}
    \name{author}{2}{}{%
      {{hash=HRA}{%
         family={Horn},
         familyi={H\bibinitperiod},
         given={Roger\bibnamedelima A.},
         giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=JCR}{%
         family={Johnson},
         familyi={J\bibinitperiod},
         given={Charles\bibnamedelima R.},
         giveni={C\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cambridge University Press}%
    }
    \keyw{Business \& Economics / Econometrics,Business \& Economics /
  Economics / General,Mathematics / Algebra / Abstract,Mathematics / Algebra /
  General,Mathematics / Geometry / Algebraic}
    \strng{namehash}{HRAJCR1}
    \strng{fullhash}{HRAJCR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{HJ12}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Linear algebra and matrix theory are fundamental tools in mathematical and
  physical science, as well as fertile fields for research. This second edition
  of this acclaimed text presents results of both classic and recent matrix
  analysis using canonical forms as a unifying theme and demonstrates their
  importance in a variety of applications. This thoroughly revised and updated
  second edition is a text for a second course on linear algebra and has more
  than 1,100 problems and exercises, new sections on the singular value and CS
  decompositions and the Weyr canonical form, expanded treatments of inverse
  problems and of block matrices, and much more.%
    }
    \field{isbn}{978-1-139-78888-5}
    \field{title}{Matrix Analysis}
    \field{langid}{english}
    \field{month}{10}
    \field{year}{2012}
  \endentry

  \entry{hansenTheorySimpleLiquids2013}{book}{}
    \name{author}{2}{}{%
      {{hash=HJP}{%
         family={Hansen},
         familyi={H\bibinitperiod},
         given={Jean-Pierre},
         giveni={J\bibinithyphendelim P\bibinitperiod},
      }}%
      {{hash=MIR}{%
         family={McDonald},
         familyi={M\bibinitperiod},
         given={I.\bibnamedelima R.},
         giveni={I\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Academic Press}%
    }
    \keyw{Science / Physics / Condensed Matter}
    \strng{namehash}{HJPMIR1}
    \strng{fullhash}{HJPMIR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{HM13}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Comprehensive coverage of topics in the theory of classical liquids Widely
  regarded as the standard text in its field, Theory of Simple Liquids gives an
  advanced but self-contained account of liquid state theory within the
  unifying framework provided by classical statistical mechanics. The structure
  of this revised and updated Fourth Edition is similar to that of the previous
  one but there are significant shifts in emphasis and much new material has
  been added. Major changes and Key Features in content include: Expansion of
  existing sections on simulation methods, liquid-vapour coexistence, the
  hierarchical reference theory of criticality, and the dynamics of
  super-cooled liquids.New sections on binary fluid mixtures, surface tension,
  wetting, the asymptotic decay of pair correlations, fluids in porous media,
  the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely
  new chapter on applications to 'soft matter' of a combination of liquid state
  theory and coarse graining strategies, with sections on polymer solutions and
  polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic
  liquid crystals, colloidal dynamics, and on clustering and gelation.
  Expansion of existing sections on simulation methods, liquid-vapour
  coexistence, the hierarchian reference of criticality, and the dynamics of
  super-cooled liquids.New sections on binary fluid mixtures, surface tension,
  wetting, the asymptotic decay of pair correlations, fluids in porous media,
  the thermodynamics of glasses, and fluid flow at solid surfaces.An entirely
  new chapter on applications to 'soft matter' of a combination of liquid state
  theory and coarse graining strategies, with sections on polymer solutions and
  polymer melts, colloidal dispersions, colloid-polymer mixtures, lyotropic
  liquid crystals, colloidal dynamics, and on clustering and gelation.%
    }
    \field{isbn}{978-0-12-387033-9}
    \field{shorttitle}{Theory of Simple Liquids}
    \field{title}{Theory of Simple Liquids: With Applications to Soft Matter}
    \field{langid}{english}
    \field{month}{08}
    \field{year}{2013}
  \endentry

  \entry{hornikApproximationCapabilitiesMultilayer1991}{article}{}
    \name{author}{1}{}{%
      {{hash=HK}{%
         family={Hornik},
         familyi={H\bibinitperiod},
         given={Kurt},
         giveni={K\bibinitperiod},
      }}%
    }
    \keyw{() approximation,Activation function,Input environment
  measure,Multilayer feedforward networks,Smooth approximation,Sobolev
  spaces,Uniform approximation,Universal approximation capabilities}
    \strng{namehash}{HK1}
    \strng{fullhash}{HK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Hor91}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    We show that standard multilayer feedforward networks with as few as a
  single hidden layer and arbitrary bounded and nonconstant activation function
  are universal approximators with respect to Lp({$\mu$}) performance criteria,
  for arbitrary finite input environment measures {$\mu$}, provided only that
  sufficiently many hidden units are available. If the activation function is
  continuous, bounded and nonconstant, then continuous mappings can be learned
  uniformly over compact input sets. We also give very general conditions
  ensuring that networks with sufficiently smooth activation functions are
  capable of arbitrarily accurate approximation to a function and its
  derivatives.%
    }
    \verb{doi}
    \verb 10.1016/0893-6080(91)90009-T
    \endverb
    \field{issn}{0893-6080}
    \field{number}{2}
    \field{pages}{251\bibrangedash 257}
    \field{title}{Approximation Capabilities of Multilayer Feedforward
  Networks}
    \field{volume}{4}
    \field{langid}{english}
    \field{journaltitle}{Neural Networks}
    \field{month}{01}
    \field{year}{1991}
  \endentry

  \entry{houghZerosGaussianAnalytic2009}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HJ}{%
         family={Hough},
         familyi={H\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=KM}{%
         family={Krishnapur},
         familyi={K\bibinitperiod},
         given={Manjunath},
         giveni={M\bibinitperiod},
      }}%
      {{hash=PY}{%
         family={Peres},
         familyi={P\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=VB}{%
         family={Vir{\'a}g},
         familyi={V\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
    }
    \strng{namehash}{HJ+1}
    \strng{fullhash}{HJKMPYVB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Hou+09}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    The book examines in some depth two important classes of point processes,
  determinantal processes and 'Gaussian zeros', i.e., zeros of random analytic
  functions with Gaussian coefficients. These processes share a property of
  'point-repulsion', where distinct points are less likely to fall close to
  each other than in processes, such as the Poisson process, that arise from
  independent sampling. Nevertheless, the treatment in the book emphasizes the
  use of independence: for random power series, the independence of
  coefficients is key; for determinantal processes, the number of points in a
  domain is a sum of independent indicators, and this yields a satisfying
  explanation of the central limit theorem (CLT) for this point count. Another
  unifying theme of the book is invariance of considered point processes under
  natural transformation groups. The book strives for balance between general
  theory and concrete examples. On the one hand, it presents a primer on modern
  techniques on the interface of probability and analysis. On the other hand, a
  wealth of determinantal processes of intrinsic interest are analyzed; these
  arise from random spanning trees and eigenvalues of random matrices, as well
  as from special power series with determinantal zeros. The material in the
  book formed the basis of a graduate course given at the IAS-Park City Summer
  School in 2007; the only background knowledge assumed can be acquired in
  first-year graduate courses in analysis and probability.%
    }
    \field{booktitle}{University Lecture Series}
    \verb{doi}
    \verb 10.1090/ULECT/051
    \endverb
    \field{title}{Zeros of Gaussian Analytic Functions and Determinantal Point
  Processes}
    \field{year}{2009}
  \endentry

  \entry{hooverMeltingTransitionCommunal1968a}{article}{}
    \name{author}{2}{}{%
      {{hash=HWG}{%
         family={Hoover},
         familyi={H\bibinitperiod},
         given={William\bibnamedelima G.},
         giveni={W\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=RFH}{%
         family={Ree},
         familyi={R\bibinitperiod},
         given={Francis\bibnamedelima H.},
         giveni={F\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{HWGRFH1}
    \strng{fullhash}{HWGRFH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{HR68}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \verb{doi}
    \verb 10.1063/1.1670641
    \endverb
    \field{issn}{0021-9606}
    \field{number}{8}
    \field{pages}{3609\bibrangedash 3617}
    \field{title}{Melting Transition and Communal Entropy for Hard Spheres}
    \field{volume}{49}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{10}
    \field{year}{1968}
  \endentry

  \entry{hornikMultilayerFeedforwardNetworks1989}{article}{}
    \name{author}{3}{}{%
      {{hash=HK}{%
         family={Hornik},
         familyi={H\bibinitperiod},
         given={Kurt},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Stinchcombe},
         familyi={S\bibinitperiod},
         given={Maxwell},
         giveni={M\bibinitperiod},
      }}%
      {{hash=WH}{%
         family={White},
         familyi={W\bibinitperiod},
         given={Halbert},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{Back-propagation networks,Feedforward networks,Mapping
  networks,Network representation capability,Sigma-Pi networks,Squashing
  functions,Stone-Weierstrass Theorem,Universal approximation}
    \strng{namehash}{HKSMWH1}
    \strng{fullhash}{HKSMWH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{HSW89}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    This paper rigorously establishes that standard multilayer feedforward
  networks with as few as one hidden layer using arbitrary squashing functions
  are capable of approximating any Borel measurable function from one finite
  dimensional space to another to any desired degree of accuracy, provided
  sufficiently many hidden units are available. In this sense, multilayer
  feedforward networks are a class of universal approximators.%
    }
    \verb{doi}
    \verb 10.1016/0893-6080(89)90020-8
    \endverb
    \field{issn}{0893-6080}
    \field{number}{5}
    \field{pages}{359\bibrangedash 366}
    \field{title}{Multilayer Feedforward Networks Are Universal Approximators}
    \field{volume}{2}
    \field{langid}{english}
    \field{journaltitle}{Neural Networks}
    \field{month}{01}
    \field{year}{1989}
  \endentry

  \entry{hastieElementsStatisticalLearning2009}{book}{}
    \name{author}{3}{}{%
      {{hash=HT}{%
         family={Hastie},
         familyi={H\bibinitperiod},
         given={Trevor},
         giveni={T\bibinitperiod},
      }}%
      {{hash=TR}{%
         family={Tibshirani},
         familyi={T\bibinitperiod},
         given={Robert},
         giveni={R\bibinitperiod},
      }}%
      {{hash=FJ}{%
         family={Friedman},
         familyi={F\bibinitperiod},
         given={Jerome},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Science \& Business Media}%
    }
    \keyw{Computers / Artificial Intelligence / General,Computers / Computer
  Science,Computers / Data Science / Data Analytics,Computers / Information
  Technology,Mathematics / Probability \& Statistics / General,Science / Life
  Sciences / Biology,Science / Life Sciences / General}
    \strng{namehash}{HTTRFJ1}
    \strng{fullhash}{HTTRFJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{HTF09}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    During the past decade there has been an explosion in computation and
  information technology. With it have come vast amounts of data in a variety
  of fields such as medicine, biology, finance, and marketing. The challenge of
  understanding these data has led to the development of new tools in the field
  of statistics, and spawned new areas such as data mining, machine learning,
  and bioinformatics. Many of these tools have common underpinnings but are
  often expressed with different terminology. This book describes the important
  ideas in these areas in a common conceptual framework. While the approach is
  statistical, the emphasis is on concepts rather than mathematics. Many
  examples are given, with a liberal use of color graphics. It is a valuable
  resource for statisticians and anyone interested in data mining in science or
  industry. The book's coverage is broad, from supervised learning (prediction)
  to unsupervised learning. The many topics include neural networks, support
  vector machines, classification trees and boosting---the first comprehensive
  treatment of this topic in any book. This major new edition features many
  topics not covered in the original, including graphical models, random
  forests, ensemble methods, least angle regression \& path algorithms for the
  lasso, non-negative matrix factorization, and spectral clustering. There is
  also a chapter on methods for ``wide'' data (p bigger than n), including
  multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani,
  and Jerome Friedman are professors of statistics at Stanford University. They
  are prominent researchers in this area: Hastie and Tibshirani developed
  generalized additive models and wrote a popular book of that title. Hastie
  co-developed much of the statistical modeling software and environment in
  R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the
  lasso and is co-author of the very successful An Introduction to the
  Bootstrap. Friedman is the co-inventor of many data-mining tools including
  CART, MARS, projection pursuit and gradient boosting.%
    }
    \field{isbn}{978-0-387-84858-7}
    \field{shorttitle}{The Elements of Statistical Learning}
    \field{title}{The Elements of Statistical Learning: Data Mining, Inference,
  and Prediction, Second Edition}
    \field{langid}{english}
    \field{month}{08}
    \field{year}{2009}
  \endentry

  \entry{huangStatisticalMechanics1987}{book}{}
    \name{author}{1}{}{%
      {{hash=HK}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Kerson},
         giveni={K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Wiley}%
    }
    \keyw{Science / Mechanics / General,Science / Mechanics / Thermodynamics}
    \strng{namehash}{HK2}
    \strng{fullhash}{HK2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Hua87}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Unlike most other texts on the subject, this clear, concise introduction to
  the theory of microscopic bodies treats the modern theory of critical
  phenomena. Provides up-to-date coverage of recent major advances, including a
  self-contained description of thermodynamics and the classical kinetic theory
  of gases, interesting applications such as superfluids and the quantum Hall
  effect, several current research applications, The last three chapters are
  devoted to the Landau-Wilson approach to critical phenomena. Many new
  problems and illustrations have been added to this edition.%
    }
    \field{isbn}{978-0-471-81518-1}
    \field{title}{Statistical Mechanics}
    \field{langid}{english}
    \field{month}{05}
    \field{year}{1987}
  \endentry

  \entry{jadrichProbabilisticInverseDesign2017}{article}{}
    \name{author}{3}{}{%
      {{hash=JRB}{%
         family={Jadrich},
         familyi={J\bibinitperiod},
         given={R.\bibnamedelima B.},
         giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=LBA}{%
         family={Lindquist},
         familyi={L\bibinitperiod},
         given={B.\bibnamedelima A.},
         giveni={B\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=TTM}{%
         family={Truskett},
         familyi={T\bibinitperiod},
         given={T.\bibnamedelima M.},
         giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{JRBLBATTM1}
    \strng{fullhash}{JRBLBATTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{JLT17}
    \field{sortinit}{J}
    \field{sortinithash}{J}
    \verb{doi}
    \verb 10.1063/1.4981796
    \endverb
    \field{issn}{0021-9606}
    \field{number}{18}
    \field{pages}{184103}
    \field{title}{Probabilistic Inverse Design for Self-Assembling Materials}
    \field{volume}{146}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{05}
    \field{year}{2017}
  \endentry

  \entry{jadrichUnsupervisedMachineLearning2018}{article}{}
    \name{author}{3}{}{%
      {{hash=JRB}{%
         family={Jadrich},
         familyi={J\bibinitperiod},
         given={R.\bibnamedelima B.},
         giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=LBA}{%
         family={Lindquist},
         familyi={L\bibinitperiod},
         given={B.\bibnamedelima A.},
         giveni={B\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=TTM}{%
         family={Truskett},
         familyi={T\bibinitperiod},
         given={T.\bibnamedelima M.},
         giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{JRBLBATTM1}
    \strng{fullhash}{JRBLBATTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{JLT18}
    \field{sortinit}{J}
    \field{sortinithash}{J}
    \verb{doi}
    \verb 10.1063/1.5049849
    \endverb
    \field{issn}{0021-9606}
    \field{number}{19}
    \field{pages}{194109}
    \field{title}{Unsupervised Machine Learning for Detection of Phase
  Transitions in Off-Lattice Systems. I. Foundations}
    \field{volume}{149}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{11}
    \field{year}{2018}
  \endentry

  \entry{karniadakisPhysicsinformedMachineLearning2021a}{article}{}
    \name{author}{6}{}{%
      {{hash=KGE}{%
         family={Karniadakis},
         familyi={K\bibinitperiod},
         given={George\bibnamedelima Em},
         giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=KIG}{%
         family={Kevrekidis},
         familyi={K\bibinitperiod},
         given={Ioannis\bibnamedelima G.},
         giveni={I\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=LL}{%
         family={Lu},
         familyi={L\bibinitperiod},
         given={Lu},
         giveni={L\bibinitperiod},
      }}%
      {{hash=PP}{%
         family={Perdikaris},
         familyi={P\bibinitperiod},
         given={Paris},
         giveni={P\bibinitperiod},
      }}%
      {{hash=WS}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Sifan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=YL}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Liu},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{KGE+1}
    \strng{fullhash}{KGEKIGLLPPWSYL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Kar+21}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Despite great progress in simulating multiphysics problems using the
  numerical discretization of partial differential equations (PDEs), one still
  cannot seamlessly incorporate noisy data into existing algorithms, mesh
  generation remains complex, and high-dimensional problems governed by
  parameterized PDEs cannot be tackled. Moreover, solving inverse problems with
  hidden physics is often prohibitively expensive and requires different
  formulations and elaborate computer codes. Machine learning has emerged as a
  promising alternative, but training deep neural networks requires big data,
  not always available for scientific problems. Instead, such networks can be
  trained from additional information obtained by enforcing the physical laws
  (for example, at random points in the continuous space-time domain). Such
  physics-informed learning integrates (noisy) data and mathematical models,
  and implements them through neural networks or other kernel-based regression
  networks. Moreover, it may be possible to design specialized network
  architectures that automatically satisfy some of the physical invariants for
  better accuracy, faster training and improved generalization. Here, we review
  some of the prevailing trends in embedding physics into machine learning,
  present some of the current capabilities and limitations and discuss diverse
  applications of physics-informed learning both for forward and inverse
  problems, including discovering hidden physics and tackling high-dimensional
  problems.%
    }
    \verb{doi}
    \verb 10.1038/s42254-021-00314-5
    \endverb
    \field{issn}{2522-5820}
    \field{number}{6}
    \field{pages}{422\bibrangedash 440}
    \field{title}{Physics-Informed Machine Learning}
    \field{volume}{3}
    \field{langid}{english}
    \field{journaltitle}{Nature Reviews Physics}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Reviews Subject\_term: Applied mathematics;Computational science
  Subject\_term\_id: applied-mathematics;computational-science%
    }
    \field{month}{06}
    \field{year}{2021}
  \endentry

  \entry{kingmaAdamMethodStochastic2017}{article}{}
    \name{author}{2}{}{%
      {{hash=KDP}{%
         family={Kingma},
         familyi={K\bibinitperiod},
         given={Diederik\bibnamedelima P.},
         giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=BJ}{%
         family={Ba},
         familyi={B\bibinitperiod},
         given={Jimmy},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Machine Learning}
    \strng{namehash}{KDPBJ1}
    \strng{fullhash}{KDPBJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{KB17}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    We introduce Adam, an algorithm for first-order gradient-based optimization
  of stochastic objective functions, based on adaptive estimates of lower-order
  moments. The method is straightforward to implement, is computationally
  efficient, has little memory requirements, is invariant to diagonal rescaling
  of the gradients, and is well suited for problems that are large in terms of
  data and/or parameters. The method is also appropriate for non-stationary
  objectives and problems with very noisy and/or sparse gradients. The
  hyper-parameters have intuitive interpretations and typically require little
  tuning. Some connections to related algorithms, on which Adam was inspired,
  are discussed. We also analyze the theoretical convergence properties of the
  algorithm and provide a regret bound on the convergence rate that is
  comparable to the best known results under the online convex optimization
  framework. Empirical results demonstrate that Adam works well in practice and
  compares favorably to other stochastic optimization methods. Finally, we
  discuss AdaMax, a variant of Adam based on the infinity norm.%
    }
    \verb{eprint}
    \verb 1412.6980
    \endverb
    \field{shorttitle}{Adam}
    \field{title}{Adam: A Method for Stochastic Optimization}
    \field{journaltitle}{arXiv:1412.6980 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{01}
    \field{year}{2017}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{kinoshitaInteractionSurfacesSolvophobicity2003}{article}{}
    \name{author}{1}{}{%
      {{hash=KM}{%
         family={Kinoshita},
         familyi={K\bibinitperiod},
         given={Masahiro},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{KM1}
    \strng{fullhash}{KM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Kin03}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \verb{doi}
    \verb 10.1063/1.1566935
    \endverb
    \field{issn}{0021-9606}
    \field{number}{19}
    \field{pages}{8969\bibrangedash 8981}
    \field{shorttitle}{Interaction between Surfaces with Solvophobicity or
  Solvophilicity Immersed in Solvent}
    \field{title}{Interaction between Surfaces with Solvophobicity or
  Solvophilicity Immersed in Solvent:\hspace{0.6em}Effects Due to Addition of
  Solvophobic or Solvophilic Solute}
    \field{volume}{118}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{05}
    \field{year}{2003}
  \endentry

  \entry{kittelElementaryStatisticalPhysics2004}{book}{}
    \name{author}{1}{}{%
      {{hash=KC}{%
         family={Kittel},
         familyi={K\bibinitperiod},
         given={Charles},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Courier Corporation}%
    }
    \keyw{Science / Physics / General}
    \strng{namehash}{KC1}
    \strng{fullhash}{KC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Kit04}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Noteworthy for the philosophical subtlety of its foundations and the
  elegance of its problem-solving methods, statistical mechanics can be
  employed in a broad range of applications \textemdash{} among them,
  astrophysics, biology, chemistry, nuclear and solid state physics,
  communications engineering, metallurgy, and mathematics. Geared toward
  graduate students in physics, this text covers such important topics as
  stochastic processes and transport theory in order to provide students with a
  working knowledge of statistical mechanics. To explain the fundamentals of
  his subject, the author uses the method of ensembles developed by J. Willard
  Gibbs. Topics include the properties of the Fermi-Dirac and Bose-Einstein
  distributions; the interrelated subjects of fluctuations, thermal noise, and
  Brownian movement; and the thermodynamics of irreversible processes. Negative
  temperature, magnetic energy, density matrix methods, and the Kramers-Kronig
  causality relations are treated briefly. Most sections include illustrative
  problems. Appendix. 28 figures. 1 table.%
    }
    \field{isbn}{978-0-486-43514-5}
    \field{title}{Elementary Statistical Physics}
    \field{langid}{english}
    \field{month}{03}
    \field{year}{2004}
  \endentry

  \entry{kwakEvaluationBridgefunctionDiagrams2005}{article}{}
    \name{author}{2}{}{%
      {{hash=KSK}{%
         family={Kwak},
         familyi={K\bibinitperiod},
         given={Sang\bibnamedelima Kyu},
         giveni={S\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
      {{hash=KDA}{%
         family={Kofke},
         familyi={K\bibinitperiod},
         given={David\bibnamedelima A.},
         giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{KSKKDA1}
    \strng{fullhash}{KSKKDA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{KK05}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \verb{doi}
    \verb 10.1063/1.1860559
    \endverb
    \field{issn}{0021-9606}
    \field{number}{10}
    \field{pages}{104508}
    \field{title}{Evaluation of Bridge-Function Diagrams via Mayer-Sampling
  Monte Carlo Simulation}
    \field{volume}{122}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{03}
    \field{year}{2005}
  \endentry

  \entry{kolafaAccurateEquationState2004}{article}{}
    \name{author}{3}{}{%
      {{hash=KJ}{%
         family={Kolafa},
         familyi={K\bibinitperiod},
         given={Ji{\v r}{\'i}},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Lab{\'i}k},
         familyi={L\bibinitperiod},
         given={Stanislav},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Malijevsk{\'y}},
         familyi={M\bibinitperiod},
         given={Anatol},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {The Royal Society of Chemistry}%
    }
    \strng{namehash}{KJLSMA1}
    \strng{fullhash}{KJLSMA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{KLM04}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    New accurate data on the compressibility factor of the hard sphere fluid
  are obtained by highly optimized molecular dynamics calculations in the range
  of reduced densities 0.20\textendash 1.03. The relative inaccuracy at the
  95\% confidence level is better than 0.00004 for all densities but the last
  deeply metastable point. This accuracy requires careful examination of finite
  size effects and other possible sources of errors and applying corrections.
  The data are fitted to a power series in y/(1 - y), where y is the packing
  fraction; the coefficients are determined so that virial coefficients B2 to
  B6 are reproduced. To do this, values of B5 and B6 are accurately
  recalculated. Virial coefficients up to B11 are then estimated from the
  equation of state.%
    }
    \verb{doi}
    \verb 10.1039/B402792B
    \endverb
    \field{issn}{1463-9084}
    \field{number}{9}
    \field{pages}{2335\bibrangedash 2340}
    \field{title}{Accurate Equation of State of the Hard Sphere Fluid in Stable
  and Metastable Regions}
    \field{volume}{6}
    \field{langid}{english}
    \field{journaltitle}{Physical Chemistry Chemical Physics}
    \field{month}{05}
    \field{year}{2004}
  \endentry

  \entry{kaelblingReinforcementLearningSurvey1996}{article}{}
    \name{author}{3}{}{%
      {{hash=KLP}{%
         family={Kaelbling},
         familyi={K\bibinitperiod},
         given={L.\bibnamedelima P.},
         giveni={L\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=LML}{%
         family={Littman},
         familyi={L\bibinitperiod},
         given={M.\bibnamedelima L.},
         giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
      {{hash=MAW}{%
         family={Moore},
         familyi={M\bibinitperiod},
         given={A.\bibnamedelima W.},
         giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \strng{namehash}{KLPLMLMAW1}
    \strng{fullhash}{KLPLMLMAW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{KLM96}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    This paper surveys the field of reinforcement learning from a
  computer-science perspective. It is written to be accessible to researchers
  familiar with machine learning. Both the historical basis of the field and a
  broad selection of current work are summarized. Reinforcement learning is the
  problem faced by an agent that learns behavior through trial-and-error
  interactions with a dynamic environment. The work described here has a
  resemblance to work in psychology, but differs considerably in the details
  and in the use of the word ``reinforcement.'' The paper discusses central
  issues of reinforcement learning, including trading off exploration and
  exploitation, establishing the foundations of the field via Markov decision
  theory, learning from delayed reinforcement, constructing empirical models to
  accelerate learning, making use of generalization and hierarchy, and coping
  with hidden state. It concludes with a survey of some implemented systems and
  an assessment of the practical utility of current methods for reinforcement
  learning.%
    }
    \verb{doi}
    \verb 10.1613/jair.301
    \endverb
    \field{issn}{1076-9757}
    \field{pages}{237\bibrangedash 285}
    \field{shorttitle}{Reinforcement Learning}
    \field{title}{Reinforcement Learning: A Survey}
    \field{volume}{4}
    \field{langid}{english}
    \field{journaltitle}{Journal of Artificial Intelligence Research}
    \field{month}{05}
    \field{year}{1996}
  \endentry

  \entry{kornerFourierAnalysis1989}{book}{}
    \name{author}{1}{}{%
      {{hash=KTW}{%
         family={K{\"o}rner},
         familyi={K\bibinitperiod},
         given={T.\bibnamedelima W.},
         giveni={T\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cambridge University Press}%
    }
    \keyw{Mathematics / Algebra / Abstract,Mathematics / Functional
  Analysis,Mathematics / Probability \& Statistics / General}
    \strng{namehash}{KTW1}
    \strng{fullhash}{KTW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{K{\"o}r89}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Fourier analysis is a subject that was born in physics but grew up in
  mathematics. Now it is part of the standard repertoire for mathematicians,
  physicists and engineers. In most books, this diversity of interest is often
  ignored, but here Dr K\"orner has provided a shop-window for some of the
  ideas, techniques and elegant results of Fourier analysis, and for their
  applications. These range from number theory, numerical analysis, control
  theory and statistics, to earth science, astronomy, and electrical
  engineering. Each application is placed in perspective by a short essay. The
  prerequisites are few (the reader with knowledge of second or third year
  undergraduate mathematics should have no difficulty following the text), and
  the style is lively and entertaining. In short, this stimulating account will
  be welcomed by all who like to read about more than the bare bones of a
  subject. For them this will be a meaty guide to Fourier analysis.%
    }
    \field{isbn}{978-0-521-38991-4}
    \field{title}{Fourier Analysis}
    \field{langid}{english}
    \field{month}{11}
    \field{year}{1989}
  \endentry

  \entry{kacprzykSpringerHandbookComputational2015}{book}{}
    \name{author}{2}{}{%
      {{hash=KJ}{%
         family={Kacprzyk},
         familyi={K\bibinitperiod},
         given={Janusz},
         giveni={J\bibinitperiod},
      }}%
      {{hash=PW}{%
         family={Pedrycz},
         familyi={P\bibinitperiod},
         given={Witold},
         giveni={W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer}%
    }
    \keyw{Computers / Artificial Intelligence / General,Technology \&
  Engineering / Engineering (General),Technology \& Engineering / General}
    \strng{namehash}{KJPW1}
    \strng{fullhash}{KJPW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{KP15}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    The Springer Handbook for Computational Intelligence is the first book
  covering the basics, the state-of-the-art and important applications of the
  dynamic and rapidly expanding discipline of computational intelligence. This
  comprehensive handbook makes readers familiar with a broad spectrum of
  approaches to solve various problems in science and technology. Possible
  approaches include, for example, those being inspired by biology, living
  organisms and animate systems. Content is organized in seven parts:
  foundations; fuzzy logic; rough sets; evolutionary computation; neural
  networks; swarm intelligence and hybrid computational intelligence systems.
  Each Part is supervised by its own Part Editor(s) so that high-quality
  content as well as completeness are assured.%
    }
    \field{isbn}{978-3-662-43505-2}
    \field{title}{Springer Handbook of Computational Intelligence}
    \field{langid}{english}
    \field{month}{05}
    \field{year}{2015}
  \endentry

  \entry{kroeseWhyMonteCarlo2014}{article}{}
    \name{author}{4}{}{%
      {{hash=KDP}{%
         family={Kroese},
         familyi={K\bibinitperiod},
         given={Dirk\bibnamedelima P.},
         giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=BT}{%
         family={Brereton},
         familyi={B\bibinitperiod},
         given={Tim},
         giveni={T\bibinitperiod},
      }}%
      {{hash=TT}{%
         family={Taimre},
         familyi={T\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=BZI}{%
         family={Botev},
         familyi={B\bibinitperiod},
         given={Zdravko\bibnamedelima I.},
         giveni={Z\bibinitperiod\bibinitdelim I\bibinitperiod},
      }}%
    }
    \keyw{estimation,MCMC,Monte Carlo method,randomized
  optimization,simulation}
    \strng{namehash}{KDP+1}
    \strng{fullhash}{KDPBTTTBZI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Kro+14}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Since the beginning of electronic computing, people have been interested in
  carrying out random experiments on a computer. Such Monte Carlo techniques
  are now an essential ingredient in many quantitative investigations. Why is
  the Monte Carlo method (MCM) so important today? This article explores the
  reasons why the MCM has evolved from a `last resort' solution to a leading
  methodology that permeates much of contemporary science, finance, and
  engineering. WIREs Comput Stat 2014, 6:386\textendash 392. doi:
  10.1002/wics.1314 This article is categorized under: Statistical and
  Graphical Methods of Data Analysis {$>$} Markov Chain Monte Carlo (MCMC)
  Statistical Models {$>$} Simulation Models%
    }
    \verb{doi}
    \verb 10.1002/wics.1314
    \endverb
    \field{issn}{1939-0068}
    \field{number}{6}
    \field{pages}{386\bibrangedash 392}
    \field{title}{Why the Monte Carlo Method Is so Important Today}
    \field{volume}{6}
    \field{langid}{english}
    \field{journaltitle}{WIREs Computational Statistics}
    \field{annotation}{%
    \_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1314%
    }
    \field{year}{2014}
  \endentry

  \entry{krizhevskyImageNetClassificationDeep2012}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KA}{%
         family={Krizhevsky},
         familyi={K\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SI}{%
         family={Sutskever},
         familyi={S\bibinitperiod},
         given={Ilya},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HGE}{%
         family={Hinton},
         familyi={H\bibinitperiod},
         given={Geoffrey\bibnamedelima E.},
         giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates Inc.}%
    }
    \strng{namehash}{KASIHGE1}
    \strng{fullhash}{KASIHGE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{KSH12}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    We trained a large, deep convolutional neural network to classify the 1.2
  million high-resolution images in the ImageNet LSVRC-2010 contest into the
  1000 different classes. On the test data, we achieved top-1 and top-5 error
  rates of 37.5\% and 17.0\% which is considerably better than the previous
  state-of-the-art. The neural network, which has 60 million parameters and
  650,000 neurons, consists of five convolutional layers, some of which are
  followed by max-pooling layers, and three fully-connected layers with a final
  1000-way softmax. To make training faster, we used non-saturating neurons and
  a very efficient GPU implementation of the convolution operation. To reduce
  overriding in the fully-connected layers we employed a recently-developed
  regularization method called "dropout" that proved to be very effective. We
  also entered a variant of this model in the ILSVRC-2012 competition and
  achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\%
  achieved by the second-best entry.%
    }
    \field{booktitle}{Proceedings of the 25th International Conference on
  Neural Information Processing Systems - Volume 1}
    \field{pages}{1097\bibrangedash 1105}
    \field{series}{NIPS'12}
    \field{title}{ImageNet Classification with Deep Convolutional Neural
  Networks}
    \list{location}{1}{%
      {Red Hook, NY, USA}%
    }
    \field{month}{12}
    \field{year}{2012}
  \endentry

  \entry{landauGuideMonteCarlo2021}{book}{}
    \name{author}{2}{}{%
      {{hash=LD}{%
         family={Landau},
         familyi={L\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BK}{%
         family={Binder},
         familyi={B\bibinitperiod},
         given={Kurt},
         giveni={K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cambridge University Press}%
    }
    \keyw{Mathematics / Applied,Science / Physics / Condensed Matter,Science /
  Physics / General,Science / Physics / Mathematical \& Computational}
    \strng{namehash}{LDBK1}
    \strng{fullhash}{LDBK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LB21}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    "Dealing with all aspects of Monte Carlo simulation of complex physical
  systems encountered in condensed-matter physics and statistical mechanics,
  this book provides an introduction to computer simulations in physics. This
  fourth edition contains extensive new material describing numerous powerful
  algorithms not covered in previous editions, in some cases representing new
  developments that have only recently appeared. Older methodologies whose
  impact was previously unclear or unappreciated are also introduced, in
  addition to many small revisions that bring the text and cited literature up
  to date. This edition also introduces the use of petascale computing
  facilities in the Monte Carlo arena"--%
    }
    \field{isbn}{978-1-108-49014-6}
    \field{title}{A Guide to Monte Carlo Simulations in Statistical Physics}
    \field{langid}{english}
    \field{month}{07}
    \field{year}{2021}
  \endentry

  \entry{lecunDeepLearning2015}{article}{}
    \name{author}{3}{}{%
      {{hash=LY}{%
         family={LeCun},
         familyi={L\bibinitperiod},
         given={Yann},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=HG}{%
         family={Hinton},
         familyi={H\bibinitperiod},
         given={Geoffrey},
         giveni={G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{LYBYHG1}
    \strng{fullhash}{LYBYHG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LBH15}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Deep learning allows computational models that are composed of multiple
  processing layers to learn representations of data with multiple levels of
  abstraction. These methods have dramatically improved the state-of-the-art in
  speech recognition, visual object recognition, object detection and many
  other domains such as drug discovery and genomics. Deep learning discovers
  intricate structure in large data sets by using the backpropagation algorithm
  to indicate how a machine should change its internal parameters that are used
  to compute the representation in each layer from the representation in the
  previous layer. Deep convolutional nets have brought about breakthroughs in
  processing images, video, speech and audio, whereas recurrent nets have shone
  light on sequential data such as text and speech.%
    }
    \verb{doi}
    \verb 10.1038/nature14539
    \endverb
    \field{issn}{1476-4687}
    \field{number}{7553}
    \field{pages}{436\bibrangedash 444}
    \field{title}{Deep Learning}
    \field{volume}{521}
    \field{langid}{english}
    \field{journaltitle}{Nature}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Reviews Subject\_term: Computer science;Mathematics and computing
  Subject\_term\_id: computer-science;mathematics-and-computing%
    }
    \field{month}{05}
    \field{year}{2015}
  \endentry

  \entry{lechnerAccurateDeterminationCrystal2008}{article}{}
    \name{author}{2}{}{%
      {{hash=LW}{%
         family={Lechner},
         familyi={L\bibinitperiod},
         given={Wolfgang},
         giveni={W\bibinitperiod},
      }}%
      {{hash=DC}{%
         family={Dellago},
         familyi={D\bibinitperiod},
         given={Christoph},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{LWDC1}
    \strng{fullhash}{LWDC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LD08}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \verb{doi}
    \verb 10.1063/1.2977970
    \endverb
    \field{issn}{0021-9606}
    \field{number}{11}
    \field{pages}{114707}
    \field{title}{Accurate Determination of Crystal Structures Based on
  Averaged Local Bond Order Parameters}
    \field{volume}{129}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{09}
    \field{year}{2008}
  \endentry

  \entry{lecunGradientbasedLearningApplied1998a}{article}{}
    \name{author}{4}{}{%
      {{hash=LY}{%
         family={Lecun},
         familyi={L\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Haffner},
         familyi={H\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{Character recognition,Feature extraction,Hidden Markov models,Machine
  learning,Multi-layer neural network,Neural networks,Optical character
  recognition software,Optical computing,Pattern recognition,Principal
  component analysis}
    \strng{namehash}{LY+1}
    \strng{fullhash}{LYBLBYHP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Lec+98}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Multilayer neural networks trained with the back-propagation algorithm
  constitute the best example of a successful gradient based learning
  technique. Given an appropriate network architecture, gradient-based learning
  algorithms can be used to synthesize a complex decision surface that can
  classify high-dimensional patterns, such as handwritten characters, with
  minimal preprocessing. This paper reviews various methods applied to
  handwritten character recognition and compares them on a standard handwritten
  digit recognition task. Convolutional neural networks, which are specifically
  designed to deal with the variability of 2D shapes, are shown to outperform
  all other techniques. Real-life document recognition systems are composed of
  multiple modules including field extraction, segmentation recognition, and
  language modeling. A new learning paradigm, called graph transformer networks
  (GTN), allows such multimodule systems to be trained globally using
  gradient-based methods so as to minimize an overall performance measure. Two
  systems for online handwriting recognition are described. Experiments
  demonstrate the advantage of global training, and the flexibility of graph
  transformer networks. A graph transformer network for reading a bank cheque
  is also described. It uses convolutional neural network character recognizers
  combined with global training techniques to provide record accuracy on
  business and personal cheques. It is deployed commercially and reads several
  million cheques per day.%
    }
    \verb{doi}
    \verb 10.1109/5.726791
    \endverb
    \field{issn}{1558-2256}
    \field{number}{11}
    \field{pages}{2278\bibrangedash 2324}
    \field{title}{Gradient-Based Learning Applied to Document Recognition}
    \field{volume}{86}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{month}{11}
    \field{year}{1998}
  \endentry

  \entry{leunissenIonicColloidalCrystals2005}{article}{}
    \name{author}{9}{}{%
      {{hash=LME}{%
         family={Leunissen},
         familyi={L\bibinitperiod},
         given={Mirjam\bibnamedelima E.},
         giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=CCG}{%
         family={Christova},
         familyi={C\bibinitperiod},
         given={Christina\bibnamedelima G.},
         giveni={C\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=HAP}{%
         family={Hynninen},
         familyi={H\bibinitperiod},
         given={Antti-Pekka},
         giveni={A\bibinithyphendelim P\bibinitperiod},
      }}%
      {{hash=RCP}{%
         family={Royall},
         familyi={R\bibinitperiod},
         given={C.\bibnamedelima Patrick},
         giveni={C\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=CAI}{%
         family={Campbell},
         familyi={C\bibinitperiod},
         given={Andrew\bibnamedelima I.},
         giveni={A\bibinitperiod\bibinitdelim I\bibinitperiod},
      }}%
      {{hash=IA}{%
         family={Imhof},
         familyi={I\bibinitperiod},
         given={Arnout},
         giveni={A\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
      {{hash=vR}{%
         family={{van Roij}},
         familyi={v\bibinitperiod},
         given={Ren{\'e}},
         giveni={R\bibinitperiod},
      }}%
      {{hash=vA}{%
         family={{van Blaaderen}},
         familyi={v\bibinitperiod},
         given={Alfons},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{LME+1}
    \strng{fullhash}{LMECCGHAPRCPCAIIADMvRvA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Leu+05}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Colloidal suspensions are widely used to study processes such as melting,
  freezing1,2,3 and glass transitions4,5. This is because they display the same
  phase behaviour as atoms or molecules, with the nano- to micrometre size of
  the colloidal particles making it possible to observe them directly in real
  space3,4. Another attractive feature is that different types of colloidal
  interactions, such as long-range repulsive1,3, short-range attractive5,
  hard-sphere-like2,3,4 and dipolar3, can be realized and give rise to
  equilibrium phases. However, spherically symmetric, long-range attractions
  (that is, ionic interactions) have so far always resulted in irreversible
  colloidal aggregation6. Here we show that the electrostatic interaction
  between oppositely charged particles can be tuned such that large ionic
  colloidal crystals form readily, with our theory and simulations confirming
  the stability of these structures. We find that in contrast to atomic
  systems, the stoichiometry of our colloidal crystals is not dictated by
  charge neutrality; this allows us to obtain a remarkable diversity of new
  binary structures. An external electric field melts the crystals, confirming
  that the constituent particles are indeed oppositely charged. Colloidal model
  systems can thus be used to study the phase behaviour of ionic species. We
  also expect that our approach to controlling opposite-charge interactions
  will facilitate the production of binary crystals of micrometre-sized
  particles, which could find use as advanced materials for photonic
  applications7.%
    }
    \verb{doi}
    \verb 10.1038/nature03946
    \endverb
    \field{issn}{1476-4687}
    \field{number}{7056}
    \field{pages}{235\bibrangedash 240}
    \field{title}{Ionic Colloidal Crystals of Oppositely Charged Particles}
    \field{volume}{437}
    \field{langid}{english}
    \field{journaltitle}{Nature}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Research%
    }
    \field{month}{09}
    \field{year}{2005}
  \endentry

  \entry{ladoSolutionsReferencehypernettedchainEquation1983}{article}{}
    \name{author}{3}{}{%
      {{hash=LF}{%
         family={Lado},
         familyi={L\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
      {{hash=FSM}{%
         family={Foiles},
         familyi={F\bibinitperiod},
         given={S.\bibnamedelima M.},
         giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=ANW}{%
         family={Ashcroft},
         familyi={A\bibinitperiod},
         given={N.\bibnamedelima W.},
         giveni={N\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{LFFSMANW1}
    \strng{fullhash}{LFFSMANW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LFA83}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    We use the Rosenfeld-Ashcroft procedure of modeling the bridge function in
  the reference\textemdash hypernetted-chain integral equation with its
  hard-sphere values, and choose the sphere diameter so that the free energy of
  the system is minimized. The resulting integral equation is solved for both
  the long-range Coulomb potential and the short-range Lennard-Jones potential.
  The results are in excellent agreement with Monte Carlo data for the
  thermodynamics and structure of both systems. The method provides an entirely
  first-principles approach to the theory of the structure and thermodynamics
  of simple classical liquids.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevA.28.2374
    \endverb
    \field{number}{4}
    \field{pages}{2374\bibrangedash 2379}
    \field{title}{Solutions of the Reference-Hypernetted-Chain Equation with
  Minimized Free Energy}
    \field{volume}{28}
    \field{journaltitle}{Physical Review A}
    \field{month}{10}
    \field{year}{1983}
  \endentry

  \entry{liDiversityPromotingObjectiveFunction2016}{article}{}
    \name{author}{5}{}{%
      {{hash=LJ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Jiwei},
         giveni={J\bibinitperiod},
      }}%
      {{hash=GM}{%
         family={Galley},
         familyi={G\bibinitperiod},
         given={Michel},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Brockett},
         familyi={B\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
      {{hash=GJ}{%
         family={Gao},
         familyi={G\bibinitperiod},
         given={Jianfeng},
         giveni={J\bibinitperiod},
      }}%
      {{hash=DB}{%
         family={Dolan},
         familyi={D\bibinitperiod},
         given={Bill},
         giveni={B\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computation and Language}
    \strng{namehash}{LJ+1}
    \strng{fullhash}{LJGMBCGJDB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Li+16}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Sequence-to-sequence neural network models for generation of conversational
  responses tend to generate safe, commonplace responses (e.g., "I don't know")
  regardless of the input. We suggest that the traditional objective function,
  i.e., the likelihood of output (response) given input (message) is unsuited
  to response generation tasks. Instead we propose using Maximum Mutual
  Information (MMI) as the objective function in neural models. Experimental
  results demonstrate that the proposed MMI models produce more diverse,
  interesting, and appropriate responses, yielding substantive gains in BLEU
  scores on two conversational datasets and in human evaluations.%
    }
    \verb{eprint}
    \verb 1510.03055
    \endverb
    \field{title}{A Diversity-Promoting Objective Function for Neural
  Conversation Models}
    \field{journaltitle}{arXiv:1510.03055 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{06}
    \field{year}{2016}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{liuCarnahanStarlingTypeEquations2021}{article}{}
    \name{author}{1}{}{%
      {{hash=LH}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Hongqin},
         giveni={H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Taylor \& Francis}%
    }
    \keyw{Equation of state,hard disc,hard sphere,thermodynamics,virial
  coefficients}
    \strng{namehash}{LH1}
    \strng{fullhash}{LH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Liu21}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    The well-known Carnahan-Starling (CS) equation of state (EoS) [N.F.
  Carnahan and K.E. Starling. J. Chem. Phys. 51 (2), 635\textendash 636 (1969).
  doi:10.1063/1.1672048] for hard sphere (HS) fluid was derived from a
  quadratic relationship between the integer portions of the virial
  coefficients, Bn,integer, and their orders, n. In this paper, the method is
  extended to cover the full virial coefficients, Bn, for the general
  D-dimensional case. We propose a (D-1)th order polynomial for the virial
  coefficients starting from the 4th order and EoS's are derived from it. For
  the hard rod (D = 1) case, the exact solution is obtained. For the stable
  hard disk fluid (D = 2), the most recent virial coefficients, up to the 10th
  order, [N. Clisby and B.M. McCoy. J. Stat. Phys. 122 (1), 15\textendash 57
  (2006). doi:10.1007/s10955-005-8080-0] and accurate compressibility data [J.
  Kolafa and M. Rottner. Mol. Phys. 104 (22\textendash 24), 3435\textendash
  3441 (2006). doi:10.1080/00268970600967963; J.J. Erpenbeck and M. Luban.
  Phys. Rev. A. 32 (5), 2920\textendash 2922 (1985).
  doi:10.1103/PhysRevA.32.2920] are employed to construct and to test the EoS.
  For the stable hard sphere (D = 3) fluid, a new CS-type EoS is obtained by
  using the most recent virial coefficients [N. Clisby and B.M. McCoy. J. Stat.
  Phys. 122 (1), 15\textendash 57 (2006). doi:10.1007/s10955-005-8080-0; A.J.
  Schultz and D.A. Kofke. Physical Review E. 90 (2) (2014).
  doi:10.1103/PhysRevE.90.023301], up to the 11th order, along with
  highly-accurate compressibility data [S. Pieprzyk et al. Phys. Chem. Chem.
  Phys. 21 (13), 6886\textendash 6899 (2019). doi:10.1039/C9CP00903E; M.N.
  Bannerman et al. J. Chem. Phys. 132 (8), 084507 (2010).
  doi:10.1063/1.3328823; J. Kolafa, et al. Phys. Chem. Chem. Phys. 6 (9),
  2335\textendash 2340 (2004). doi:10.1039/B402792B]. The simple new EoS's
  prove to be as accurate as the Pad\'e approximations based on all available
  virial coefficients, which significantly improve the accuracy of the CS-type
  EoS in the hard sphere case. This research also reveals that as long as the
  virial coefficients obey a polynomial function, any EoS derived from it will
  diverge at the non-physical packing fraction, {$\eta$}=1.%
    }
    \verb{doi}
    \verb 10.1080/00268976.2021.1886364
    \endverb
    \field{issn}{0026-8976}
    \field{number}{9}
    \field{pages}{e1886364}
    \field{title}{Carnahan-Starling Type Equations of State for Stable Hard
  Disk and Hard Sphere Fluids}
    \field{volume}{119}
    \field{journaltitle}{Molecular Physics}
    \field{annotation}{%
    \_eprint: https://doi.org/10.1080/00268976.2021.1886364%
    }
    \field{month}{05}
    \field{year}{2021}
  \endentry

  \entry{lindquistCommunicationInverseDesign2016}{article}{}
    \name{author}{3}{}{%
      {{hash=LBA}{%
         family={Lindquist},
         familyi={L\bibinitperiod},
         given={Beth\bibnamedelima A.},
         giveni={B\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=JRB}{%
         family={Jadrich},
         familyi={J\bibinitperiod},
         given={Ryan\bibnamedelima B.},
         giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=TTM}{%
         family={Truskett},
         familyi={T\bibinitperiod},
         given={Thomas\bibnamedelima M.},
         giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{LBAJRBTTM1}
    \strng{fullhash}{LBAJRBTTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{LJT16}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Inverse methods of statistical mechanics have facilitated the discovery of
  pair potentials that stabilize a wide variety of targeted lattices at zero
  temperature. However, such methods are complicated by the need to compare,
  within the optimization framework, the energy of the desired lattice to all
  possibly relevant competing structures, which are not generally known in
  advance. Furthermore, ground-state stability does not guarantee that the
  target will readily assemble from the fluid upon cooling from higher
  temperature. Here, we introduce a molecular dynamics simulation-based,
  optimization design strategy that iteratively and systematically refines the
  pair interaction according to the fluid and crystalline structural ensembles
  encountered during the assembly process. We successfully apply this
  probabilistic, machine-learning approach to the design of repulsive,
  isotropic pair potentials that assemble into honeycomb, kagome, square,
  rectangular, truncated square, and truncated hexagonal lattices.%
    }
    \verb{doi}
    \verb 10.1063/1.4962754
    \endverb
    \field{issn}{0021-9606}
    \field{number}{11}
    \field{pages}{111101}
    \field{shorttitle}{Communication}
    \field{title}{Communication: Inverse Design for Self-Assembly via
  on-the-Fly Optimization}
    \field{volume}{145}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{09}
    \field{year}{2016}
  \endentry

  \entry{liMolecularDynamicsOntheFly2015}{article}{}
    \name{author}{3}{}{%
      {{hash=LZ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Zhenwei},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=KJR}{%
         family={Kermode},
         familyi={K\bibinitperiod},
         given={James\bibnamedelima R.},
         giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=DVA}{%
         family={De\bibnamedelima Vita},
         familyi={D\bibinitperiod\bibinitdelim V\bibinitperiod},
         given={Alessandro},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{LZKJRDVA1}
    \strng{fullhash}{LZKJRDVA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LKDV15}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    We present a molecular dynamics scheme which combines first-principles and
  machine-learning (ML) techniques in a single information-efficient approach.
  Forces on atoms are either predicted by Bayesian inference or, if necessary,
  computed by on-the-fly quantum-mechanical (QM) calculations and added to a
  growing ML database, whose completeness is, thus, never required. As a
  result, the scheme is accurate and general, while progressively fewer QM
  calls are needed when a new chemical process is encountered for the second
  and subsequent times, as demonstrated by tests on crystalline and molten
  silicon.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevLett.114.096405
    \endverb
    \field{number}{9}
    \field{pages}{096405}
    \field{title}{Molecular Dynamics with On-the-Fly Machine Learning of
  Quantum-Mechanical Forces}
    \field{volume}{114}
    \field{journaltitle}{Physical Review Letters}
    \field{month}{03}
    \field{year}{2015}
  \endentry

  \entry{libbrechtMachineLearningApplications2015}{article}{}
    \name{author}{2}{}{%
      {{hash=LMW}{%
         family={Libbrecht},
         familyi={L\bibinitperiod},
         given={Maxwell\bibnamedelima W.},
         giveni={M\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=NWS}{%
         family={Noble},
         familyi={N\bibinitperiod},
         given={William\bibnamedelima Stafford},
         giveni={W\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{LMWNWS1}
    \strng{fullhash}{LMWNWS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{LN15}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    The field of machine learning includes the development and application of
  computer algorithms that improve with experience.Machine learning methods can
  be divided into supervised, semi-supervised and unsupervised methods.
  Supervised methods are trained on examples with labels (for example, 'gene'
  or 'not gene') and are then used to predict these labels on other examples,
  whereas unsupervised methods find patterns in data sets without the use of
  labels. Semi-supervised methods combine these two approaches, leveraging
  patterns in unlabelled data to improve power in the prediction of
  labels.Different machine learning methods may be required for an application,
  depending on whether one is interested in interpreting the output model or is
  simply concerned with predictive power. Generative models, which posit a
  probabilistic distribution over input data, are generally best for
  interpretability, whereas discriminative models, which seek only to model
  labels, are generally best for predictive power.Prior information can be
  added to a model in order to train the model more effectively when it is
  provided with limited data, to limit the complexity of the model or to
  incorporate data that are not used by the model directly. Prior information
  can be incorporated explicitly in a probabilistic model or implicitly through
  the choice of features or similarity measures.The choice of an appropriate
  performance measure depends strongly on the application task. Machine
  learning methods are most effective when they optimize an appropriate
  performance measure.Network estimation methods are appropriate when the data
  contain complex dependencies among examples. These methods work best when
  they take into account the confounding effects of indirect relationships.%
    }
    \verb{doi}
    \verb 10.1038/nrg3920
    \endverb
    \field{issn}{1471-0064}
    \field{number}{6}
    \field{pages}{321\bibrangedash 332}
    \field{title}{Machine Learning Applications in Genetics and Genomics}
    \field{volume}{16}
    \field{langid}{english}
    \field{journaltitle}{Nature Reviews Genetics}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Reviews Subject\_term: Genomics;Machine learning;Statistical methods
  Subject\_term\_id: genomics;machine-learning;statistical-methods%
    }
    \field{month}{06}
    \field{year}{2015}
  \endentry

  \entry{lopez-sanchezDemixingTransitionStructure2013a}{article}{}
    \name{author}{6}{}{%
      {{hash=LSE}{%
         family={López-Sánchez},
         familyi={L\bibinithyphendelim S\bibinitperiod},
         given={Erik},
         giveni={E\bibinitperiod},
      }}%
      {{hash=ECD}{%
         family={Estrada-Álvarez},
         familyi={E\bibinithyphendelim \bibinitperiod},
         given={César\bibnamedelima D.},
         giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=PG}{%
         family={Pérez-Ángel},
         familyi={P\bibinithyphendelim \bibinitperiod},
         given={Gabriel},
         giveni={G\bibinitperiod},
      }}%
      {{hash=MAJM}{%
         family={Méndez-Alcaraz},
         familyi={M\bibinithyphendelim A\bibinitperiod},
         given={José\bibnamedelima Miguel},
         giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=GMP}{%
         family={González-Mozuelos},
         familyi={G\bibinithyphendelim M\bibinitperiod},
         given={Pedro},
         giveni={P\bibinitperiod},
      }}%
      {{hash=CPR}{%
         family={Castañeda-Priego},
         familyi={C\bibinithyphendelim P\bibinitperiod},
         given={Ramón},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{LSE+1}
    \strng{fullhash}{LSEECDPGMAJMGMPCPR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{LS+13}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \verb{doi}
    \verb 10.1063/1.4820559
    \endverb
    \field{issn}{0021-9606}
    \field{number}{10}
    \field{pages}{104908}
    \field{shorttitle}{Demixing Transition, Structure, and Depletion Forces in
  Binary Mixtures of Hard-Spheres}
    \field{title}{Demixing Transition, Structure, and Depletion Forces in
  Binary Mixtures of Hard-Spheres: The Role of Bridge Functions}
    \field{volume}{139}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{09}
    \field{year}{2013}
  \endentry

  \entry{mcquarrieStatisticalMechanics2000}{book}{}
    \name{author}{1}{}{%
      {{hash=MDA}{%
         family={McQuarrie},
         familyi={M\bibinitperiod},
         given={Donald\bibnamedelima A.},
         giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {University Science Books}%
    }
    \keyw{Science / Chemistry / Physical \& Theoretical,Science / Mechanics /
  General,Science / Physics / General,Science / Physics / Mathematical \&
  Computational}
    \strng{namehash}{MDA1}
    \strng{fullhash}{MDA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{McQ00}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    The canonical ensemble - Other ensembles and fluctuations - Boltzmann
  statistics, fermi-dirac statistics, and bose-einstein statistics - Ideal
  monatomic gas - Ideal diatomic - Classical statistical mechanics - Ideal
  polyatomic - Chemical equilibrium - Quantum statistics - Crystals - Imperfect
  gases - Distribution functions in classical monatomic liquids - Perturbation
  theories of liquids - Solutions of strong electrolytes - Kinetic theory of
  gases and molecular collisions - Continuum mechanics - Kinetic theory
  of-gases and the boltzmann equation - Transport processes in dilute gases -
  Theory of brownian motion - The time-correlation function formalism.%
    }
    \field{isbn}{978-1-891389-15-3}
    \field{title}{Statistical Mechanics}
    \field{langid}{english}
    \field{month}{06}
    \field{year}{2000}
  \endentry

  \entry{mehligMachineLearningNeural2021}{article}{}
    \name{author}{1}{}{%
      {{hash=MB}{%
         family={Mehlig},
         familyi={M\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Machine Learning,Condensed Matter - Statistical
  Mechanics,Statistics - Machine Learning}
    \strng{namehash}{MB1}
    \strng{fullhash}{MB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Meh21}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Lecture notes for my course on machine learning with neural networks that I
  have given at Gothenburg University and Chalmers Technical University in
  Gothenburg, Sweden.%
    }
    \verb{eprint}
    \verb 1901.05639
    \endverb
    \field{title}{Machine Learning with Neural Networks}
    \field{journaltitle}{arXiv:1901.05639 [cond-mat, stat]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cond-mat, stat}
    \field{month}{02}
    \field{year}{2021}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{malijevskyBridgeFunctionHard1987}{article}{}
    \name{author}{2}{}{%
      {{hash=MA}{%
         family={Malijevsk{\'y}},
         familyi={M\bibinitperiod},
         given={Anatol},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Lab{\'i}k},
         familyi={L\bibinitperiod},
         given={Stanislav},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Taylor \& Francis}%
    }
    \strng{namehash}{MALS1}
    \strng{fullhash}{MALS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{ML87}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    The paper presents an empirical formula for expressing the bridge function
  (the sum of elementary graphs) in terms of the interparticle separation and
  the density. The formulae is fully consistent with the best
  computer-simulation thermodynamic and structural data for hard spheres in the
  fluid region. It can serve as both a direct and convenient testing ground for
  the integral-equation theories of hard spheres and an input to the
  reference-hypernetted chain approximation for simple fluids.%
    }
    \verb{doi}
    \verb 10.1080/00268978700100441
    \endverb
    \field{issn}{0026-8976}
    \field{number}{3}
    \field{pages}{663\bibrangedash 669}
    \field{title}{The Bridge Function for Hard Spheres}
    \field{volume}{60}
    \field{journaltitle}{Molecular Physics}
    \field{annotation}{%
    \_eprint: https://doi.org/10.1080/00268978700100441%
    }
    \field{month}{02}
    \field{year}{1987}
  \endentry

  \entry{mccoyAssessmentTimeSeriesMachine2018}{article}{}
    \name{author}{3}{}{%
      {{hash=MTHJ}{%
         family={McCoy},
         familyi={M\bibinitperiod},
         suffix={Jr},
         suffixi={J\bibinitperiod},
         given={Thomas\bibnamedelima H.},
         giveni={T\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=PAM}{%
         family={Pellegrini},
         familyi={P\bibinitperiod},
         given={Amelia\bibnamedelima M.},
         giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=PRH}{%
         family={Perlis},
         familyi={P\bibinitperiod},
         given={Roy\bibnamedelima H.},
         giveni={R\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \strng{namehash}{MJTHPAMPRH1}
    \strng{fullhash}{MJTHPAMPRH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{MPP18}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Forecasting the volume of hospital discharges has important implications
  for resource allocation and represents an opportunity to improve patient
  safety at periods of elevated risk.To determine the performance of a new
  time-series machine learning method for forecasting hospital discharge volume
  compared with simpler methods.A retrospective cohort study of daily hospital
  discharge volumes at 2 large, New England academic medical centers between
  January 1, 2005, and December 31, 2014 (hospital 1), or January 1, 2005, and
  December 31, 2010 (hospital 2), comparing time-series forecasting methods for
  prediction was performed. Data analysis was conducted from February 28, 2017,
  to August 30, 2018. Group-level data for all discharges from inpatient units
  were included. In addition to conventional methods, a technique originally
  developed for allocating data center resources, and comparison strategies for
  incorporating prior data and frequency of model updates, was conducted to
  identify the model application that optimized forecast accuracy.Model
  calibration as measured by R2 and, secondarily, number of days with errors
  greater than 1 SD of daily volume.During the forecasted year, hospital 1 had
  54 411 discharges (daily mean, 149) and hospital 2 had 47 456 discharges
  (daily mean, 130). The machine learning method was well calibrated at both
  sites (R2, 0.843 and 0.726, respectively) and made errors greater than 1 SD
  of daily volume on only 13 and 22 days, respectively, of the forecast year at
  the 2 sites. Last-value-carried-forward models performed somewhat less well
  (calibration R2, 0.781 and 0.596, respectively) with 13 and 46 errors of 1 SD
  or greater, respectively. More frequent retraining and training sets of
  longer than 1 year had minimal effects on the machine learning method's
  performance.Volume of hospital discharges can perhaps be reliably forecasted
  using simple carry-forward models as well as methods drawn from machine
  learning. The benefit of the latter does not appear to be dependent on
  extensive training data and may enable forecasts up to 1 year in advance with
  superior absolute accuracy to carry-forward models.%
    }
    \verb{doi}
    \verb 10.1001/jamanetworkopen.2018.4087
    \endverb
    \field{issn}{2574-3805}
    \field{number}{7}
    \field{pages}{e184087}
    \field{title}{Assessment of Time-Series Machine Learning Methods for
  Forecasting Hospital Discharge Volume}
    \field{volume}{1}
    \field{journaltitle}{JAMA Network Open}
    \field{month}{11}
    \field{year}{2018}
  \endentry

  \entry{mcdonaldCalculationThermodynamicProperties1967}{article}{}
    \name{author}{2}{}{%
      {{hash=MIR}{%
         family={McDonald},
         familyi={M\bibinitperiod},
         given={I.\bibnamedelima R.},
         giveni={I\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Singer},
         familyi={S\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {The Royal Society of Chemistry}%
    }
    \strng{namehash}{MIRSK1}
    \strng{fullhash}{MIRSK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{MS67}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Extensions of the Monte Carlo method of Metropolis et al. are described
  which permit both isochoric and isothermal extrapolations of Monte Carlo
  data. Thermodynamic properties are calculated for liquid argon from
  Lennard-Jones parameters for a wide V, T-range. The agreement between
  calculated and experimental values is, on the whole, satisfactory.%
    }
    \verb{doi}
    \verb 10.1039/DF9674300040
    \endverb
    \field{issn}{0366-9033}
    \field{number}{0}
    \field{pages}{40\bibrangedash 49}
    \field{title}{Calculation of Thermodynamic Properties of Liquid Argon from
  Lennard-Jones Parameters by a Monte Carlo Method}
    \field{volume}{43}
    \field{langid}{english}
    \field{journaltitle}{Discussions of the Faraday Society}
    \field{month}{01}
    \field{year}{1967}
  \endentry

  \entry{murphyMachineLearningProbabilistic2012}{book}{}
    \name{author}{1}{}{%
      {{hash=MKP}{%
         family={Murphy},
         familyi={M\bibinitperiod},
         given={Kevin\bibnamedelima P.},
         giveni={K\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{Computers / Artificial Intelligence / General,Computers / Machine
  Theory}
    \strng{namehash}{MKP1}
    \strng{fullhash}{MKP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Mur12}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    A comprehensive introduction to machine learning that uses probabilistic
  models and inference as a unifying approach.Today's Web-enabled deluge of
  electronic data calls for automated methods of data analysis. Machine
  learning provides these, developing methods that can automatically detect
  patterns in data and then use the uncovered patterns to predict future data.
  This textbook offers a comprehensive and self-contained introduction to the
  field of machine learning, based on a unified, probabilistic approach.The
  coverage combines breadth and depth, offering necessary background material
  on such topics as probability, optimization, and linear algebra as well as
  discussion of recent developments in the field, including conditional random
  fields, L1 regularization, and deep learning. The book is written in an
  informal, accessible style, complete with pseudo-code for the most important
  algorithms. All topics are copiously illustrated with color images and worked
  examples drawn from such application domains as biology, text processing,
  computer vision, and robotics. Rather than providing a cookbook of different
  heuristic methods, the book stresses a principled model-based approach, often
  using the language of graphical models to specify models in a concise and
  intuitive way. Almost all the models described have been implemented in a
  MATLAB software package\textemdash PMTK (probabilistic modeling
  toolkit)\textemdash that is freely available online. The book is suitable for
  upper-level undergraduates with an introductory-level college math background
  and beginning graduate students.%
    }
    \field{isbn}{978-0-262-01802-9}
    \field{shorttitle}{Machine Learning}
    \field{title}{Machine Learning: A Probabilistic Perspective}
    \field{langid}{english}
    \field{month}{08}
    \field{year}{2012}
  \endentry

  \entry{neyshaburRoleOverparametrizationGeneralization2018}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=NB}{%
         family={Neyshabur},
         familyi={N\bibinitperiod},
         given={Behnam},
         giveni={B\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Zhiyuan},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=BS}{%
         family={Bhojanapalli},
         familyi={B\bibinitperiod},
         given={Srinadh},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={LeCun},
         familyi={L\bibinitperiod},
         given={Yann},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Srebro},
         familyi={S\bibinitperiod},
         given={Nathan},
         giveni={N\bibinitperiod},
      }}%
    }
    \strng{namehash}{NB+1}
    \strng{fullhash}{NBLZBSLYSN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ney+18}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{abstract}{%
    We suggest a generalization bound that could partly explain the improvement
  in generalization with over-parametrization.%
    }
    \field{booktitle}{International Conference on Learning Representations}
    \field{title}{The Role of Over-Parametrization in Generalization of Neural
  Networks}
    \field{langid}{english}
    \field{month}{09}
    \field{year}{2018}
  \endentry

  \entry{ngHypernettedChainSolutions1974}{article}{}
    \name{author}{1}{}{%
      {{hash=NKC}{%
         family={Ng},
         familyi={N\bibinitperiod},
         given={Kin-Chue},
         giveni={K\bibinithyphendelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{NKC1}
    \strng{fullhash}{NKC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ng74}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \verb{doi}
    \verb 10.1063/1.1682399
    \endverb
    \field{issn}{0021-9606}
    \field{number}{7}
    \field{pages}{2680\bibrangedash 2689}
    \field{title}{Hypernetted Chain Solutions for the Classical One-component
  Plasma up to {$\Gamma$}=7000}
    \field{volume}{61}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{10}
    \field{year}{1974}
  \endentry

  \entry{nielsenNeuralNetworksDeep2015}{book}{}
    \name{author}{1}{}{%
      {{hash=NM}{%
         family={Nielsen},
         familyi={N\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{NM1}
    \strng{fullhash}{NM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Nie15}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{title}{Neural Networks and Deep Learning}
    \field{year}{2015}
  \endentry

  \entry{nomuraDistanceweightedExponentialNatural2021}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=NM}{%
         family={Nomura},
         familyi={N\bibinitperiod},
         given={Masahiro},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Sakai},
         familyi={S\bibinitperiod},
         given={Nobuyuki},
         giveni={N\bibinitperiod},
      }}%
      {{hash=FN}{%
         family={Fukushima},
         familyi={F\bibinitperiod},
         given={Nobusumi},
         giveni={N\bibinitperiod},
      }}%
      {{hash=OI}{%
         family={Ono},
         familyi={O\bibinitperiod},
         given={Isao},
         giveni={I\bibinitperiod},
      }}%
    }
    \keyw{Benchmark testing,Estimation,Evolutionary computation,Gaussian
  distribution,Implicit Constraint,Linear programming,Natural Evolution
  Strategy,Optimization,Probability distribution}
    \strng{namehash}{NM+1}
    \strng{fullhash}{NMSNFNOI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Nom+21}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{abstract}{%
    This paper presents a natural evolution strategy for implicitly constrained
  black-box function optimization. The black-box function optimization is
  challenging because explicit representations of objective functions are not
  given, and only evaluation values of solutions can be used. In implicitly
  constrained black-box function problems, constraints are not explicitly
  given, and only the feasibility of a solution is obtained when the objective
  function is evaluated. In other words, the amount of constraint violation
  cannot be obtained, which makes the optimization difficult. Natural Evolution
  Strategies (NES) is one of the promising frameworks for black-box function
  optimization. DX-NES is an improved version of xNES which is a promising NES
  using a multivariate normal distribution as the probability distribution.
  DX-NES has been reported to show good performance on unconstrained black-box
  function optimization problems. However, DX-NES has a serious problem in that
  its performance degrades when applied to implicitly constrained problems. In
  order to address the problem, we propose DX-NES taking account of Implicit
  Constraint (DX-NES-IC). In experiments using benchmark problems and a lens
  system design problem, DX-NES-IC showed better performance than DX-NES, xNES,
  CMA-ES, and those with the resampling technique in terms of the number of
  successful trials and that of evaluations, where the resampling technique is
  a constraint handling method which can be used for implicitly constrained
  problems.%
    }
    \field{booktitle}{2021 IEEE Congress on Evolutionary Computation (CEC)}
    \verb{doi}
    \verb 10.1109/CEC45853.2021.9504865
    \endverb
    \field{pages}{1099\bibrangedash 1106}
    \field{title}{Distance-Weighted Exponential Natural Evolution Strategy for
  Implicitly Constrained Black-Box Function Optimization}
    \field{month}{06}
    \field{year}{2021}
  \endentry

  \entry{nocedalNumericalOptimization2006}{book}{}
    \name{author}{2}{}{%
      {{hash=NJ}{%
         family={Nocedal},
         familyi={N\bibinitperiod},
         given={Jorge},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WS}{%
         family={Wright},
         familyi={W\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer-Verlag}%
    }
    \strng{namehash}{NJWS1}
    \strng{fullhash}{NJWS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{NW06}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{abstract}{%
    Numerical Optimization presents a comprehensive and up-to-date description
  of the most effective methods in continuous optimization. It responds to the
  growing interest in optimization in engineering, science, and business by
  focusing on the methods that are best suited to practical problems. For this
  new edition the book has been thoroughly updated throughout. There are new
  chapters on nonlinear interior methods and derivative-free methods for
  optimization, both of which are used widely in practice and the focus of much
  current research. Because of the emphasis on practical methods, as well as
  the extensive illustrations and exercises, the book is accessible to a wide
  audience. It can be used as a graduate text in engineering, operations
  research, mathematics, computer science, and business. It also serves as a
  handbook for researchers and practitioners in the field. The authors have
  strived to produce a text that is pleasant to read, informative, and rigorous
  - one that reveals both the beautiful nature of the discipline and its
  practical side.%
    }
    \verb{doi}
    \verb 10.1007/978-0-387-40065-5
    \endverb
    \field{edition}{Second}
    \field{isbn}{978-0-387-30303-1}
    \field{series}{Springer Series in Operations Research and Financial
  Engineering}
    \field{title}{Numerical Optimization}
    \field{langid}{english}
    \list{location}{1}{%
      {New York}%
    }
    \field{year}{2006}
  \endentry

  \entry{nwankpaActivationFunctionsComparison2018}{article}{}
    \name{author}{4}{}{%
      {{hash=NC}{%
         family={Nwankpa},
         familyi={N\bibinitperiod},
         given={Chigozie},
         giveni={C\bibinitperiod},
      }}%
      {{hash=IW}{%
         family={Ijomah},
         familyi={I\bibinitperiod},
         given={Winifred},
         giveni={W\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Gachagan},
         familyi={G\bibinitperiod},
         given={Anthony},
         giveni={A\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Marshall},
         familyi={M\bibinitperiod},
         given={Stephen},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer
  Science - Machine Learning}
    \strng{namehash}{NC+1}
    \strng{fullhash}{NCIWGAMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Nwa+18}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{abstract}{%
    Deep neural networks have been successfully used in diverse emerging
  domains to solve real world complex problems with may more deep learning(DL)
  architectures, being developed to date. To achieve these state-of-the-art
  performances, the DL architectures use activation functions (AFs), to perform
  diverse computations between the hidden layers and the output layers of any
  given DL architecture. This paper presents a survey on the existing AFs used
  in deep learning applications and highlights the recent trends in the use of
  the activation functions for deep learning applications. The novelty of this
  paper is that it compiles majority of the AFs used in DL and outlines the
  current trends in the applications and usage of these functions in practical
  deep learning deployments against the state-of-the-art research results. This
  compilation will aid in making effective decisions in the choice of the most
  suitable and appropriate activation function for any given application, ready
  for deployment. This paper is timely because most research papers on AF
  highlights similar works and results while this paper will be the first, to
  compile the trends in AF applications in practice against the research
  results from literature, found in deep learning research to date.%
    }
    \verb{eprint}
    \verb 1811.03378
    \endverb
    \field{shorttitle}{Activation Functions}
    \field{title}{Activation Functions: Comparison of Trends in Practice and
  Research for Deep Learning}
    \field{journaltitle}{arXiv:1811.03378 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{11}
    \field{year}{2018}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{parkMinimumWidthUniversal2020}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=PS}{%
         family={Park},
         familyi={P\bibinitperiod},
         given={Sejun},
         giveni={S\bibinitperiod},
      }}%
      {{hash=YC}{%
         family={Yun},
         familyi={Y\bibinitperiod},
         given={Chulhee},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={Jaeho},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Shin},
         familyi={S\bibinitperiod},
         given={Jinwoo},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{PS+1}
    \strng{fullhash}{PSYCLJSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Par+20}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    The universal approximation property of width-bounded networks has been
  studied as a dual of classical universal approximation results on
  depth-bounded networks. However, the critical width...%
    }
    \field{booktitle}{International Conference on Learning Representations}
    \field{title}{Minimum Width for Universal Approximation}
    \field{langid}{english}
    \field{month}{09}
    \field{year}{2020}
  \endentry

  \entry{pittsHowWeKnow1947}{article}{}
    \name{author}{2}{}{%
      {{hash=PW}{%
         family={Pitts},
         familyi={P\bibinitperiod},
         given={Walter},
         giveni={W\bibinitperiod},
      }}%
      {{hash=MWS}{%
         family={McCulloch},
         familyi={M\bibinitperiod},
         given={Warren\bibnamedelima S.},
         giveni={W\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \strng{namehash}{PWMWS1}
    \strng{fullhash}{PWMWS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{PM47}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Two neural mechanisms are described which exhibit recognition of forms.
  Both are independent of small perturbations at synapses of excitation,
  threshold, and synchrony, and are referred to partiular appropriate regions
  of the nervous system, thus suggesting experimental verification. The first
  mechanism averages an apparition over a group, and in the treatment of this
  mechanism it is suggested that scansion plays a significant part. The second
  mechanism reduces an apparition to a standard selected from among its many
  legitimate presentations. The former mechanism is exemplified by the
  recognition of chords regardless of pitch and shapes regardless of size. The
  latter is exemplified here only in the reflexive mechanism translating
  apparitions to the fovea. Both are extensions to contemporaneous functions of
  the knowing of universals heretofore treated by the authors only with respect
  to sequence in time.%
    }
    \verb{doi}
    \verb 10.1007/BF02478291
    \endverb
    \field{issn}{1522-9602}
    \field{number}{3}
    \field{pages}{127\bibrangedash 147}
    \field{title}{How We Know Universals the Perception of Auditory and Visual
  Forms}
    \field{volume}{9}
    \field{langid}{english}
    \field{journaltitle}{The bulletin of mathematical biophysics}
    \field{month}{09}
    \field{year}{1947}
  \endentry

  \entry{powellUOBYQAUnconstrainedOptimization2002}{article}{}
    \name{author}{1}{}{%
      {{hash=PM}{%
         family={Powell},
         familyi={P\bibinitperiod},
         given={M.J.D.},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{PM1}
    \strng{fullhash}{PM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Pow02}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    UOBYQA is a new algorithm for general unconstrained optimization
  calculations, that takes account of the curvature of the objective function,
  F say, by forming quadratic models by interpolation. Therefore, because no
  first derivatives are required, each model is defined by
  {$\frac{1}{2}$}(n+1)(n+2) values of F, where n is the number of variables,
  and the interpolation points must have the property that no nonzero quadratic
  polynomial vanishes at all of them. A typical iteration of the algorithm
  generates a new vector of variables, \$\textbackslash
  widetilde\{\textbackslash underline\{x\}\}\$tsay, either by minimizing the
  quadratic model subject to a trust region bound, or by a procedure that
  should improve the accuracy of the model. Then usually F(\$\textbackslash
  widetilde\{\textbackslash underline\{x\}\}\$t) is obtained, and one of the
  interpolation points is replaced by \$\textbackslash
  widetilde\{\textbackslash underline\{x\}\}\$t. Therefore the paper addresses
  the initial positions of the interpolation points, the adjustment of trust
  region radii, the calculation of \$\textbackslash widetilde\{\textbackslash
  underline\{x\}\}\$tin the two cases that have been mentioned, and the
  selection of the point to be replaced. Further, UOBYQA works with the
  Lagrange functions of the interpolation equations explicitly, so their
  coefficients are updated when an interpolation point is moved. The Lagrange
  functions assist the procedure that improves the model, and also they provide
  an estimate of the error of the quadratic approximation to F, which allows
  the algorithm to achieve a fast rate of convergence. These features are
  discussed and a summary of the algorithm is given. Finally, a Fortran
  implementation of UOBYQA is applied to several choices of F, in order to
  investigate accuracy, robustness in the presence of rounding errors, the
  effects of first derivative discontinuities, and the amount of work. The
  numerical results are very promising for n{$\leq$}20, but larger values are
  problematical, because the routine work of an iteration is of fourth order in
  the number of variables.%
    }
    \verb{doi}
    \verb 10.1007/s101070100290
    \endverb
    \field{issn}{1436-4646}
    \field{number}{3}
    \field{pages}{555\bibrangedash 582}
    \field{shorttitle}{UOBYQA}
    \field{title}{UOBYQA: Unconstrained Optimization by Quadratic
  Approximation}
    \field{volume}{92}
    \field{langid}{english}
    \field{journaltitle}{Mathematical Programming}
    \field{month}{05}
    \field{year}{2002}
  \endentry

  \entry{puseyPhaseBehaviourConcentrated1986}{article}{}
    \name{author}{2}{}{%
      {{hash=PPN}{%
         family={Pusey},
         familyi={P\bibinitperiod},
         given={P.\bibnamedelima N.},
         giveni={P\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=vW}{%
         family={{van Megen}},
         familyi={v\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{PPNvW1}
    \strng{fullhash}{PPNvW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Pv86}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Suspensions of spherical colloidal particles in a liquid show a fascinating
  variety of phase behaviour which can mimic that of simple atomic liquids and
  solids. `Colloidal fluids'1\textendash 4, in which there are significant
  short-range correlations between the positions of neighbouring particles, and
  `colloidal crystals'5\textendash 7, which have long-range spatial order, have
  been investigated extensively. We report here a detailed study of the phase
  diagram of suspensions of colloidal spheres which interact through a steep
  repulsive potential. With increasing particle concentration we observed a
  progression from colloidal fluid, to fluid and crystal phases in coexistence,
  to fully crystallized samples. At the highest concentrations we obtained very
  viscous samples in which full crystallization had not occurred after several
  months and in which the particles appeared to be arranged as an amorphous
  `colloidal glass'. The empirical phase diagram can be reproduced reasonably
  well by an effective hard-sphere model. The observation of the colloidal
  glass phase is interesting both in itself and because of possible relevance
  to the manufacture of high-strength ceramics8.%
    }
    \verb{doi}
    \verb 10.1038/320340a0
    \endverb
    \field{issn}{1476-4687}
    \field{number}{6060}
    \field{pages}{340\bibrangedash 342}
    \field{title}{Phase Behaviour of Concentrated Suspensions of Nearly Hard
  Colloidal Spheres}
    \field{volume}{320}
    \field{langid}{english}
    \field{journaltitle}{Nature}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Research%
    }
    \field{month}{03}
    \field{year}{1986}
  \endentry

  \entry{percusAnalysisClassicalStatistical1958}{article}{}
    \name{author}{2}{}{%
      {{hash=PJK}{%
         family={Percus},
         familyi={P\bibinitperiod},
         given={Jerome\bibnamedelima K.},
         giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
      {{hash=YGJ}{%
         family={Yevick},
         familyi={Y\bibinitperiod},
         given={George\bibnamedelima J.},
         giveni={G\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{PJKYGJ1}
    \strng{fullhash}{PJKYGJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{PY58}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    The three-dimensional classical many-body system is approximated by the use
  of collective coordinates, through the assumed knowledge of two-body
  correlation functions. The resulting approximate statistical state is used to
  obtain the two-body correlation function. Thus, a self-consistent formulation
  is available for determining the correlation function. Then, the
  self-consistent integral equation is solved in virial expansion, and the
  thermodynamic quantities of the system thereby ascertained. The first three
  virial coefficients are exactly reproduced, while the fourth is nearly
  correct, as evidenced by numerical results for the case of hard spheres.%
    }
    \verb{doi}
    \verb 10.1103/PhysRev.110.1
    \endverb
    \field{number}{1}
    \field{pages}{1\bibrangedash 13}
    \field{title}{Analysis of Classical Statistical Mechanics by Means of
  Collective Coordinates}
    \field{volume}{110}
    \field{journaltitle}{Physical Review}
    \field{month}{04}
    \field{year}{1958}
  \endentry

  \entry{radovicMachineLearningEnergy2018}{article}{}
    \name{author}{9}{}{%
      {{hash=RA}{%
         family={Radovic},
         familyi={R\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=WM}{%
         family={Williams},
         familyi={W\bibinitperiod},
         given={Mike},
         giveni={M\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rousseau},
         familyi={R\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=KM}{%
         family={Kagan},
         familyi={K\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BD}{%
         family={Bonacorsi},
         familyi={B\bibinitperiod},
         given={Daniele},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Himmel},
         familyi={H\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={Aurisano},
         familyi={A\bibinitperiod},
         given={Adam},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TK}{%
         family={Terao},
         familyi={T\bibinitperiod},
         given={Kazuhiro},
         giveni={K\bibinitperiod},
      }}%
      {{hash=WT}{%
         family={Wongjirad},
         familyi={W\bibinitperiod},
         given={Taritree},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{RA+1}
    \strng{fullhash}{RAWMRDKMBDHAAATKWT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Rad+18}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    Our knowledge of the fundamental particles of nature and their interactions
  is summarized by the standard model of particle physics. Advancing our
  understanding in this field has required experiments that operate at ever
  higher energies and intensities, which produce extremely large and
  information-rich data samples. The use of machine-learning techniques is
  revolutionizing how we interpret these data samples, greatly increasing the
  discovery potential of present and future experiments. Here we summarize the
  challenges and opportunities that come with the use of machine learning at
  the frontiers of particle physics.%
    }
    \verb{doi}
    \verb 10.1038/s41586-018-0361-2
    \endverb
    \field{issn}{1476-4687}
    \field{number}{7716}
    \field{pages}{41\bibrangedash 48}
    \field{title}{Machine Learning at the Energy and Intensity Frontiers of
  Particle Physics}
    \field{volume}{560}
    \field{langid}{english}
    \field{journaltitle}{Nature}
    \field{annotation}{%
    Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype:
  Reviews Subject\_term: Computer science;Experimental particle physics
  Subject\_term\_id: computer-science;experimental-particle-physics%
    }
    \field{month}{08}
    \field{year}{2018}
  \endentry

  \entry{redmonYOLOv3IncrementalImprovement2018}{article}{}
    \name{author}{2}{}{%
      {{hash=RJ}{%
         family={Redmon},
         familyi={R\bibinitperiod},
         given={Joseph},
         giveni={J\bibinitperiod},
      }}%
      {{hash=FA}{%
         family={Farhadi},
         familyi={F\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \strng{namehash}{RJFA1}
    \strng{fullhash}{RJFA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{RF18}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    We present some updates to YOLO! We made a bunch of little design changes
  to make it better. We also trained this new network that's pretty swell. It's
  a little bigger than last time but more accurate. It's still fast though,
  don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD
  but three times faster. When we look at the old .5 IOU mAP detection metric
  YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared
  to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster.
  As always, all the code is online at https://pjreddie.com/yolo/%
    }
    \verb{eprint}
    \verb 1804.02767
    \endverb
    \field{shorttitle}{YOLOv3}
    \field{title}{YOLOv3: An Incremental Improvement}
    \field{journaltitle}{arXiv:1804.02767 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{04}
    \field{year}{2018}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{ronnebergerUNetConvolutionalNetworks2015}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=RO}{%
         family={Ronneberger},
         familyi={R\bibinitperiod},
         given={Olaf},
         giveni={O\bibinitperiod},
      }}%
      {{hash=FP}{%
         family={Fischer},
         familyi={F\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
      {{hash=BT}{%
         family={Brox},
         familyi={B\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=NN}{%
         family={Navab},
         familyi={N\bibinitperiod},
         given={Nassir},
         giveni={N\bibinitperiod},
      }}%
      {{hash=HJ}{%
         family={Hornegger},
         familyi={H\bibinitperiod},
         given={Joachim},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WWM}{%
         family={Wells},
         familyi={W\bibinitperiod},
         given={William\bibnamedelima M.},
         giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=FAF}{%
         family={Frangi},
         familyi={F\bibinitperiod},
         given={Alejandro\bibnamedelima F.},
         giveni={A\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \keyw{Convolutional Layer,Data Augmentation,Deep Network,Ground Truth
  Segmentation,Training Image}
    \strng{namehash}{ROFPBT1}
    \strng{fullhash}{ROFPBT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{RFB15}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    There is large consent that successful training of deep networks requires
  many thousand annotated training samples. In this paper, we present a network
  and training strategy that relies on the strong use of data augmentation to
  use the available annotated samples more efficiently. The architecture
  consists of a contracting path to capture context and a symmetric expanding
  path that enables precise localization. We show that such a network can be
  trained end-to-end from very few images and outperforms the prior best method
  (a sliding-window convolutional network) on the ISBI challenge for
  segmentation of neuronal structures in electron microscopic stacks. Using the
  same network trained on transmitted light microscopy images (phase contrast
  and DIC) we won the ISBI cell tracking challenge 2015 in these categories by
  a large margin. Moreover, the network is fast. Segmentation of a 512x512
  image takes less than a second on a recent GPU. The full implementation
  (based on Caffe) and the trained networks are available at
  http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .%
    }
    \field{booktitle}{Medical Image Computing and Computer-Assisted
  Intervention \textendash{} MICCAI 2015}
    \verb{doi}
    \verb 10.1007/978-3-319-24574-4_28
    \endverb
    \field{isbn}{978-3-319-24574-4}
    \field{pages}{234\bibrangedash 241}
    \field{series}{Lecture Notes in Computer Science}
    \field{shorttitle}{U-Net}
    \field{title}{U-Net: Convolutional Networks for Biomedical Image
  Segmentation}
    \field{langid}{english}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2015}
  \endentry

  \entry{razafindralandyReviewGeometricIntegrators2018}{article}{}
    \name{author}{3}{}{%
      {{hash=RD}{%
         family={Razafindralandy},
         familyi={R\bibinitperiod},
         given={Dina},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Hamdouni},
         familyi={H\bibinitperiod},
         given={Aziz},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Chhay},
         familyi={C\bibinitperiod},
         given={Marx},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{Discrete Exterior Calculus,Geometric integration,Lie-symmetry
  preserving scheme,Multisymplectic,Symplectic integrator,Variational
  integrator}
    \strng{namehash}{RDHACM1}
    \strng{fullhash}{RDHACM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{RHC18}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    Some of the most important geometric integrators for both ordinary and
  partial differential equations are reviewed and illustrated with examples in
  mechanics. The class of Hamiltonian differential systems is recalled and its
  symplectic structure is highlighted. The associated natural geometric
  integrators, known as symplectic integrators, are then presented. In
  particular, their ability to numerically reproduce first integrals with a
  bounded error over a long time interval is shown. The extension to partial
  differential Hamiltonian systems and to multisymplectic integrators is
  presented afterwards. Next, the class of Lagrangian systems is described. It
  is highlighted that the variational structure carries both the dynamics
  (Euler\textendash Lagrange equations) and the conservation laws (N\oe ther's
  theorem). Integrators preserving the variational structure are constructed by
  mimicking the calculus of variation at the discrete level. We show that this
  approach leads to numerical schemes which preserve exactly the energy of the
  system. After that, the Lie group of local symmetries of partial differential
  equations is recalled. A construction of Lie-symmetry-preserving numerical
  scheme is then exposed. This is done via the moving frame method.
  Applications to Burgers equation are shown. The last part is devoted to the
  Discrete Exterior Calculus, which is a structure-preserving integrator based
  on differential geometry and exterior calculus. The efficiency of the
  approach is demonstrated on fluid flow problems with a passive scalar
  advection.%
    }
    \verb{doi}
    \verb 10.1186/s40323-018-0110-y
    \endverb
    \field{issn}{2213-7467}
    \field{number}{1}
    \field{pages}{16}
    \field{title}{A Review of Some Geometric Integrators}
    \field{volume}{5}
    \field{journaltitle}{Advanced Modeling and Simulation in Engineering
  Sciences}
    \field{month}{06}
    \field{year}{2018}
  \endentry

  \entry{riceMathematicalStatisticsData2006}{book}{}
    \name{author}{1}{}{%
      {{hash=RJA}{%
         family={Rice},
         familyi={R\bibinitperiod},
         given={John\bibnamedelima A.},
         giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cengage Learning}%
    }
    \keyw{Mathematics / Probability \& Statistics / General}
    \strng{namehash}{RJA1}
    \strng{fullhash}{RJA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ric06}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    This is the first text in a generation to re-examine the purpose of the
  mathematical statistics course. The book's approach interweaves traditional
  topics with data analysis and reflects the use of the computer with close
  ties to the practice of statistics. The author stresses analysis of data,
  examines real problems with real data, and motivates the theory. The book's
  descriptive statistics, graphical displays, and realistic applications stand
  in strong contrast to traditional texts that are set in abstract
  settings.Important Notice: Media content referenced within the product
  description or the product text may not be available in the ebook version.%
    }
    \field{isbn}{978-0-534-39942-9}
    \field{title}{Mathematical Statistics and Data Analysis}
    \field{langid}{english}
    \field{month}{04}
    \field{year}{2006}
  \endentry

  \entry{redaMachineLearningApplications2020}{article}{}
    \name{author}{3}{}{%
      {{hash=RC}{%
         family={R{\'e}da},
         familyi={R\bibinitperiod},
         given={Cl{\'e}mence},
         giveni={C\bibinitperiod},
      }}%
      {{hash=KE}{%
         family={Kaufmann},
         familyi={K\bibinitperiod},
         given={Emilie},
         giveni={E\bibinitperiod},
      }}%
      {{hash=DA}{%
         family={{Delahaye-Duriez}},
         familyi={D\bibinitperiod},
         given={Andr{\'e}e},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{Adaptive clinical trial,Bayesian optimization,Collaborative
  filtering,Drug discovery,Drug repurposing,Multi-armed bandit}
    \strng{namehash}{RCKEDA1}
    \strng{fullhash}{RCKEDA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{RKD20}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    Due to the huge amount of biological and medical data available today,
  along with well-established machine learning algorithms, the design of
  largely automated drug development pipelines can now be envisioned. These
  pipelines may guide, or speed up, drug discovery; provide a better
  understanding of diseases and associated biological phenomena; help planning
  preclinical wet-lab experiments, and even future clinical trials. This
  automation of the drug development process might be key to the current issue
  of low productivity rate that pharmaceutical companies currently face. In
  this survey, we will particularly focus on two classes of methods: sequential
  learning and recommender systems, which are active biomedical fields of
  research.%
    }
    \verb{doi}
    \verb 10.1016/j.csbj.2019.12.006
    \endverb
    \field{issn}{2001-0370}
    \field{pages}{241\bibrangedash 252}
    \field{title}{Machine Learning Applications in Drug Development}
    \field{volume}{18}
    \field{langid}{english}
    \field{journaltitle}{Computational and Structural Biotechnology Journal}
    \field{month}{01}
    \field{year}{2020}
  \endentry

  \entry{roblesNoteEquationState2014a}{article}{}
    \name{author}{3}{}{%
      {{hash=RM}{%
         family={Robles},
         familyi={R\bibinitperiod},
         given={Miguel},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LM}{%
         family={{L{\'o}pez de Haro}},
         familyi={L\bibinitperiod},
         given={Mariano},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Santos},
         familyi={S\bibinitperiod},
         given={Andr{\'e}s},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{RMLMSA1}
    \strng{fullhash}{RMLMSA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{RLS14}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \verb{doi}
    \verb 10.1063/1.4870524
    \endverb
    \field{issn}{0021-9606}
    \field{number}{13}
    \field{pages}{136101}
    \field{shorttitle}{Note}
    \field{title}{Note: Equation of State and the Freezing Point in the
  Hard-Sphere Model}
    \field{volume}{140}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{04}
    \field{year}{2014}
  \endentry

  \entry{rosenblattPerceptronProbabilisticModel1958}{article}{}
    \name{author}{1}{}{%
      {{hash=RF}{%
         family={Rosenblatt},
         familyi={R\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Psychological Association}%
    }
    \keyw{Brain,Cognition,Memory,Nervous System}
    \strng{namehash}{RF1}
    \strng{fullhash}{RF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{Ros58}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    To answer the questions of how information about the physical world is
  sensed, in what form is information remembered, and how does information
  retained in memory influence recognition and behavior, a theory is developed
  for a hypothetical nervous system called a perceptron. The theory serves as a
  bridge between biophysics and psychology. It is possible to predict learning
  curves from neurological variables and vice versa. The quantitative
  statistical approach is fruitful in the understanding of the organization of
  cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all
  rights reserved)%
    }
    \verb{doi}
    \verb 10.1037/h0042519
    \endverb
    \field{issn}{1939-1471}
    \field{number}{6}
    \field{pages}{386\bibrangedash 408}
    \field{shorttitle}{The Perceptron}
    \field{title}{The Perceptron: A Probabilistic Model for Information Storage
  and Organization in the Brain}
    \field{volume}{65}
    \list{location}{1}{%
      {US}%
    }
    \field{journaltitle}{Psychological Review}
    \field{year}{1958}
  \endentry

  \entry{rogersIonicLiquidsSolvents2003}{article}{}
    \name{author}{2}{}{%
      {{hash=RRD}{%
         family={Rogers},
         familyi={R\bibinitperiod},
         given={Robin\bibnamedelima D.},
         giveni={R\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SKR}{%
         family={Seddon},
         familyi={S\bibinitperiod},
         given={Kenneth\bibnamedelima R.},
         giveni={K\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Association for the Advancement of Science}%
    }
    \strng{namehash}{RRDSKR1}
    \strng{fullhash}{RRDSKR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{RS03}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \verb{doi}
    \verb 10.1126/science.1090313
    \endverb
    \field{number}{5646}
    \field{pages}{792\bibrangedash 793}
    \field{title}{Ionic Liquids--Solvents of the Future?}
    \field{volume}{302}
    \field{journaltitle}{Science}
    \field{month}{10}
    \field{year}{2003}
  \endentry

  \entry{rudinPrinciplesMathematicalAnalysis2013}{book}{}
    \name{author}{1}{}{%
      {{hash=RW}{%
         family={Rudin},
         familyi={R\bibinitperiod},
         given={Walter},
         giveni={W\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {McGraw-Hill}%
    }
    \strng{namehash}{RW1}
    \strng{fullhash}{RW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Rud13}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    The third edition of this well known text continues to provide a solid
  foundation in mathematical analysis for undergraduate and first-year graduate
  students. The text begins with a discussion of the real number system as a
  complete ordered field. (Dedekind's construction is now treated in an
  appendix to Chapter I.) The topological background needed for the development
  of convergence, continuity, differentiation and integration is provided in
  Chapter 2. There is a new section on the gamma function, and many new and
  interesting exercises are included. -- Publisher description.%
    }
    \field{isbn}{978-1-259-06478-4}
    \field{title}{Principles of Mathematical Analysis}
    \field{langid}{english}
    \field{year}{2013}
  \endentry

  \entry{ruderOverviewGradientDescent2017}{article}{}
    \name{author}{1}{}{%
      {{hash=RS}{%
         family={Ruder},
         familyi={R\bibinitperiod},
         given={Sebastian},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Machine Learning}
    \strng{namehash}{RS1}
    \strng{fullhash}{RS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Rud17}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    Gradient descent optimization algorithms, while increasingly popular, are
  often used as black-box optimizers, as practical explanations of their
  strengths and weaknesses are hard to come by. This article aims to provide
  the reader with intuitions with regard to the behaviour of different
  algorithms that will allow her to put them to use. In the course of this
  overview, we look at different variants of gradient descent, summarize
  challenges, introduce the most common optimization algorithms, review
  architectures in a parallel and distributed setting, and investigate
  additional strategies for optimizing gradient descent.%
    }
    \verb{eprint}
    \verb 1609.04747
    \endverb
    \field{title}{An Overview of Gradient Descent Optimization Algorithms}
    \field{journaltitle}{arXiv:1609.04747 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{06}
    \field{year}{2017}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{rasmussenGaussianProcessesMachine2006}{book}{}
    \name{author}{3}{}{%
      {{hash=RCE}{%
         family={Rasmussen},
         familyi={R\bibinitperiod},
         given={Carl\bibnamedelima Edward},
         giveni={C\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=WCKI}{%
         family={Williams},
         familyi={W\bibinitperiod},
         given={Christopher K.\bibnamedelima I.},
         giveni={C\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim
  I\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{Computers / Computer Science,Computers / Machine Theory}
    \strng{namehash}{RCEWCKIBF1}
    \strng{fullhash}{RCEWCKIBF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{RWB06}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    A comprehensive and self-contained introduction to Gaussian processes,
  which provide a principled, practical, probabilistic approach to learning in
  kernel machines. Gaussian processes (GPs) provide a principled, practical,
  probabilistic approach to learning in kernel machines. GPs have received
  increased attention in the machine-learning community over the past decade,
  and this book provides a long-needed systematic and unified treatment of
  theoretical and practical aspects of GPs in machine learning. The treatment
  is comprehensive and self-contained, targeted at researchers and students in
  machine learning and applied statistics. The book deals with the
  supervised-learning problem for both regression and classification, and
  includes detailed algorithms. A wide variety of covariance (kernel) functions
  are presented and their properties discussed. Model selection is discussed
  both from a Bayesian and a classical perspective. Many connections to other
  well-known techniques from machine learning and statistics are discussed,
  including support-vector machines, neural networks, splines, regularization
  networks, relevance vector machines and others. Theoretical issues including
  learning curves and the PAC-Bayesian framework are treated, and several
  approximation methods for learning with large datasets are discussed. The
  book contains illustrative examples and exercises, and code and datasets are
  available on the Web. Appendixes provide mathematical background and a
  discussion of Gaussian Markov processes.%
    }
    \field{isbn}{978-0-262-18253-9}
    \field{title}{Gaussian Processes for Machine Learning}
    \field{langid}{english}
    \field{year}{2006}
  \endentry

  \entry{rogersNewThermodynamicallyConsistent1984b}{article}{}
    \name{author}{2}{}{%
      {{hash=RFJ}{%
         family={Rogers},
         familyi={R\bibinitperiod},
         given={Forrest\bibnamedelima J.},
         giveni={F\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=YDA}{%
         family={Young},
         familyi={Y\bibinitperiod},
         given={David\bibnamedelima A.},
         giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{RFJYDA1}
    \strng{fullhash}{RFJYDA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{RY84}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    A new integral equation in which the hypernetted-chain and Percus-Yevick
  approximations are "mixed" as a function of interparticle separation is
  described. An adjustable parameter {$\alpha$} in the mixing function is used
  to enforce thermodynamic consistency. For simple 1rn potential fluids,
  {$\alpha$} is constant for all densities, and the solutions of the integral
  equations are in very good agreement with Monte Carlo calculations. For the
  one-component plasma, {$\alpha$} is a slowly varying function of density, but
  the agreement between calculated solutions and Monte Carlo is also good. This
  approach has definite advantages over previous thermodynamically consistent
  equations.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevA.30.999
    \endverb
    \field{number}{2}
    \field{pages}{999\bibrangedash 1007}
    \field{title}{New, Thermodynamically Consistent, Integral Equation for
  Simple Fluids}
    \field{volume}{30}
    \field{journaltitle}{Physical Review A}
    \field{month}{08}
    \field{year}{1984}
  \endentry

  \entry{ramachandranSearchingActivationFunctions2017}{article}{}
    \name{author}{3}{}{%
      {{hash=RP}{%
         family={Ramachandran},
         familyi={R\bibinitperiod},
         given={Prajit},
         giveni={P\bibinitperiod},
      }}%
      {{hash=ZB}{%
         family={Zoph},
         familyi={Z\bibinitperiod},
         given={Barret},
         giveni={B\bibinitperiod},
      }}%
      {{hash=LQV}{%
         family={Le},
         familyi={L\bibinitperiod},
         given={Quoc\bibnamedelima V.},
         giveni={Q\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer
  Science - Machine Learning,Computer Science - Neural and Evolutionary
  Computing}
    \strng{namehash}{RPZBLQV1}
    \strng{fullhash}{RPZBLQV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{RZL17}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{abstract}{%
    The choice of activation functions in deep networks has a significant
  effect on the training dynamics and task performance. Currently, the most
  successful and widely-used activation function is the Rectified Linear Unit
  (ReLU). Although various hand-designed alternatives to ReLU have been
  proposed, none have managed to replace it due to inconsistent gains. In this
  work, we propose to leverage automatic search techniques to discover new
  activation functions. Using a combination of exhaustive and reinforcement
  learning-based search, we discover multiple novel activation functions. We
  verify the effectiveness of the searches by conducting an empirical
  evaluation with the best discovered activation function. Our experiments show
  that the best discovered activation function, \$f(x) = x \textbackslash cdot
  \textbackslash text\{sigmoid\}(\textbackslash beta x)\$, which we name Swish,
  tends to work better than ReLU on deeper models across a number of
  challenging datasets. For example, simply replacing ReLUs with Swish units
  improves top-1 classification accuracy on ImageNet by 0.9\textbackslash\% for
  Mobile NASNet-A and 0.6\textbackslash\% for Inception-ResNet-v2. The
  simplicity of Swish and its similarity to ReLU make it easy for practitioners
  to replace ReLUs with Swish units in any neural network.%
    }
    \verb{eprint}
    \verb 1710.05941
    \endverb
    \field{title}{Searching for Activation Functions}
    \field{journaltitle}{arXiv:1710.05941 [cs]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs}
    \field{month}{10}
    \field{year}{2017}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{saadPracticalUseKrylov1984a}{article}{}
    \name{author}{1}{}{%
      {{hash=SY}{%
         family={Saad},
         familyi={S\bibinitperiod},
         given={Youcef},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{2}{%
      {Society for Industrial}%
      {Applied Mathematics}%
    }
    \keyw{conjugate gradients,iterative methods,nonsymmetric systems,numerical
  linear algebra}
    \strng{namehash}{SY1}
    \strng{fullhash}{SY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Saa84}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    The main purpose of this paper is to develop stable versions of some Krylov
  subspace methods for solving linear systems of equations \$Ax = b\$. As in
  the case of Paige and Saunders's SYMMLQ [SIAM J. Numer. Anal., 12 (1975), pp.
  617\textendash 624], our algorithms are based on stable factorizations of the
  banded Hessenberg matrix representing the restriction of the linear
  application A to a Krylov subspace. We will show how an algorithm similar to
  the SYMMLQ can be derived for nonsymmetric problems and we will describe a
  more economical algorithm based upon the \$LU\$ factorization with partial
  pivoting. In the particular case where A is symmetric indefinite the new
  algorithm is theoretically equivalent to SYMMLQ but slightly more economical.
  As a consequence, an advantage of the new approach is that nonsymmetric or
  symmetric indefinite or both nonsymmetric and indefinite systems of linear
  equations can be handled by a single algorithm.%
    }
    \verb{doi}
    \verb 10.1137/0905015
    \endverb
    \field{issn}{0196-5204}
    \field{number}{1}
    \field{pages}{203\bibrangedash 228}
    \field{title}{Practical Use of Some Krylov Subspace Methods for Solving
  Indefinite and Nonsymmetric Linear Systems}
    \field{volume}{5}
    \field{journaltitle}{SIAM Journal on Scientific and Statistical Computing}
    \field{month}{03}
    \field{year}{1984}
  \endentry

  \entry{suttonReinforcementLearningSecond2018}{book}{}
    \name{author}{2}{}{%
      {{hash=SRS}{%
         family={Sutton},
         familyi={S\bibinitperiod},
         given={Richard\bibnamedelima S.},
         giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=BAG}{%
         family={Barto},
         familyi={B\bibinitperiod},
         given={Andrew\bibnamedelima G.},
         giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{Computers / Artificial Intelligence / General}
    \strng{namehash}{SRSBAG1}
    \strng{fullhash}{SRSBAG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{SB18}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    The significantly expanded and updated new edition of a widely used text on
  reinforcement learning, one of the most active research areas in artificial
  intelligence.Reinforcement learning, one of the most active research areas in
  artificial intelligence, is a computational approach to learning whereby an
  agent tries to maximize the total amount of reward it receives while
  interacting with a complex, uncertain environment. In Reinforcement Learning,
  Richard Sutton and Andrew Barto provide a clear and simple account of the
  field's key ideas and algorithms. This second edition has been significantly
  expanded and updated, presenting new topics and updating coverage of other
  topics.Like the first edition, this second edition focuses on core online
  learning algorithms, with the more mathematical material set off in shaded
  boxes. Part I covers as much of reinforcement learning as possible without
  going beyond the tabular case for which exact solutions can be found. Many
  algorithms presented in this part are new to the second edition, including
  UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to
  function approximation, with new sections on such topics as artificial neural
  networks and the Fourier basis, and offers expanded treatment of off-policy
  learning and policy-gradient methods. Part III has new chapters on
  reinforcement learning's relationships to psychology and neuroscience, as
  well as an updated case-studies chapter including AlphaGo and AlphaGo Zero,
  Atari game playing, and IBM Watson's wagering strategy. The final chapter
  discusses the future societal impacts of reinforcement learning.%
    }
    \field{isbn}{978-0-262-35270-3}
    \field{shorttitle}{Reinforcement Learning, Second Edition}
    \field{title}{Reinforcement Learning, Second Edition: An Introduction}
    \field{langid}{english}
    \field{month}{11}
    \field{year}{2018}
  \endentry

  \entry{steinwartSupportVectorMachines2008}{book}{}
    \name{author}{2}{}{%
      {{hash=SI}{%
         family={Steinwart},
         familyi={S\bibinitperiod},
         given={Ingo},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Christmann},
         familyi={C\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Science \& Business Media}%
    }
    \keyw{Computers / Artificial Intelligence / Computer Vision \& Pattern
  Recognition,Computers / Artificial Intelligence / General,Computers / Data
  Science / Data Analytics,Computers / Information Technology,Computers /
  Mathematical \& Statistical Software,Computers / Optical Data
  Processing,Mathematics / Discrete Mathematics,Technology \& Engineering /
  Electronics / General,Technology \& Engineering / Imaging Systems}
    \strng{namehash}{SICA1}
    \strng{fullhash}{SICA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{SC08}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Every mathematical discipline goes through three periods of development:
  the naive, the formal, and the critical. David Hilbert The goal of this book
  is to explain the principles that made support vector machines (SVMs) a
  successful modeling and prediction tool for a variety of applications. We try
  to achieve this by presenting the basic ideas of SVMs together with the
  latest developments and current research questions in a uni?ed style. In a
  nutshell, we identify at least three reasons for the success of SVMs: their
  ability to learn well with only a very small number of free parameters, their
  robustness against several types of model violations and outliers, and last
  but not least their computational e?ciency compared with several other
  methods. Although there are several roots and precursors of SVMs, these
  methods gained particular momentum during the last 15 years since Vapnik
  (1995, 1998) published his well-known textbooks on statistical learning
  theory with aspecialemphasisonsupportvectormachines.
  Sincethen,the?eldofmachine
  learninghaswitnessedintenseactivityinthestudyofSVMs,whichhasspread
  moreandmoretootherdisciplinessuchasstatisticsandmathematics. Thusit seems
  fair to say that several communities are currently working on support vector
  machines and on related kernel-based methods. Although there are many
  interactions between these communities, we think that there is still
  roomforadditionalfruitfulinteractionandwouldbegladifthistextbookwere found
  helpful in stimulating further research. Many of the results presented in
  this book have previously been scattered in the journal literature or are
  still under review. As a consequence, these results have been accessible only
  to a relativelysmallnumberofspecialists,sometimesprobablyonlytopeoplefrom one
  community but not the others.%
    }
    \field{isbn}{978-0-387-77242-4}
    \field{title}{Support Vector Machines}
    \field{langid}{english}
    \field{month}{09}
    \field{year}{2008}
  \endentry

  \entry{schoenholzStructuralApproachRelaxation2016}{article}{}
    \name{author}{5}{}{%
      {{hash=SSS}{%
         family={Schoenholz},
         familyi={S\bibinitperiod},
         given={S.\bibnamedelima S.},
         giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=CED}{%
         family={Cubuk},
         familyi={C\bibinitperiod},
         given={E.\bibnamedelima D.},
         giveni={E\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SDM}{%
         family={Sussman},
         familyi={S\bibinitperiod},
         given={D.\bibnamedelima M.},
         giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=KE}{%
         family={Kaxiras},
         familyi={K\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=LAJ}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={A.\bibnamedelima J.},
         giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{SSS+1}
    \strng{fullhash}{SSSCEDSDMKELAJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Sch+16}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    The relation between structure and dynamics in glasses is not fully
  understood. A new approach based on machine learning now reveals a
  correlation between softness\textemdash a structural property\textemdash and
  glassy dynamics.%
    }
    \verb{doi}
    \verb 10.1038/nphys3644
    \endverb
    \field{issn}{1745-2481}
    \field{number}{5}
    \field{pages}{469\bibrangedash 471}
    \field{title}{A Structural Approach to Relaxation in Glassy Liquids}
    \field{volume}{12}
    \field{langid}{english}
    \field{journaltitle}{Nature Physics}
    \field{month}{05}
    \field{year}{2016}
  \endentry

  \entry{sezerFinancialTimeSeries2020}{article}{}
    \name{author}{3}{}{%
      {{hash=SOB}{%
         family={Sezer},
         familyi={S\bibinitperiod},
         given={Omer\bibnamedelima Berat},
         giveni={O\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=GMU}{%
         family={Gudelek},
         familyi={G\bibinitperiod},
         given={Mehmet\bibnamedelima Ugur},
         giveni={M\bibinitperiod\bibinitdelim U\bibinitperiod},
      }}%
      {{hash=OAM}{%
         family={Ozbayoglu},
         familyi={O\bibinitperiod},
         given={Ahmet\bibnamedelima Murat},
         giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \keyw{CNN,Computational intelligence,Deep learning,Finance,LSTM,Machine
  learning,RNN,Time series forecasting}
    \strng{namehash}{SOBGMUOAM1}
    \strng{fullhash}{SOBGMUOAM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{SGO20}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Financial time series forecasting is undoubtedly the top choice of
  computational intelligence for finance researchers in both academia and the
  finance industry due to its broad implementation areas and substantial
  impact. Machine Learning (ML) researchers have created various models, and a
  vast number of studies have been published accordingly. As such, a
  significant number of surveys exist covering ML studies on financial time
  series forecasting. Lately, Deep Learning (DL) models have appeared within
  the field, with results that significantly outperform their traditional ML
  counterparts. Even though there is a growing interest in developing models
  for financial time series forecasting, there is a lack of review papers that
  solely focus on DL for finance. Hence, the motivation of this paper is to
  provide a comprehensive literature review of DL studies on financial time
  series forecasting implementation. We not only categorized the studies
  according to their intended forecasting implementation areas, such as index,
  forex, and commodity forecasting, but we also grouped them based on their DL
  model choices, such as Convolutional Neural Networks (CNNs), Deep Belief
  Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision
  the future of the field by highlighting its possible setbacks and
  opportunities for the benefit of interested researchers.%
    }
    \verb{doi}
    \verb 10.1016/j.asoc.2020.106181
    \endverb
    \field{issn}{1568-4946}
    \field{pages}{106181}
    \field{shorttitle}{Financial Time Series Forecasting with Deep Learning}
    \field{title}{Financial Time Series Forecasting with Deep Learning : A
  Systematic Literature Review: 2005\textendash 2019}
    \field{volume}{90}
    \field{langid}{english}
    \field{journaltitle}{Applied Soft Computing}
    \field{month}{05}
    \field{year}{2020}
  \endentry

  \entry{shermanInverseMethodsDesign2020a}{article}{}
    \name{author}{5}{}{%
      {{hash=SZM}{%
         family={Sherman},
         familyi={S\bibinitperiod},
         given={Zachary\bibnamedelima M.},
         giveni={Z\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=HMP}{%
         family={Howard},
         familyi={H\bibinitperiod},
         given={Michael\bibnamedelima P.},
         giveni={M\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=LBA}{%
         family={Lindquist},
         familyi={L\bibinitperiod},
         given={Beth\bibnamedelima A.},
         giveni={B\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=JRB}{%
         family={Jadrich},
         familyi={J\bibinitperiod},
         given={Ryan\bibnamedelima B.},
         giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=TTM}{%
         family={Truskett},
         familyi={T\bibinitperiod},
         given={Thomas\bibnamedelima M.},
         giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{SZM+1}
    \strng{fullhash}{SZMHMPLBAJRBTTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{She+20}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Functional soft materials, comprising colloidal and molecular building
  blocks that self-organize into complex structures as a result of their
  tunable interactions, enable a wide array of technological applications.
  Inverse methods provide a systematic means for navigating their inherently
  high-dimensional design spaces to create materials with targeted properties.
  While multiple physically motivated inverse strategies have been successfully
  implemented in silico, their translation to guiding experimental materials
  discovery has thus far been limited to a handful of proof-of-concept studies.
  In this perspective, we discuss recent advances in inverse methods for design
  of soft materials that address two challenges: (1) methodological limitations
  that prevent such approaches from satisfying design constraints and (2)
  computational challenges that limit the size and complexity of systems that
  can be addressed. Strategies that leverage machine learning have proven
  particularly effective, including methods to discover order parameters that
  characterize complex structural motifs and schemes to efficiently compute
  macroscopic properties from the underlying structure. We also highlight
  promising opportunities to improve the experimental realizability of
  materials designed computationally, including discovery of materials with
  functionality at multiple thermodynamic states, design of externally directed
  assembly protocols that are simple to implement in experiments, and
  strategies to improve the accuracy and computational efficiency of
  experimentally relevant models.%
    }
    \verb{doi}
    \verb 10.1063/1.5145177
    \endverb
    \field{issn}{0021-9606}
    \field{number}{14}
    \field{pages}{140902}
    \field{title}{Inverse Methods for Design of Soft Materials}
    \field{volume}{152}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{04}
    \field{year}{2020}
  \endentry

  \entry{steinhardtBondorientationalOrderLiquids1983}{article}{}
    \name{author}{3}{}{%
      {{hash=SPJ}{%
         family={Steinhardt},
         familyi={S\bibinitperiod},
         given={Paul\bibnamedelima J.},
         giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=NDR}{%
         family={Nelson},
         familyi={N\bibinitperiod},
         given={David\bibnamedelima R.},
         giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RM}{%
         family={Ronchetti},
         familyi={R\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{SPJNDRRM1}
    \strng{fullhash}{SPJNDRRM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{SNR83}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Bond-orientational order in molecular-dynamics simulations of supercooled
  liquids and in models of metallic glasses is studied. Quadratic and
  third-order invariants formed from bond spherical harmonics allow
  quantitative measures of cluster symmetries in these systems. A state with
  short-range translational order, but extended correlations in the
  orientations of particle clusters, starts to develop about 10\% below the
  equilibrium melting temperature in a supercooled Lennard-Jones liquid. The
  order is predominantly icosahedral, although there is also a cubic component
  which we attribute to the periodic boundary conditions. Results are obtained
  for liquids cooled in an icosahedral pair potential as well. Only a modest
  amount of orientational order appears in a relaxed Finney
  dense-random-packing model. In contrast, we find essentially perfect
  icosahedral bond correlations in alternative "amorphon" cluster models of
  glass structure.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevB.28.784
    \endverb
    \field{number}{2}
    \field{pages}{784\bibrangedash 805}
    \field{title}{Bond-Orientational Order in Liquids and Glasses}
    \field{volume}{28}
    \field{journaltitle}{Physical Review B}
    \field{month}{07}
    \field{year}{1983}
  \endentry

  \entry{stoneApplicationsTheoryBoolean1937}{article}{}
    \name{author}{1}{}{%
      {{hash=SMH}{%
         family={Stone},
         familyi={S\bibinitperiod},
         given={M.\bibnamedelima H.},
         giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \strng{namehash}{SMH1}
    \strng{fullhash}{SMH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Sto37}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Advancing research. Creating connections.%
    }
    \verb{doi}
    \verb 10.1090/S0002-9947-1937-1501905-7
    \endverb
    \field{issn}{0002-9947, 1088-6850}
    \field{number}{3}
    \field{pages}{375\bibrangedash 481}
    \field{title}{Applications of the Theory of Boolean Rings to General
  Topology}
    \field{volume}{41}
    \field{langid}{english}
    \field{journaltitle}{Transactions of the American Mathematical Society}
    \field{year}{1937}
  \endentry

  \entry{stoneGeneralizedWeierstrassApproximation1948}{article}{}
    \name{author}{1}{}{%
      {{hash=SMH}{%
         family={Stone},
         familyi={S\bibinitperiod},
         given={M.\bibnamedelima H.},
         giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Mathematical Association of America}%
    }
    \strng{namehash}{SMH1}
    \strng{fullhash}{SMH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Sto48}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \verb{doi}
    \verb 10.2307/3029750
    \endverb
    \field{issn}{0025-570X}
    \field{number}{4}
    \field{pages}{167\bibrangedash 184}
    \field{title}{The Generalized Weierstrass Approximation Theorem}
    \field{volume}{21}
    \field{journaltitle}{Mathematics Magazine}
    \field{year}{1948}
  \endentry

  \entry{taoIntroductionMeasureTheory2011}{book}{}
    \name{author}{1}{}{%
      {{hash=TT}{%
         family={Tao},
         familyi={T\bibinitperiod},
         given={Terence},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Mathematical Soc.}%
    }
    \keyw{Mathematics / General}
    \strng{namehash}{TT1}
    \strng{fullhash}{TT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Tao11}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    This is a graduate text introducing the fundamentals of measure theory and
  integration theory, which is the foundation of modern real analysis. The text
  focuses first on the concrete setting of Lebesgue measure and the Lebesgue
  integral (which in turn is motivated by the more classical concepts of Jordan
  measure and the Riemann integral), before moving on to abstract measure and
  integration theory, including the standard convergence theorems,
  Fubini\&\#39;s theorem, and the Caratheodory extension theorem. Classical
  differentiation theorems, such as the Lebesgue and Rademacher differentiation
  theorems, are also covered, as are connections with probability theory. The
  material is intended to cover a quarter or semester\&\#39;s worth of material
  for a first graduate course in real analysis. There is an emphasis in the
  text on tying together the abstract and the concrete sides of the subject,
  using the latter to illustrate and motivate the former. The central role of
  key principles (such as Littlewood\&\#39;s three principles) as providing
  guiding intuition to the subject is also emphasized. There are a large number
  of exercises throughout that develop key aspects of the theory, and are thus
  an integral component of the text. As a supplementary section, a discussion
  of general problem-solving strategies in analysis is also given. The last
  three sections discuss optional topics related to the main matter of the
  book.%
    }
    \field{isbn}{978-0-8218-6919-2}
    \field{title}{An Introduction to Measure Theory}
    \field{langid}{english}
    \field{month}{09}
    \field{year}{2011}
  \endentry

  \entry{trefethenNumericalLinearAlgebra1997}{book}{}
    \name{author}{2}{}{%
      {{hash=TLN}{%
         family={Trefethen},
         familyi={T\bibinitperiod},
         given={Lloyd\bibnamedelima N.},
         giveni={L\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
      {{hash=IDB}{%
         family={III},
         familyi={I\bibinitperiod},
         given={David\bibnamedelima Bau},
         giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {SIAM}%
    }
    \keyw{Mathematics / Algebra / General,Mathematics / Algebra /
  Linear,Mathematics / Applied,Mathematics / Mathematical Analysis,Technology
  \& Engineering / Engineering (General)}
    \strng{namehash}{TLNIDB1}
    \strng{fullhash}{TLNIDB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{TI97}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    This is a concise, insightful introduction to the field of numerical linear
  algebra. The clarity and eloquence of the presentation make it popular with
  teachers and students alike. The text aims to expand the reader\&\#39;s view
  of the field and to present standard material in a novel way. All of the most
  important topics in the field are covered with a fresh perspective, including
  iterative methods for systems of equations and eigenvalue problems and the
  underlying principles of conditioning and stability. Presentation is in the
  form of 40 lectures, which each focus on one or two central ideas. The unity
  between topics is emphasized throughout, with no risk of getting lost in
  details and technicalities. The book breaks with tradition by beginning with
  the QR factorization - an important and fresh idea for students, and the
  thread that connects most of the algorithms of numerical linear algebra.%
    }
    \field{isbn}{978-0-89871-361-9}
    \field{title}{Numerical Linear Algebra}
    \field{langid}{english}
    \field{month}{06}
    \field{year}{1997}
  \endentry

  \entry{tsedneeClosureOrnsteinZernikeEquation2019}{article}{}
    \name{author}{2}{}{%
      {{hash=TT}{%
         family={Tsednee},
         familyi={T\bibinitperiod},
         given={Tsogbayar},
         giveni={T\bibinitperiod},
      }}%
      {{hash=LT}{%
         family={Luchko},
         familyi={L\bibinitperiod},
         given={Tyler},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{TTLT1}
    \strng{fullhash}{TTLT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{TL19}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    The Ornstein-Zernike (OZ) integral equation theory is a powerful approach
  to simple liquids due to its low computational cost and the fact that, when
  combined with an appropriate closure equation, the theory is
  thermodynamically complete. However, approximate closures proposed to date
  exhibit pressure or free energy inconsistencies that produce inaccurate or
  ambiguous results, limiting the usefulness of the Ornstein-Zernike approach.
  To address this problem, we combine methods to enforce both pressure and free
  energy consistency to create a new closure approximation and test it for a
  single-component Lennard-Jones fluid. The closure is a simple power series in
  the direct and total correlation functions for which we have derived
  analytical formulas for the excess Helmholtz free energy and chemical
  potential. These expressions contain a partial molar volumelike term, similar
  to excess chemical potential correction terms recently developed. Using our
  bridge approximation, we have calculated the pressure, Helmholtz free energy,
  and chemical potential for the Lennard-Jones fluid using the Kirkwood
  charging, thermodynamic integration techniques, and analytic expressions.
  These results are compared with those from the hypernetted chain equation and
  the Verlet-modified closure against Monte Carlo and equations-of-state data
  for reduced densities of {$\rho{_\ast}<$}1 and temperatures of
  T{${_\ast}$}=1.5, 2.74, and 5. Our closure shows consistency among all
  thermodynamic paths, except for one expression of the Gibbs-Duhem relation,
  whereas the hypernetted chain equation and the Verlet-modified closure
  exhibit consistency between only a few relations. Accuracy of the closure is
  comparable to the Verlet-modified closure and a significant improvement to
  results obtained from the hypernetted chain equation.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevE.99.032130
    \endverb
    \field{number}{3}
    \field{pages}{032130}
    \field{title}{Closure for the Ornstein-Zernike Equation with Pressure and
  Free Energy Consistency}
    \field{volume}{99}
    \field{journaltitle}{Physical Review E}
    \field{month}{03}
    \field{year}{2019}
  \endentry

  \entry{tolmanPrinciplesStatisticalMechanics1979}{book}{}
    \name{author}{1}{}{%
      {{hash=TRC}{%
         family={Tolman},
         familyi={T\bibinitperiod},
         given={Richard\bibnamedelima Chace},
         giveni={R\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Courier Corporation}%
    }
    \keyw{Science / Mechanics / General,Science / Physics / General,Technology
  \& Engineering / General}
    \strng{namehash}{TRC1}
    \strng{fullhash}{TRC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Tol79}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    Classic treatment of a subject essential to contemporary physics. Classical
  and quantum statistical mechanics, plus application to thermodynamic
  behavior.%
    }
    \field{isbn}{978-0-486-63896-6}
    \field{title}{The Principles of Statistical Mechanics}
    \field{langid}{english}
    \field{month}{01}
    \field{year}{1979}
  \endentry

  \entry{trefethenApproximationTheoryApproximation2013}{book}{}
    \name{author}{1}{}{%
      {{hash=TLN}{%
         family={Trefethen},
         familyi={T\bibinitperiod},
         given={Lloyd\bibnamedelima N.},
         giveni={L\bibinitperiod\bibinitdelim N\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {SIAM}%
    }
    \keyw{Computers / Programming / Algorithms,Mathematics /
  Calculus,Mathematics / General,Mathematics / Mathematical
  Analysis,Mathematics / Numerical Analysis}
    \strng{namehash}{TLN1}
    \strng{fullhash}{TLN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Tre13}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    This book presents a twenty-first century approach to classical polynomial
  and rational approximation theory. The reader will find a strikingly original
  treatment of the subject, completely unlike any of the existing literature on
  approximation theory, with a rich set of both computational and theoretical
  exercises for the classroom. There are many original features that set this
  book apart: the emphasis is on topics close to numerical algorithms; every
  idea is illustrated with Chebfun examples; each chapter has an accompanying
  Matlab file for the reader to download; the text focuses on theorems and
  methods for analytic functions; original sources are cited rather than
  textbooks, and each item in the bibliography is accompanied by an editorial
  comment. This textbook is ideal for advanced undergraduates and graduate
  students across all of applied mathematics.%
    }
    \field{isbn}{978-1-61197-240-5}
    \field{title}{Approximation Theory and Approximation Practice}
    \field{langid}{english}
    \field{month}{01}
    \field{year}{2013}
  \endentry

  \entry{vandammeClassifyingCrystalsRounded2020}{article}{}
    \name{author}{4}{}{%
      {{hash=vR}{%
         family={{van Damme}},
         familyi={v\bibinitperiod},
         given={Robin},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CGM}{%
         family={Coli},
         familyi={C\bibinitperiod},
         given={Gabriele\bibnamedelima M.},
         giveni={G\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=vR}{%
         family={{van Roij}},
         familyi={v\bibinitperiod},
         given={Ren{\'e}},
         giveni={R\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dijkstra},
         familyi={D\bibinitperiod},
         given={Marjolein},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Chemical Society}%
    }
    \strng{namehash}{vR+1}
    \strng{fullhash}{vRCGMvRDM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{{van}+20}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{abstract}{%
    Using simulations we study the phase behavior of a family of hard
  spherotetrahedra, a shape that interpolates between tetrahedra and spheres.
  We identify 13 close-packed structures, some with packings that are
  significantly denser than previously reported. Twelve of these are crystals
  with unit cells of N = 2 or N = 4 particles, but in the shape regime of
  slightly rounded tetrahedra we find that the densest structure is a
  quasicrystal approximant with a unit cell of N = 82 particles. All 13
  structures are also stable below close packing, together with an additional
  14th plastic crystal phase at the sphere side of the phase diagram, and upon
  sufficient dilution to packing fractions below 50\textendash 60\% all
  structures melt. Interestingly, however, upon compressing the fluid phase,
  self-assembly takes place spontaneously only at the tetrahedron and the
  sphere side of the family but not in an intermediate regime of tetrahedra
  with rounded edges. We describe the local environment of each particle by a
  set of l-fold bond orientational order parameters {$\overline q$}l, which we
  use in an extensive principal component analysis. We find that the total
  packing fraction as well as several particular linear combinations of
  {$\overline q$}l rather than individual {$\overline q$}l's are optimally
  distinctive, specifically the differences {$\overline q$}4 \textendash{}
  {$\overline q$}6 for separating tetragonal from hexagonal structures and
  {$\overline q$}4\textendash{$\overline q$}8 for distinguishing tetragonal
  structures. We argue that these characteristic combinations are also useful
  as reliable order parameters in nucleation studies, enhanced sampling
  techniques, or inverse-design methods involving odd-shaped particles in
  general.%
    }
    \verb{doi}
    \verb 10.1021/acsnano.0c05288
    \endverb
    \field{issn}{1936-0851}
    \field{number}{11}
    \field{pages}{15144\bibrangedash 15153}
    \field{title}{Classifying Crystals of Rounded Tetrahedra and Determining
  Their Order Parameters Using Dimensionality Reduction}
    \field{volume}{14}
    \field{journaltitle}{ACS Nano}
    \field{month}{11}
    \field{year}{2020}
  \endentry

  \entry{verletComputerExperimentsClassical1967a}{article}{}
    \name{author}{1}{}{%
      {{hash=VL}{%
         family={Verlet},
         familyi={V\bibinitperiod},
         given={Loup},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{VL1}
    \strng{fullhash}{VL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ver67}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{abstract}{%
    The equation of motion of a system of 864 particles interacting through a
  Lennard-Jones potential has been integrated for various values of the
  temperature and density, relative, generally, to a fluid state. The
  equilibrium properties have been calculated and are shown to agree very well
  with the corresponding properties of argon. It is concluded that, to a good
  approximation, the equilibrium state of argon can be described through a
  two-body potential.%
    }
    \verb{doi}
    \verb 10.1103/PhysRev.159.98
    \endverb
    \field{number}{1}
    \field{pages}{98\bibrangedash 103}
    \field{title}{Computer "Experiments" on Classical Fluids. I.
  Thermodynamical Properties of Lennard-Jones Molecules}
    \field{volume}{159}
    \field{journaltitle}{Physical Review}
    \field{month}{07}
    \field{year}{1967}
  \endentry

  \entry{verletIntegralEquationsClassical1980}{article}{}
    \name{author}{1}{}{%
      {{hash=VL}{%
         family={Verlet},
         familyi={V\bibinitperiod},
         given={Loup},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Taylor \& Francis}%
    }
    \strng{namehash}{VL1}
    \strng{fullhash}{VL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ver80}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{abstract}{%
    A semi-phenomenological equation for the radial distribution function of
  hard spheres is presented and solved. This equation yields results which
  agree with the `exact' results within 1 per cent. The virial pressure and the
  pressure obtained from the inverse compressibility formula are consistent
  within 2 per cent.%
    }
    \verb{doi}
    \verb 10.1080/00268978000102671
    \endverb
    \field{issn}{0026-8976}
    \field{number}{1}
    \field{pages}{183\bibrangedash 190}
    \field{title}{Integral Equations for Classical Fluids. I. The Hard Sphere
  Case}
    \field{volume}{41}
    \field{journaltitle}{Molecular Physics}
    \field{annotation}{%
    \_eprint: https://doi.org/10.1080/00268978000102671%
    }
    \field{month}{09}
    \field{year}{1980}
  \endentry

  \entry{verletIntegralEquationsClassical1981}{article}{}
    \name{author}{1}{}{%
      {{hash=VL}{%
         family={Verlet},
         familyi={V\bibinitperiod},
         given={Loup},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Taylor \& Francis}%
    }
    \strng{namehash}{VL1}
    \strng{fullhash}{VL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Ver81}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{abstract}{%
    With the help of a Pad\'e technique, it is shown that, given the two
  requirements that the equation for the hard spheres r.d.f. should involve
  only convolution integrals (as in the PY and HNC equations) and that it
  should obey the internal consistency criterion imposed by the equality of the
  pressures calculated using the virial or the Ornstein-Zernike theorem, an
  integral equation can be derived without resorting to any postulate regarding
  its functional form. This equation leads to an equation of state which is
  satisfactory except in the vicinity of the Alder transition where it
  under-estimates the pressure by nearly 4 per cent. A detailed examination of
  the fourth virial coefficient reveals some possible reasons for the validity
  of the integral equation derived in the present paper.%
    }
    \verb{doi}
    \verb 10.1080/00268978100100971
    \endverb
    \field{issn}{0026-8976}
    \field{number}{6}
    \field{pages}{1291\bibrangedash 1302}
    \field{title}{Integral Equations for Classical Fluids. II. Hard Spheres
  Again}
    \field{volume}{42}
    \field{journaltitle}{Molecular Physics}
    \field{annotation}{%
    \_eprint: https://doi.org/10.1080/00268978100100971%
    }
    \field{month}{04}
    \field{year}{1981}
  \endentry

  \entry{valadez-perezReversibleAggregationColloidal2018}{article}{}
    \name{author}{3}{}{%
      {{hash=VNE}{%
         family={{Valadez-P{\'e}rez}},
         familyi={V\bibinitperiod},
         given={N{\'e}stor\bibnamedelima E.},
         giveni={N\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yun},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={{Casta{\~n}eda-Priego}},
         familyi={C\bibinitperiod},
         given={Ram{\'o}n},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Physical Society}%
    }
    \strng{namehash}{VNELYCR1}
    \strng{fullhash}{VNELYCR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{VLC18}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \field{abstract}{%
    Cluster morphology of spherical particles interacting with a short-range
  attraction has been extensively studied due to its relevance to many
  applications, such as the large-scale structure in amorphous materials, phase
  separation, protein aggregation, and organelle formation in cells. Although
  it was widely accepted that the range of the attraction solely controls the
  fractal dimension of clusters, recent experimental results challenged this
  concept by also showing the importance of the strength of attraction. Using
  Monte Carlo simulations, we conclusively demonstrate that it is possible to
  reduce the dependence of the cluster morphology to a single variable, namely,
  the reduced second virial coefficient, B{${_\ast}$}2, linking the local
  properties of colloidal systems to the extended law of corresponding states.
  Furthermore, the cluster size distribution exhibits two well-defined regimes:
  one identified for small clusters, whose fractal dimension, df, does not
  depend on the details of the attraction, i.e., small clusters have the same
  df, and another related to large clusters, whose morphology depends
  exclusively on B{${_\ast}$}2, i.e., df of large aggregates follows a master
  curve, which is only a function of B{${_\ast}$}2. This physical scenario is
  confirmed with the reanalysis of experimental results on colloidal-polymer
  mixtures.%
    }
    \verb{doi}
    \verb 10.1103/PhysRevLett.120.248004
    \endverb
    \field{number}{24}
    \field{pages}{248004}
    \field{shorttitle}{Reversible Aggregation and Colloidal Cluster Morphology}
    \field{title}{Reversible Aggregation and Colloidal Cluster Morphology: The
  Importance of the Extended Law of Corresponding States}
    \field{volume}{120}
    \field{journaltitle}{Physical Review Letters}
    \field{month}{06}
    \field{year}{2018}
  \endentry

  \entry{vompeBridgeFunctionExpansion1994}{article}{}
    \name{author}{2}{}{%
      {{hash=VAG}{%
         family={Vompe},
         familyi={V\bibinitperiod},
         given={A.\bibnamedelima G.},
         giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=MGA}{%
         family={Martynov},
         familyi={M\bibinitperiod},
         given={G.\bibnamedelima A.},
         giveni={G\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{VAGMGA1}
    \strng{fullhash}{VAGMGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{VM94}
    \field{sortinit}{V}
    \field{sortinithash}{V}
    \verb{doi}
    \verb 10.1063/1.467189
    \endverb
    \field{issn}{0021-9606}
    \field{number}{7}
    \field{pages}{5249\bibrangedash 5258}
    \field{title}{The Bridge Function Expansion and the Self-consistency
  Problem of the Ornstein\textendash Zernike Equation Solution}
    \field{volume}{100}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{04}
    \field{year}{1994}
  \endentry

  \entry{wierstraNaturalEvolutionStrategies2014a}{article}{}
    \name{author}{6}{}{%
      {{hash=WD}{%
         family={Wierstra},
         familyi={W\bibinitperiod},
         given={Daan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=ST}{%
         family={Schaul},
         familyi={S\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Glasmachers},
         familyi={G\bibinitperiod},
         given={Tobias},
         giveni={T\bibinitperiod},
      }}%
      {{hash=SY}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Yi},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=PJ}{%
         family={Peters},
         familyi={P\bibinitperiod},
         given={Jan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={J{\textbackslash}"\{u\}rgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{WD+1}
    \strng{fullhash}{WDSTGTSYPJSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Wie+14}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{abstract}{%
    This paper presents Natural Evolution Strategies (NES), a recent family of
  black-box optimization algorithms that use the natural gradient to update a
  parameterized search distribution in the direction of higher expected
  fitness. We introduce a collection of techniques that address issues of
  convergence, robustness, sample complexity, computational complexity and
  sensitivity to hyperparameters. This paper explores a number of
  implementations of the NES family, such as general-purpose multi-variate
  normal distributions and separable distributions tailored towards search in
  high dimensional spaces. Experimental results show best published performance
  on various standard benchmarks, as well as competitive performance on
  others.%
    }
    \field{issn}{1533-7928}
    \field{number}{27}
    \field{pages}{949\bibrangedash 980}
    \field{title}{Natural Evolution Strategies}
    \field{volume}{15}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2014}
  \endentry

  \entry{xiaoFashionMNISTNovelImage2017a}{article}{}
    \name{author}{3}{}{%
      {{hash=XH}{%
         family={Xiao},
         familyi={X\bibinitperiod},
         given={Han},
         giveni={H\bibinitperiod},
      }}%
      {{hash=RK}{%
         family={Rasul},
         familyi={R\bibinitperiod},
         given={Kashif},
         giveni={K\bibinitperiod},
      }}%
      {{hash=VR}{%
         family={Vollgraf},
         familyi={V\bibinitperiod},
         given={Roland},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer
  Science - Machine Learning,Statistics - Machine Learning}
    \strng{namehash}{XHRKVR1}
    \strng{fullhash}{XHRKVR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{XRV17}
    \field{sortinit}{X}
    \field{sortinithash}{X}
    \field{abstract}{%
    We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale
  images of 70,000 fashion products from 10 categories, with 7,000 images per
  category. The training set has 60,000 images and the test set has 10,000
  images. Fashion-MNIST is intended to serve as a direct drop-in replacement
  for the original MNIST dataset for benchmarking machine learning algorithms,
  as it shares the same image size, data format and the structure of training
  and testing splits. The dataset is freely available at
  https://github.com/zalandoresearch/fashion-mnist%
    }
    \verb{eprint}
    \verb 1708.07747
    \endverb
    \field{shorttitle}{Fashion-MNIST}
    \field{title}{Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine
  Learning Algorithms}
    \field{journaltitle}{arXiv:1708.07747 [cs, stat]}
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs, stat}
    \field{month}{09}
    \field{year}{2017}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{yuIntroductionEvolutionaryAlgorithms2010}{book}{}
    \name{author}{2}{}{%
      {{hash=YX}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Xinjie},
         giveni={X\bibinitperiod},
      }}%
      {{hash=GM}{%
         family={Gen},
         familyi={G\bibinitperiod},
         given={Mitsuo},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Science \& Business Media}%
    }
    \keyw{Computers / Artificial Intelligence / General,Computers / Computer
  Simulation,Computers / Data Science / Data Modeling \& Design,Computers /
  Information Technology,Computers / Programming / Algorithms,Language Arts \&
  Disciplines / Library \& Information Science / General,Technology \&
  Engineering / Automation,Technology \& Engineering / Engineering (General)}
    \strng{namehash}{YXGM1}
    \strng{fullhash}{YXGM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{YG10}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{abstract}{%
    Evolutionary algorithms (EAs) are becoming increasingly attractive for
  researchers from various disciplines, such as operations research, computer
  science, industrial engineering, electrical engineering, social science,
  economics, etc. This book presents an insightful, comprehensive, and
  up-to-date treatment of EAs, such as genetic algorithms, differential
  evolution, evolution strategy, constraint optimization, multimodal
  optimization, multiobjective optimization, combinatorial optimization,
  evolvable hardware, estimation of distribution algorithms, ant colony
  optimization, particle swarm optimization, artificial immune systems,
  artificial life, genetic programming, etc. It emphasises the initiative ideas
  of the algorithm, contains discussions in the contexts, and suggests further
  readings and possible research projects. All the methods form a pedagogical
  way to make EAs easy and interesting. This textbook also introduces the
  applications of EAs as many as possible. At least one real-life application
  is introduced by the end of almost every chapter. The authors focus on the
  kernel part of applications, such as how to model real-life problems, how to
  encode and decode the individuals, how to design effective search operators
  according to the chromosome structures, etc. This textbook adopts pedagogical
  ways of making EAs easy and interesting. Its methods include an introduction
  at the beginning of each chapter, emphasising the initiative, discussions in
  the contexts, summaries at the end of every chapter, suggested further
  reading, exercises, and possible research projects. Introduction to
  Evolutionary Algorithms will enable students to: \textbullet{} establish a
  strong background on evolutionary algorithms; \textbullet{} appreciate the
  cutting edge of EAs; \textbullet{} perform their own research projects by
  simulating the application introduced in the book; and \textbullet{} apply
  their intuitive ideas to academic search. This book is aimed at senior
  undergraduate students or first-year graduate students as a textbook or
  self-study material.%
    }
    \field{isbn}{978-1-84996-129-5}
    \field{title}{Introduction to Evolutionary Algorithms}
    \field{langid}{english}
    \field{month}{06}
    \field{year}{2010}
  \endentry

  \entry{zerahEfficientNewtonMethod1985}{article}{}
    \name{author}{1}{}{%
      {{hash=ZG}{%
         family={Zerah},
         familyi={Z\bibinitperiod},
         given={Gilles},
         giveni={G\bibinitperiod},
      }}%
    }
    \strng{namehash}{ZG1}
    \strng{fullhash}{ZG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Zer85}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    We propose a stable, straightforward algorithm for the numerical solution
  of integral equations for fluid pair distribution functions. The integral
  equation is not solved by Picard's standard iterative procedure but by
  Newton's method of solution of non-linear equations. The large matrix
  appearing in Newton's method is inverted by a conjugate gradient procedure
  used as a rapidly converging iterative method.%
    }
    \verb{doi}
    \verb 10.1016/0021-9991(85)90087-7
    \endverb
    \field{issn}{0021-9991}
    \field{number}{2}
    \field{pages}{280\bibrangedash 285}
    \field{title}{An Efficient Newton's Method for the Numerical Solution of
  Fluid Integral Equations}
    \field{volume}{61}
    \field{langid}{english}
    \field{journaltitle}{Journal of Computational Physics}
    \field{month}{11}
    \field{year}{1985}
  \endentry

  \entry{zerahSelfConsistentIntegral1986}{article}{}
    \name{author}{2}{}{%
      {{hash=ZG}{%
         family={Zerah},
         familyi={Z\bibinitperiod},
         given={Gilles},
         giveni={G\bibinitperiod},
      }}%
      {{hash=HJP}{%
         family={Hansen},
         familyi={H\bibinitperiod},
         given={Jean-Pierre},
         giveni={J\bibinithyphendelim P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{ZGHJP1}
    \strng{fullhash}{ZGHJP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{ZH86}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \verb{doi}
    \verb 10.1063/1.450397
    \endverb
    \field{issn}{0021-9606}
    \field{number}{4}
    \field{pages}{2336\bibrangedash 2343}
    \field{shorttitle}{Self-consistent Integral Equations for Fluid Pair
  Distribution Functions}
    \field{title}{Self-consistent Integral Equations for Fluid Pair
  Distribution Functions: Another Attempt}
    \field{volume}{84}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{02}
    \field{year}{1986}
  \endentry

  \entry{zhouUniversalityDeepConvolutional2020}{article}{}
    \name{author}{1}{}{%
      {{hash=ZDX}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Ding-Xuan},
         giveni={D\bibinithyphendelim X\bibinitperiod},
      }}%
    }
    \keyw{Approximation theory,Convolutional neural network,Deep
  learning,Universality}
    \strng{namehash}{ZDX1}
    \strng{fullhash}{ZDX1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Zho20}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    Deep learning has been widely applied and brought breakthroughs in speech
  recognition, computer vision, and many other domains. Deep neural network
  architectures and computational issues have been well studied in machine
  learning. But there lacks a theoretical foundation for understanding the
  approximation or generalization ability of deep learning methods generated by
  the network architectures such as deep convolutional neural networks. Here we
  show that a deep convolutional neural network (CNN) is universal, meaning
  that it can be used to approximate any continuous function to an arbitrary
  accuracy when the depth of the neural network is large enough. This answers
  an open question in learning theory. Our quantitative estimate, given tightly
  in terms of the number of free parameters to be computed, verifies the
  efficiency of deep CNNs in dealing with large dimensional data. Our study
  also demonstrates the role of convolutions in deep CNNs.%
    }
    \verb{doi}
    \verb 10.1016/j.acha.2019.06.004
    \endverb
    \field{issn}{1063-5203}
    \field{number}{2}
    \field{pages}{787\bibrangedash 794}
    \field{title}{Universality of Deep Convolutional Neural Networks}
    \field{volume}{48}
    \field{langid}{english}
    \field{journaltitle}{Applied and Computational Harmonic Analysis}
    \field{month}{03}
    \field{year}{2020}
  \endentry

  \entry{zhuPhysicsconstrainedDeepLearning2019}{article}{}
    \name{author}{4}{}{%
      {{hash=ZY}{%
         family={Zhu},
         familyi={Z\bibinitperiod},
         given={Yinhao},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZN}{%
         family={Zabaras},
         familyi={Z\bibinitperiod},
         given={Nicholas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=KPS}{%
         family={Koutsourelakis},
         familyi={K\bibinitperiod},
         given={Phaedon-Stelios},
         giveni={P\bibinithyphendelim S\bibinitperiod},
      }}%
      {{hash=PP}{%
         family={Perdikaris},
         familyi={P\bibinitperiod},
         given={Paris},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{Conditional generative model,Normalizing
  flow,Physics-constrained,Reverse KL divergence,Surrogate modeling,Uncertainty
  quantification}
    \strng{namehash}{ZY+1}
    \strng{fullhash}{ZYZNKPSPP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelalpha}{Zhu+19}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    Surrogate modeling and uncertainty quantification tasks for PDE systems are
  most often considered as supervised learning problems where input and output
  data pairs are used for training. The construction of such emulators is by
  definition a small data problem which poses challenges to deep learning
  approaches that have been developed to operate in the big data regime. Even
  in cases where such models have been shown to have good predictive capability
  in high dimensions, they fail to address constraints in the data implied by
  the PDE model. This paper provides a methodology that incorporates the
  governing equations of the physical model in the loss/likelihood functions.
  The resulting physics-constrained, deep learning models are trained without
  any labeled data (e.g. employing only input data) and provide comparable
  predictive responses with data-driven models while obeying the constraints of
  the problem at hand. This work employs a convolutional encoder-decoder neural
  network approach as well as a conditional flow-based generative model for the
  solution of PDEs, surrogate model construction, and uncertainty
  quantification tasks. The methodology is posed as a minimization problem of
  the reverse Kullback-Leibler (KL) divergence between the model predictive
  density and the reference conditional density, where the later is defined as
  the Boltzmann-Gibbs distribution at a given inverse temperature with the
  underlying potential relating to the PDE system of interest. The
  generalization capability of these models to out-of-distribution input is
  considered. Quantification and interpretation of the predictive uncertainty
  is provided for a number of problems.%
    }
    \verb{doi}
    \verb 10.1016/j.jcp.2019.05.024
    \endverb
    \field{issn}{0021-9991}
    \field{pages}{56\bibrangedash 81}
    \field{title}{Physics-Constrained Deep Learning for High-Dimensional
  Surrogate Modeling and Uncertainty Quantification without Labeled Data}
    \field{volume}{394}
    \field{langid}{english}
    \field{journaltitle}{Journal of Computational Physics}
    \field{month}{10}
    \field{year}{2019}
  \endentry

  \entry{zhouLocalStructureThermodynamics2019}{article}{}
    \name{author}{2}{}{%
      {{hash=ZY}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Yuxing},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=SKS}{%
         family={Schweizer},
         familyi={S\bibinitperiod},
         given={Kenneth\bibnamedelima S.},
         giveni={K\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Institute of Physics}%
    }
    \strng{namehash}{ZYSKS1}
    \strng{fullhash}{ZYSKS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelalpha}{ZS19}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \verb{doi}
    \verb 10.1063/1.5099369
    \endverb
    \field{issn}{0021-9606}
    \field{number}{21}
    \field{pages}{214902}
    \field{shorttitle}{Local Structure, Thermodynamics, and Phase Behavior of
  Asymmetric Particle Mixtures}
    \field{title}{Local Structure, Thermodynamics, and Phase Behavior of
  Asymmetric Particle Mixtures: Comparison between Integral Equation Theories
  and Simulation}
    \field{volume}{150}
    \field{journaltitle}{The Journal of Chemical Physics}
    \field{month}{06}
    \field{year}{2019}
  \endentry
\enddatalist
\endinput
